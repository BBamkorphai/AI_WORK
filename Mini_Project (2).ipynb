{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JiHf-TbEzU4"
      },
      "source": [
        "# Initialize Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTvOWwY7FSDs"
      },
      "source": [
        "## Import Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ-wizGUCz0-",
        "outputId": "209697ae-4753-4f17-a443-f7817aad50e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.4.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.4.1+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (71.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
        "from torchsummary import summary\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "%matplotlib inline\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, normalized_mutual_info_score, adjusted_rand_score\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, normalized_mutual_info_score, adjusted_rand_score, f1_score\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import tqdm\n",
        "import os.path as osp\n",
        "\n",
        "import torch\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.use(\"Agg\")\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as col\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as col\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import SGD\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "# from .meter import AverageMeter\n",
        "# from ..metric import binary_accuracy\n",
        "\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from google.colab import drive\n",
        "\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "import itertools\n",
        "from torch.autograd import Function\n",
        "from torch.autograd import Function\n",
        "\n",
        "import os\n",
        "import random\n",
        "from glob import glob\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "!pip install torchmetrics\n",
        "import torchmetrics\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torchmetrics import Accuracy, ConfusionMatrix, F1Score, Recall\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.utils.data\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torchmetrics.classification import F1Score\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import argparse\n",
        "import psutil\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "import itertools\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support, normalized_mutual_info_score, adjusted_rand_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import SGD\n",
        "\n",
        "from typing import Optional, List\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import csv\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support, normalized_mutual_info_score, adjusted_rand_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "import torch.nn.utils.prune as prune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFmCsTccIH3j"
      },
      "source": [
        "## Global Parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SkFB4XWIMwF"
      },
      "outputs": [],
      "source": [
        "image_size = 224\n",
        "batch_size = 64\n",
        "channel_size = 3\n",
        "lr = 1e-3\n",
        "num_classes = 31\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.manual_seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ARxVmsKFUfo"
      },
      "source": [
        "## Used Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YebgFahFXKv"
      },
      "outputs": [],
      "source": [
        "# Plot Graph Function\n",
        "def plot_graph(history, name):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_figwidth(10)\n",
        "    fig.suptitle(f\"Train vs Validation ({name})\")\n",
        "    ax1.plot(history[\"train_student_acc\"], label=\"Train\")\n",
        "    ax1.plot(history[\"val_student_acc\"], label=\"Validation\")\n",
        "    ax1.legend()\n",
        "    ax1.set_title(\"Accuracy\")\n",
        "\n",
        "    ax2.plot(history[\"train_student_loss\"], label=\"Train\")\n",
        "    ax2.plot(history[\"val_student_loss\"], label=\"Validation\")\n",
        "    ax2.legend()\n",
        "    ax2.set_title(\"Loss\")\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def print_model_param_nums(model=None):\n",
        "    if model == None:\n",
        "        model = torchvision.models.alexnet()\n",
        "    total = sum([param.nelement() if param.requires_grad else 0 for param in model.parameters()])\n",
        "    print('  + Number of params: %.4fM' % (total / 1e6))\n",
        "\n",
        "def count_model_param_flops(model=None, input_res=224, multiply_adds=True):\n",
        "\n",
        "    prods = {}\n",
        "    def save_hook(name):\n",
        "        def hook_per(self, input, output):\n",
        "            prods[name] = np.prod(input[0].shape)\n",
        "        return hook_per\n",
        "\n",
        "    list_1=[]\n",
        "    def simple_hook(self, input, output):\n",
        "        list_1.append(np.prod(input[0].shape))\n",
        "    list_2={}\n",
        "    def simple_hook2(self, input, output):\n",
        "        list_2['names'] = np.prod(input[0].shape)\n",
        "\n",
        "\n",
        "    list_conv=[]\n",
        "    def conv_hook(self, input, output):\n",
        "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
        "        output_channels, output_height, output_width = output[0].size()\n",
        "\n",
        "        kernel_ops = self.kernel_size[0] * self.kernel_size[1] * (self.in_channels / self.groups)\n",
        "        bias_ops = 1 if self.bias is not None else 0\n",
        "\n",
        "        params = output_channels * (kernel_ops + bias_ops)\n",
        "        # flops = (kernel_ops * (2 if multiply_adds else 1) + bias_ops) * output_channels * output_height * output_width * batch_size\n",
        "\n",
        "        num_weight_params = (self.weight.data != 0).float().sum()\n",
        "        flops = (num_weight_params * (2 if multiply_adds else 1) + bias_ops * output_channels) * output_height * output_width * batch_size\n",
        "\n",
        "        list_conv.append(flops)\n",
        "\n",
        "    list_linear=[]\n",
        "    def linear_hook(self, input, output):\n",
        "        batch_size = input[0].size(0) if input[0].dim() == 2 else 1\n",
        "\n",
        "        weight_ops = self.weight.nelement() * (2 if multiply_adds else 1)\n",
        "        bias_ops = self.bias.nelement()\n",
        "\n",
        "        flops = batch_size * (weight_ops + bias_ops)\n",
        "        list_linear.append(flops)\n",
        "\n",
        "    list_bn=[]\n",
        "    def bn_hook(self, input, output):\n",
        "        list_bn.append(input[0].nelement() * 2)\n",
        "\n",
        "    list_relu=[]\n",
        "    def relu_hook(self, input, output):\n",
        "        list_relu.append(input[0].nelement())\n",
        "\n",
        "    list_pooling=[]\n",
        "    def pooling_hook(self, input, output):\n",
        "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
        "        output_channels, output_height, output_width = output[0].size()\n",
        "\n",
        "        kernel_ops = self.kernel_size * self.kernel_size\n",
        "        bias_ops = 0\n",
        "        params = 0\n",
        "        flops = (kernel_ops + bias_ops) * output_channels * output_height * output_width * batch_size\n",
        "\n",
        "        list_pooling.append(flops)\n",
        "\n",
        "    list_upsample=[]\n",
        "\n",
        "    # For bilinear upsample\n",
        "    def upsample_hook(self, input, output):\n",
        "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
        "        output_channels, output_height, output_width = output[0].size()\n",
        "\n",
        "        flops = output_height * output_width * output_channels * batch_size * 12\n",
        "        list_upsample.append(flops)\n",
        "\n",
        "    def foo(net):\n",
        "        childrens = list(net.children())\n",
        "        if not childrens:\n",
        "            if isinstance(net, torch.nn.Conv2d):\n",
        "                net.register_forward_hook(conv_hook)\n",
        "            if isinstance(net, torch.nn.Linear):\n",
        "                net.register_forward_hook(linear_hook)\n",
        "            if isinstance(net, torch.nn.BatchNorm2d):\n",
        "                net.register_forward_hook(bn_hook)\n",
        "            if isinstance(net, torch.nn.ReLU):\n",
        "                net.register_forward_hook(relu_hook)\n",
        "            if isinstance(net, torch.nn.MaxPool2d) or isinstance(net, torch.nn.AvgPool2d):\n",
        "                net.register_forward_hook(pooling_hook)\n",
        "            if isinstance(net, torch.nn.Upsample):\n",
        "                net.register_forward_hook(upsample_hook)\n",
        "            return\n",
        "        for c in childrens:\n",
        "            foo(c)\n",
        "\n",
        "    if model == None:\n",
        "        model = torchvision.models.alexnet()\n",
        "    foo(model)\n",
        "    input = Variable(torch.rand(3,input_res,input_res).unsqueeze(0), requires_grad = True).to(device)\n",
        "    out = model(input)\n",
        "\n",
        "\n",
        "    total_flops = (sum(list_conv) + sum(list_linear) + sum(list_bn) + sum(list_relu) + sum(list_pooling) + sum(list_upsample))\n",
        "\n",
        "    print('Number of FLOPs: %.6f GFLOPs (%.2f MFLOPs)' % (total_flops / 1e9, total_flops / 1e6))\n",
        "\n",
        "    return total_flops\n",
        "\n",
        "# Inference Function (Collect lb, loss)\n",
        "def inference(loaded_model, validation_loader, mode=\"cuda\"):\n",
        "    device = torch.device(mode)\n",
        "    if mode == \"cpu\":\n",
        "        loaded_model.cpu()\n",
        "        stored_lbs, stored_preds = torch.empty(0, dtype=torch.float32), torch.empty(0, dtype=torch.float32)\n",
        "    else:\n",
        "        loaded_model.to(device)\n",
        "        stored_lbs, stored_preds = torch.empty(0, dtype=torch.float32).to(device), torch.empty(0, dtype=torch.float32).to(device)\n",
        "    acc_test = 0\n",
        "    test_loss = 0\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    loaded_model.eval()\n",
        "\n",
        "    for i, tdata in enumerate(validation_loader):\n",
        "        with torch.no_grad():\n",
        "            tinputs, tlabels = tdata[0].to(device), tdata[1].to(device)\n",
        "            toutputs = loaded_model(tinputs)\n",
        "            if isinstance(toutputs, (tuple, list)):\n",
        "                toutputs = toutputs[0]\n",
        "            loss = loss_fn(toutputs, tlabels)\n",
        "            test_loss += loss\n",
        "            _, preds_t = torch.max(toutputs, 1)\n",
        "            acc_test += (preds_t == tlabels).float().mean().item()\n",
        "            stored_lbs = torch.cat((stored_lbs, tlabels), 0)\n",
        "            stored_preds = torch.cat((stored_preds, preds_t), 0)\n",
        "\n",
        "    accuracy_t = round(acc_test / float(len(validation_loader)), 4)\n",
        "    avg_tloss = test_loss / (i + 1)\n",
        "    print(f\"[(Inference || test loss: {avg_tloss}] [accuracy_test: {accuracy_t * 100} %]\")\n",
        "\n",
        "    return stored_lbs, stored_preds, avg_tloss.item(), accuracy_t\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title=\"Confusion matrix\", cmap=plt.cm.Blues, show_labels=False, show_numbers=False):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    given a sklearn confusion matrix (cm), make a nice plot.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
        "    classes:      class names, for example: ['class1', 'class2', ...]\n",
        "    title:        the text to display at the top of the matrix\n",
        "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
        "\n",
        "    normalize:    If False, plot the raw numbers.\n",
        "                  If True, plot the proportions.\n",
        "    show_labels:  If False, hide class labels on the x and y axes.\n",
        "    show_numbers: If False, hide the numbers inside each grid.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if show_labels:\n",
        "        tick_marks = np.arange(len(classes))\n",
        "        plt.xticks(tick_marks, classes, rotation=45, fontsize=10)\n",
        "        plt.yticks(tick_marks, classes, fontsize=10)\n",
        "    else:\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype(\"float\") / (cm.sum(axis=1)[:, np.newaxis] + 1)\n",
        "\n",
        "    if show_numbers:\n",
        "        formated = \".2f\" if normalize else \"d\"\n",
        "        thresh = cm.max() / 2.0\n",
        "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "            plt.text(j, i, format(cm[i, j], formated), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.gcf().set_size_inches(10, 8)\n",
        "    plt.ylabel(\"True label\")\n",
        "    plt.xlabel(\"Predicted label\")\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "\n",
        "def collect_feature(data_loader: DataLoader, feature_extractor: nn.Module, device: torch.device, max_num_features=None) -> torch.Tensor:\n",
        "    feature_extractor.eval()\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for i, (images, target) in enumerate(tqdm.tqdm(data_loader)):\n",
        "            images = images.to(device)\n",
        "            features = feature_extractor(images)\n",
        "            if isinstance(features, tuple):\n",
        "                feature_tensor = features[0]\n",
        "                feature_tensor = feature_tensor.to(device)\n",
        "            else:\n",
        "                feature_tensor = features.to(device)\n",
        "\n",
        "            all_features.append(feature_tensor)\n",
        "            all_labels.append(target)\n",
        "\n",
        "    return torch.cat(all_features, dim=0), torch.cat(all_labels, dim=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def visualize(source_feature: torch.Tensor, target_feature: torch.Tensor, filename: str, source_color=\"r\", target_color=\"b\"):\n",
        "    \"\"\"\n",
        "    Visualize features from different domains using t-SNE.\n",
        "    Args:\n",
        "        source_feature (tensor): features from source domain in shape :math:`(minibatch, F)`\n",
        "        target_feature (tensor): features from target domain in shape :math:`(minibatch, F)`\n",
        "        filename (str): the file name to save t-SNE\n",
        "        source_color (str): the color of the source features. Default: 'r'\n",
        "        target_color (str): the color of the target features. Default: 'b'\n",
        "    \"\"\"\n",
        "    source_feature = source_feature.cpu().numpy()\n",
        "    target_feature = target_feature.cpu().numpy()\n",
        "    features = np.concatenate([source_feature, target_feature], axis=0)\n",
        "\n",
        "    X_tsne = TSNE(n_components=2, random_state=33).fit_transform(features)\n",
        "\n",
        "    domains = np.concatenate((np.ones(len(source_feature)), np.zeros(len(target_feature))))\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=domains, cmap=col.ListedColormap([source_color, target_color]), s=20)  # default: s=2\n",
        "    plt.savefig(filename)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  Arg: t-SNE for class clustering visualization\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def visualize_class_n_domain(\n",
        "    source_feature: torch.Tensor, target_feature: torch.Tensor, source_labels: torch.Tensor, target_labels: torch.Tensor, filename: str, source_color=\"r\", target_color=\"b\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Visualize features from different domains using t-SNE.\n",
        "    Args:\n",
        "        source_feature (tensor): features from source domain in shape :math:`(minibatch, F)`\n",
        "        target_feature (tensor): features from target domain in shape :math:`(minibatch, F)`\n",
        "        source_labels (tensor): class labels for source domain features\n",
        "        target_labels (tensor): class labels for target domain features\n",
        "        filename (str): the file name to save t-SNE\n",
        "        source_color (str): the color of the source features. Default: 'r'\n",
        "        target_color (str): the color of the target features. Default: 'b'\n",
        "    \"\"\"\n",
        "    source_feature = source_feature.cpu().numpy()\n",
        "    target_feature = target_feature.cpu().numpy()\n",
        "    source_labels = source_labels.cpu().numpy()\n",
        "    target_labels = target_labels.cpu().numpy()\n",
        "\n",
        "    features = np.concatenate([source_feature, target_feature], axis=0)\n",
        "    labels = np.concatenate([source_labels, target_labels], axis=0)\n",
        "    domains = np.concatenate((np.ones(len(source_feature)), np.zeros(len(target_feature))))\n",
        "\n",
        "    X_tsne = TSNE(n_components=2, random_state=33).fit_transform(features)\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "\n",
        "    unique_labels = np.unique(labels)\n",
        "\n",
        "    cmap_s = plt.get_cmap(\"nipy_spectral\", len(unique_labels))\n",
        "    cmap_r = plt.get_cmap(\"gist_rainbow\", len(unique_labels))\n",
        "\n",
        "    for label in unique_labels:\n",
        "        for domain in [0, 1]:\n",
        "            mask = (labels == label) & (domains == domain)\n",
        "            plt.scatter(\n",
        "                X_tsne[mask, 0],\n",
        "                X_tsne[mask, 1],\n",
        "                c=cmap_s(label),\n",
        "                s=10,\n",
        "                # label=f\"Class {label}, Domain {domain}\",\n",
        "            )\n",
        "\n",
        "    plt.legend()\n",
        "    plt.savefig(filename)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  Args: A-distance computation\n",
        "  Ref: https://github.com/zhjscut/Bridging_UDA_SSL/blob/e0be6742f1203bb983261e3e1e57d34e1e03299d/common/utils/analysis/a_distance.py\n",
        "\"\"\"\n",
        "\n",
        "def binary_accuracy(output: torch.Tensor, target: torch.Tensor) -> float:\n",
        "    \"\"\"Computes the accuracy for binary classification\"\"\"\n",
        "    with torch.no_grad():\n",
        "        batch_size = target.size(0)\n",
        "        pred = (output >= 0.5).float().t().view(-1)\n",
        "        correct = pred.eq(target.view(-1)).float().sum()\n",
        "        correct.mul_(100. / batch_size)\n",
        "        return correct\n",
        "\n",
        "class ANet(nn.Module):\n",
        "    def __init__(self, in_feature):\n",
        "        super(ANet, self).__init__()\n",
        "        self.layer = nn.Linear(in_feature, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def Adist_calculate(source_feature: torch.Tensor, target_feature: torch.Tensor,\n",
        "              device, progress=True, training_epochs=10):\n",
        "    \"\"\"\n",
        "    Calculate the :math:`\\mathcal{A}`-distance, which is a measure for distribution discrepancy.\n",
        "    The definition is :math:`dist_\\mathcal{A} = 2 (1-2\\epsilon)`, where :math:`\\epsilon` is the\n",
        "    test error of a classifier trained to discriminate the source from the target.\n",
        "    Args:\n",
        "        source_feature (tensor): features from source domain in shape :math:`(minibatch, F)`\n",
        "        target_feature (tensor): features from target domain in shape :math:`(minibatch, F)`\n",
        "        device (torch.device)\n",
        "        progress (bool): if True, displays a the progress of training A-Net\n",
        "        training_epochs (int): the number of epochs when training the classifier\n",
        "    Returns:\n",
        "        :math:`\\mathcal{A}`-distance\n",
        "    \"\"\"\n",
        "    source_label = torch.ones((source_feature.shape[0], 1))\n",
        "    target_label = torch.zeros((target_feature.shape[0], 1))\n",
        "    feature = torch.cat([source_feature, target_feature], dim=0)\n",
        "    label = torch.cat([source_label, target_label], dim=0)\n",
        "\n",
        "    dataset = TensorDataset(feature, label)\n",
        "    length = len(dataset)\n",
        "    train_size = int(0.8 * length)\n",
        "    val_size = length - train_size\n",
        "\n",
        "    # Specify a random generator for CUDA\n",
        "    generator = torch.Generator(device=device)\n",
        "\n",
        "    train_set, val_set = torch.utils.data.random_split(dataset, [train_size, val_size], generator)\n",
        "    train_loader = DataLoader(train_set, batch_size=2, shuffle=True, generator=generator)\n",
        "    val_loader = DataLoader(val_set, batch_size=8, shuffle=False, generator=generator)\n",
        "\n",
        "    anet = ANet(feature.shape[1]).to(device)\n",
        "    optimizer = SGD(anet.parameters(), lr=0.01)\n",
        "    a_distance = 2.0\n",
        "    for epoch in range(training_epochs):\n",
        "        anet.train()\n",
        "        for (x, label) in train_loader:\n",
        "            x = x.to(device)\n",
        "            label = label.to(device)\n",
        "            anet.zero_grad()\n",
        "            y = anet(x)\n",
        "            loss = F.binary_cross_entropy(y, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        anet.eval()\n",
        "        meter = AverageMeter(\"accuracy\", \":4.2f\")\n",
        "        with torch.no_grad():\n",
        "            for (x, label) in val_loader:\n",
        "                x = x.to(device)\n",
        "                label = label.to(device)\n",
        "                y = anet(x)\n",
        "                acc = binary_accuracy(y, label)\n",
        "                meter.update(acc, x.shape[0])\n",
        "        error = 1 - meter.avg / 100\n",
        "        a_distance = 2 * (1 - 2 * error)\n",
        "        if progress:\n",
        "            print(\"epoch {} accuracy: {} A-dist: {}\".format(epoch, meter.avg, a_distance))\n",
        "\n",
        "    return a_distance\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    r\"\"\"Computes and stores the average and current value.\n",
        "    Examples::\n",
        "        >>> # Initialize a meter to record loss\n",
        "        >>> losses = AverageMeter()\n",
        "        >>> # Update meter after every minibatch update\n",
        "        >>> losses.update(loss_value, batch_size)\n",
        "    \"\"\"\n",
        "    def __init__(self, name: str, fmt: Optional[str] = ':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        if self.count > 0:\n",
        "            self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class AverageMeterDict(object):\n",
        "    def __init__(self, names: List, fmt: Optional[str] = ':f'):\n",
        "        self.dict = {\n",
        "            name: AverageMeter(name, fmt) for name in names\n",
        "        }\n",
        "\n",
        "    def reset(self):\n",
        "        for meter in self.dict.values():\n",
        "            meter.reset()\n",
        "\n",
        "    def update(self, accuracies, n=1):\n",
        "        for name, acc in accuracies.items():\n",
        "            self.dict[name].update(acc, n)\n",
        "\n",
        "    def average(self):\n",
        "        return {\n",
        "            name: meter.avg for name, meter in self.dict.items()\n",
        "        }\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.dict[item]\n",
        "\n",
        "\n",
        "class Meter(object):\n",
        "    \"\"\"Computes and stores the current value.\"\"\"\n",
        "    def __init__(self, name: str, fmt: Optional[str] = ':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "\n",
        "    def update(self, val):\n",
        "        self.val = val\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '}'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rK3fj6A1f4PX"
      },
      "outputs": [],
      "source": [
        "def plot_graphDA(history, name):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_figwidth(10)\n",
        "    fig.suptitle(f\"Train vs Validation {name}\")\n",
        "    ax1.plot(history[\"train_src_acc\"], label=\"Train_src_acc\")\n",
        "    ax1.plot(history[\"train_tar_acc\"], label=\"Train_tar_acc\")\n",
        "    ax1.legend()\n",
        "    ax1.set_title(\"Accuracy (Src/Tar)\")\n",
        "\n",
        "    ax2.plot(history[\"train_loss\"], label=\"Train_loss\")\n",
        "    ax2.legend()\n",
        "    ax2.set_title(\"Loss\")\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7CkYoKhhq7d"
      },
      "outputs": [],
      "source": [
        "def get_loader_DA(src, tar):\n",
        "  if src == \"amazon\":\n",
        "    src_tl = amazon_tl\n",
        "  elif src == \"dslr\":\n",
        "    src_tl = dslr_tl\n",
        "  elif src == \"webcam\":\n",
        "    src_tl = webcam_tl\n",
        "  else:\n",
        "    print(\"Undefined Source\")\n",
        "    return None, None\n",
        "  if tar == \"amazon\":\n",
        "    tar_tl = amazon_tl\n",
        "  elif tar == \"dslr\":\n",
        "    tar_tl = dslr_tl\n",
        "  elif tar == \"webcam\":\n",
        "    tar_tl = webcam_tl\n",
        "  else:\n",
        "    print(\"Undefined Target\")\n",
        "    return None, None\n",
        "  return src_tl, tar_tl\n",
        "\n",
        "def get_loader(sd):\n",
        "\n",
        "    if sd == \"amazon\":\n",
        "        return amazon_tl, amazon_vl\n",
        "    elif sd == \"dslr\":\n",
        "        return dslr_tl, dslr_vl\n",
        "    elif sd == \"webcam\":\n",
        "        return webcam_tl, webcam_vl\n",
        "    else:\n",
        "        print(\"Undefined Dataset\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irBMkZHkjqWW"
      },
      "outputs": [],
      "source": [
        "def save_training_logs(logs, filename):\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(logs, f, indent=4)\n",
        "    print(f\"Training logs saved to {filename}\")\n",
        "\n",
        "def load_training_logs(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        logs = json.load(f)\n",
        "    print(f\"Training logs loaded from {filename}\")\n",
        "    return logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6SmjM5gMGHE"
      },
      "outputs": [],
      "source": [
        "def zip_directory(folder_path, zip_name):\n",
        "    # Create a zip file\n",
        "    with zipfile.ZipFile(zip_name, 'w') as zipf:\n",
        "        # Walk through the directory and add files\n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                zipf.write(file_path, os.path.relpath(file_path, folder_path))\n",
        "\n",
        "def zip_log_files(zip_name='log_files.zip'):\n",
        "    \"\"\"\n",
        "    Zips all files in the current directory that start with 'LOG_'.\n",
        "\n",
        "    Parameters:\n",
        "    zip_name (str): The name of the resulting zip file.\n",
        "    \"\"\"\n",
        "    # Create a Zip file\n",
        "    with zipfile.ZipFile(zip_name, 'w') as zipf:\n",
        "        # Loop through files in the current directory\n",
        "        for file in os.listdir('.'):\n",
        "            # Check if the file starts with 'LOG_'\n",
        "            if file.startswith('LOG_'):\n",
        "                zipf.write(file)\n",
        "                print(f\"Added {file} to the zip\")\n",
        "\n",
        "    print(f\"All files starting with 'LOG_' have been zipped into {zip_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgCpru5m_hzr"
      },
      "outputs": [],
      "source": [
        "# @title FLOPS computation\n",
        "# Code from https://github.com/Eric-mingjie/rethinking-network-pruning/blob/master/imagenet/l1-norm-pruning/compute_flops.py\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def print_model_param_nums(model=None):\n",
        "    if model == None:\n",
        "        model = torchvision.models.alexnet()\n",
        "    total = sum([param.nelement() if param.requires_grad else 0 for param in model.parameters()])\n",
        "    print('  + Number of params: %.4fM' % (total / 1e6))\n",
        "\n",
        "def count_model_param_flops(model=None, input_res=224, multiply_adds=True):\n",
        "\n",
        "    prods = {}\n",
        "    def save_hook(name):\n",
        "        def hook_per(self, input, output):\n",
        "            prods[name] = np.prod(input[0].shape)\n",
        "        return hook_per\n",
        "\n",
        "    list_1=[]\n",
        "    def simple_hook(self, input, output):\n",
        "        list_1.append(np.prod(input[0].shape))\n",
        "    list_2={}\n",
        "    def simple_hook2(self, input, output):\n",
        "        list_2['names'] = np.prod(input[0].shape)\n",
        "\n",
        "\n",
        "    list_conv=[]\n",
        "    def conv_hook(self, input, output):\n",
        "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
        "        output_channels, output_height, output_width = output[0].size()\n",
        "\n",
        "        kernel_ops = self.kernel_size[0] * self.kernel_size[1] * (self.in_channels / self.groups)\n",
        "        bias_ops = 1 if self.bias is not None else 0\n",
        "\n",
        "        params = output_channels * (kernel_ops + bias_ops)\n",
        "        # flops = (kernel_ops * (2 if multiply_adds else 1) + bias_ops) * output_channels * output_height * output_width * batch_size\n",
        "\n",
        "        num_weight_params = (self.weight.data != 0).float().sum()\n",
        "        flops = (num_weight_params * (2 if multiply_adds else 1) + bias_ops * output_channels) * output_height * output_width * batch_size\n",
        "\n",
        "        list_conv.append(flops)\n",
        "\n",
        "    list_linear=[]\n",
        "    def linear_hook(self, input, output):\n",
        "        batch_size = input[0].size(0) if input[0].dim() == 2 else 1\n",
        "\n",
        "        weight_ops = self.weight.nelement() * (2 if multiply_adds else 1)\n",
        "        bias_ops = self.bias.nelement()\n",
        "\n",
        "        flops = batch_size * (weight_ops + bias_ops)\n",
        "        list_linear.append(flops)\n",
        "\n",
        "    list_bn=[]\n",
        "    def bn_hook(self, input, output):\n",
        "        list_bn.append(input[0].nelement() * 2)\n",
        "\n",
        "    list_relu=[]\n",
        "    def relu_hook(self, input, output):\n",
        "        list_relu.append(input[0].nelement())\n",
        "\n",
        "    list_pooling=[]\n",
        "    def pooling_hook(self, input, output):\n",
        "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
        "        output_channels, output_height, output_width = output[0].size()\n",
        "\n",
        "        kernel_ops = self.kernel_size * self.kernel_size\n",
        "        bias_ops = 0\n",
        "        params = 0\n",
        "        flops = (kernel_ops + bias_ops) * output_channels * output_height * output_width * batch_size\n",
        "\n",
        "        list_pooling.append(flops)\n",
        "\n",
        "    list_upsample=[]\n",
        "\n",
        "    # For bilinear upsample\n",
        "    def upsample_hook(self, input, output):\n",
        "        batch_size, input_channels, input_height, input_width = input[0].size()\n",
        "        output_channels, output_height, output_width = output[0].size()\n",
        "\n",
        "        flops = output_height * output_width * output_channels * batch_size * 12\n",
        "        list_upsample.append(flops)\n",
        "\n",
        "    def foo(net):\n",
        "        childrens = list(net.children())\n",
        "        if not childrens:\n",
        "            if isinstance(net, torch.nn.Conv2d):\n",
        "                net.register_forward_hook(conv_hook)\n",
        "            if isinstance(net, torch.nn.Linear):\n",
        "                net.register_forward_hook(linear_hook)\n",
        "            if isinstance(net, torch.nn.BatchNorm2d):\n",
        "                net.register_forward_hook(bn_hook)\n",
        "            if isinstance(net, torch.nn.ReLU):\n",
        "                net.register_forward_hook(relu_hook)\n",
        "            if isinstance(net, torch.nn.MaxPool2d) or isinstance(net, torch.nn.AvgPool2d):\n",
        "                net.register_forward_hook(pooling_hook)\n",
        "            if isinstance(net, torch.nn.Upsample):\n",
        "                net.register_forward_hook(upsample_hook)\n",
        "            return\n",
        "        for c in childrens:\n",
        "            foo(c)\n",
        "\n",
        "    if model == None:\n",
        "        model = torchvision.models.alexnet()\n",
        "    foo(model)\n",
        "    input = Variable(torch.rand(3,input_res,input_res).unsqueeze(0), requires_grad = True).to(device)\n",
        "    out = model(input)\n",
        "\n",
        "\n",
        "    total_flops = (sum(list_conv) + sum(list_linear) + sum(list_bn) + sum(list_relu) + sum(list_pooling) + sum(list_upsample))\n",
        "\n",
        "    print('Number of FLOPs: %.6f GFLOPs (%.2f MFLOPs)' % (total_flops / 1e9, total_flops / 1e6))\n",
        "\n",
        "    return total_flops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tty0LBrTLWTi"
      },
      "source": [
        "## Data Proprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CZrmZ4rLbLT",
        "outputId": "cfb1a32b-ac51-4ebf-8bbb-2ed4cffa9dd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at ./drive/; to attempt to forcibly remount, call drive.mount(\"./drive/\", force_remount=True).\n",
            "Archive:  ./drive/MyDrive/99H_datasets/Office-31.zip\n",
            "  inflating: Office-31/amazon/back_pack/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/back_pack/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/bike/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/bike_helmet/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/bookcase/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/bottle/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/calculator/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/desk_chair/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0095.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0096.jpg  \n",
            "  inflating: Office-31/amazon/desk_lamp/frame_0097.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0095.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0096.jpg  \n",
            "  inflating: Office-31/amazon/desktop_computer/frame_0097.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/file_cabinet/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0095.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0096.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0097.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0098.jpg  \n",
            "  inflating: Office-31/amazon/headphones/frame_0099.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0095.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0096.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0097.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0098.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0099.jpg  \n",
            "  inflating: Office-31/amazon/keyboard/frame_0100.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0095.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0096.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0097.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0098.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0099.jpg  \n",
            "  inflating: Office-31/amazon/laptop_computer/frame_0100.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0095.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0096.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0097.jpg  \n",
            "  inflating: Office-31/amazon/letter_tray/frame_0098.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0095.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0096.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0097.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0098.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0099.jpg  \n",
            "  inflating: Office-31/amazon/mobile_phone/frame_0100.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0095.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0096.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0097.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0098.jpg  \n",
            "  inflating: Office-31/amazon/monitor/frame_0099.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0095.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0096.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0097.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0098.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0099.jpg  \n",
            "  inflating: Office-31/amazon/mouse/frame_0100.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/mug/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0095.jpg  \n",
            "  inflating: Office-31/amazon/paper_notebook/frame_0096.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/pen/frame_0095.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/phone/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0095.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0096.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0097.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0098.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0099.jpg  \n",
            "  inflating: Office-31/amazon/printer/frame_0100.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0095.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0096.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0097.jpg  \n",
            "  inflating: Office-31/amazon/projector/frame_0098.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0095.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0096.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0097.jpg  \n",
            "  inflating: Office-31/amazon/punchers/frame_0098.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/ring_binder/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/ruler/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0095.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0096.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0097.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0098.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0099.jpg  \n",
            "  inflating: Office-31/amazon/scissors/frame_0100.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0095.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0096.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0097.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0098.jpg  \n",
            "  inflating: Office-31/amazon/speaker/frame_0099.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0095.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0096.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0097.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0098.jpg  \n",
            "  inflating: Office-31/amazon/stapler/frame_0099.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0064.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0065.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0066.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0067.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0068.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0069.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0070.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0071.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0072.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0073.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0074.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0075.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0076.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0077.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0078.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0079.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0080.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0081.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0082.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0083.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0084.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0085.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0086.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0087.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0088.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0089.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0090.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0091.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0092.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0093.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0094.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0095.jpg  \n",
            "  inflating: Office-31/amazon/tape_dispenser/frame_0096.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0001.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0002.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0003.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0004.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0005.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0006.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0007.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0008.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0009.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0010.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0011.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0012.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0013.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0014.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0015.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0016.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0017.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0018.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0019.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0020.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0021.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0022.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0023.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0024.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0025.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0026.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0027.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0028.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0029.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0030.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0031.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0032.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0033.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0034.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0035.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0036.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0037.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0038.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0039.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0040.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0041.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0042.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0043.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0044.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0045.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0046.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0047.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0048.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0049.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0050.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0051.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0052.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0053.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0054.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0055.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0056.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0057.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0058.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0059.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0060.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0061.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0062.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0063.jpg  \n",
            "  inflating: Office-31/amazon/trash_can/frame_0064.jpg  \n",
            "  inflating: Office-31/dslr/back_pack/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/back_pack/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/back_pack/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/back_pack/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/back_pack/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/back_pack/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/back_pack/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/back_pack/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/back_pack/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/back_pack/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/back_pack/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/back_pack/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0014.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0015.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0016.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0017.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0018.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0019.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0020.jpg  \n",
            "  inflating: Office-31/dslr/bike/frame_0021.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0014.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0015.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0016.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0017.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0018.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0019.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0020.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0021.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0022.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0023.jpg  \n",
            "  inflating: Office-31/dslr/bike_helmet/frame_0024.jpg  \n",
            "  inflating: Office-31/dslr/bookcase/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/bookcase/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/bookcase/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/bookcase/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/bookcase/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/bookcase/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/bookcase/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/bookcase/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/bookcase/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/bookcase/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/bookcase/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/bookcase/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/bottle/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/bottle/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/bottle/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/bottle/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/bottle/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/bottle/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/bottle/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/bottle/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/bottle/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/bottle/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/bottle/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/bottle/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/bottle/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/bottle/frame_0014.jpg  \n",
            "  inflating: Office-31/dslr/bottle/frame_0015.jpg  \n",
            "  inflating: Office-31/dslr/bottle/frame_0016.jpg  \n",
            "  inflating: Office-31/dslr/calculator/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/calculator/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/calculator/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/calculator/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/calculator/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/calculator/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/calculator/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/calculator/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/calculator/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/calculator/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/calculator/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/calculator/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/desk_chair/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/desk_chair/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/desk_chair/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/desk_chair/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/desk_chair/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/desk_chair/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/desk_chair/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/desk_chair/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/desk_chair/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/desk_chair/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/desk_chair/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/desk_chair/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/desk_chair/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/desk_lamp/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/desk_lamp/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/desk_lamp/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/desk_lamp/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/desk_lamp/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/desk_lamp/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/desk_lamp/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/desk_lamp/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/desk_lamp/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/desk_lamp/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/desk_lamp/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/desk_lamp/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/desk_lamp/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/desk_lamp/frame_0014.jpg  \n",
            "  inflating: Office-31/dslr/desktop_computer/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/desktop_computer/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/desktop_computer/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/desktop_computer/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/desktop_computer/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/desktop_computer/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/desktop_computer/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/desktop_computer/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/desktop_computer/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/desktop_computer/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/desktop_computer/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/desktop_computer/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/desktop_computer/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/desktop_computer/frame_0014.jpg  \n",
            "  inflating: Office-31/dslr/desktop_computer/frame_0015.jpg  \n",
            "  inflating: Office-31/dslr/file_cabinet/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/file_cabinet/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/file_cabinet/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/file_cabinet/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/file_cabinet/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/file_cabinet/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/file_cabinet/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/file_cabinet/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/file_cabinet/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/file_cabinet/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/file_cabinet/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/file_cabinet/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/file_cabinet/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/file_cabinet/frame_0014.jpg  \n",
            "  inflating: Office-31/dslr/file_cabinet/frame_0015.jpg  \n",
            "  inflating: Office-31/dslr/headphones/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/headphones/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/headphones/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/headphones/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/headphones/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/headphones/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/headphones/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/headphones/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/headphones/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/headphones/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/headphones/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/headphones/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/headphones/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/keyboard/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/keyboard/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/keyboard/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/keyboard/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/keyboard/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/keyboard/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/keyboard/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/keyboard/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/keyboard/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/keyboard/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0014.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0015.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0016.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0017.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0018.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0019.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0020.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0021.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0022.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0023.jpg  \n",
            "  inflating: Office-31/dslr/laptop_computer/frame_0024.jpg  \n",
            "  inflating: Office-31/dslr/letter_tray/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/letter_tray/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/letter_tray/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/letter_tray/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/letter_tray/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/letter_tray/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/letter_tray/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/letter_tray/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/letter_tray/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/letter_tray/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/letter_tray/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/letter_tray/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/letter_tray/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/letter_tray/frame_0014.jpg  \n",
            "  inflating: Office-31/dslr/letter_tray/frame_0015.jpg  \n",
            "  inflating: Office-31/dslr/letter_tray/frame_0016.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0014.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0015.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0016.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0017.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0018.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0019.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0020.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0021.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0022.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0023.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0024.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0025.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0026.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0027.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0028.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0029.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0030.jpg  \n",
            "  inflating: Office-31/dslr/mobile_phone/frame_0031.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0014.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0015.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0016.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0017.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0018.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0019.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0020.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0021.jpg  \n",
            "  inflating: Office-31/dslr/monitor/frame_0022.jpg  \n",
            "  inflating: Office-31/dslr/mouse/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/mouse/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/mouse/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/mouse/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/mouse/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/mouse/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/mouse/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/mouse/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/mouse/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/mouse/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/mouse/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/mouse/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/mug/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/mug/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/mug/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/mug/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/mug/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/mug/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/mug/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/mug/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/paper_notebook/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/paper_notebook/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/paper_notebook/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/paper_notebook/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/paper_notebook/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/paper_notebook/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/paper_notebook/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/paper_notebook/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/paper_notebook/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/paper_notebook/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/pen/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/pen/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/pen/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/pen/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/pen/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/pen/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/pen/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/pen/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/pen/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/pen/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/phone/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/phone/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/phone/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/phone/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/phone/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/phone/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/phone/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/phone/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/phone/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/phone/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/phone/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/phone/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/phone/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/printer/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/printer/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/printer/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/printer/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/printer/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/printer/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/printer/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/printer/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/printer/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/printer/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/printer/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/printer/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/printer/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/printer/frame_0014.jpg  \n",
            "  inflating: Office-31/dslr/printer/frame_0015.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0014.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0015.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0016.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0017.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0018.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0019.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0020.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0021.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0022.jpg  \n",
            "  inflating: Office-31/dslr/projector/frame_0023.jpg  \n",
            "  inflating: Office-31/dslr/punchers/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/punchers/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/punchers/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/punchers/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/punchers/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/punchers/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/punchers/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/punchers/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/punchers/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/punchers/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/punchers/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/punchers/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/punchers/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/punchers/frame_0014.jpg  \n",
            "  inflating: Office-31/dslr/punchers/frame_0015.jpg  \n",
            "  inflating: Office-31/dslr/punchers/frame_0016.jpg  \n",
            "  inflating: Office-31/dslr/punchers/frame_0017.jpg  \n",
            "  inflating: Office-31/dslr/punchers/frame_0018.jpg  \n",
            "  inflating: Office-31/dslr/ring_binder/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/ring_binder/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/ring_binder/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/ring_binder/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/ring_binder/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/ring_binder/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/ring_binder/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/ring_binder/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/ring_binder/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/ring_binder/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/ruler/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/ruler/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/ruler/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/ruler/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/ruler/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/ruler/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/ruler/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/scissors/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/scissors/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/scissors/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/scissors/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/scissors/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/scissors/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/scissors/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/scissors/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/scissors/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/scissors/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/scissors/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/scissors/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/scissors/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/scissors/frame_0014.jpg  \n",
            "  inflating: Office-31/dslr/scissors/frame_0015.jpg  \n",
            "  inflating: Office-31/dslr/scissors/frame_0016.jpg  \n",
            "  inflating: Office-31/dslr/scissors/frame_0017.jpg  \n",
            "  inflating: Office-31/dslr/scissors/frame_0018.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0014.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0015.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0016.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0017.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0018.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0019.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0020.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0021.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0022.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0023.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0024.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0025.jpg  \n",
            "  inflating: Office-31/dslr/speaker/frame_0026.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0014.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0015.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0016.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0017.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0018.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0019.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0020.jpg  \n",
            "  inflating: Office-31/dslr/stapler/frame_0021.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0014.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0015.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0016.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0017.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0018.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0019.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0020.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0021.jpg  \n",
            "  inflating: Office-31/dslr/tape_dispenser/frame_0022.jpg  \n",
            "  inflating: Office-31/dslr/trash_can/frame_0001.jpg  \n",
            "  inflating: Office-31/dslr/trash_can/frame_0002.jpg  \n",
            "  inflating: Office-31/dslr/trash_can/frame_0003.jpg  \n",
            "  inflating: Office-31/dslr/trash_can/frame_0004.jpg  \n",
            "  inflating: Office-31/dslr/trash_can/frame_0005.jpg  \n",
            "  inflating: Office-31/dslr/trash_can/frame_0006.jpg  \n",
            "  inflating: Office-31/dslr/trash_can/frame_0007.jpg  \n",
            "  inflating: Office-31/dslr/trash_can/frame_0008.jpg  \n",
            "  inflating: Office-31/dslr/trash_can/frame_0009.jpg  \n",
            "  inflating: Office-31/dslr/trash_can/frame_0010.jpg  \n",
            "  inflating: Office-31/dslr/trash_can/frame_0011.jpg  \n",
            "  inflating: Office-31/dslr/trash_can/frame_0012.jpg  \n",
            "  inflating: Office-31/dslr/trash_can/frame_0013.jpg  \n",
            "  inflating: Office-31/dslr/trash_can/frame_0014.jpg  \n",
            "  inflating: Office-31/dslr/trash_can/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0025.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0026.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0027.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0028.jpg  \n",
            "  inflating: Office-31/webcam/back_pack/frame_0029.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/bike/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0025.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0026.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0027.jpg  \n",
            "  inflating: Office-31/webcam/bike_helmet/frame_0028.jpg  \n",
            "  inflating: Office-31/webcam/bookcase/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/bookcase/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/bookcase/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/bookcase/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/bookcase/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/bookcase/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/bookcase/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/bookcase/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/bookcase/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/bookcase/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/bookcase/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/bookcase/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/bottle/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/bottle/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/bottle/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/bottle/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/bottle/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/bottle/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/bottle/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/bottle/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/bottle/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/bottle/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/bottle/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/bottle/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/bottle/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/bottle/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/bottle/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/bottle/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0025.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0026.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0027.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0028.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0029.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0030.jpg  \n",
            "  inflating: Office-31/webcam/calculator/frame_0031.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0025.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0026.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0027.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0028.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0029.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0030.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0031.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0032.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0033.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0034.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0035.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0036.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0037.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0038.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0039.jpg  \n",
            "  inflating: Office-31/webcam/desk_chair/frame_0040.jpg  \n",
            "  inflating: Office-31/webcam/desk_lamp/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/desk_lamp/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/desk_lamp/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/desk_lamp/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/desk_lamp/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/desk_lamp/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/desk_lamp/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/desk_lamp/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/desk_lamp/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/desk_lamp/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/desk_lamp/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/desk_lamp/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/desk_lamp/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/desk_lamp/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/desk_lamp/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/desk_lamp/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/desk_lamp/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/desk_lamp/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/desktop_computer/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/file_cabinet/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0025.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0026.jpg  \n",
            "  inflating: Office-31/webcam/headphones/frame_0027.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0025.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0026.jpg  \n",
            "  inflating: Office-31/webcam/keyboard/frame_0027.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0025.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0026.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0027.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0028.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0029.jpg  \n",
            "  inflating: Office-31/webcam/laptop_computer/frame_0030.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/letter_tray/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0025.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0026.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0027.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0028.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0029.jpg  \n",
            "  inflating: Office-31/webcam/mobile_phone/frame_0030.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0025.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0026.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0027.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0028.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0029.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0030.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0031.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0032.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0033.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0034.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0035.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0036.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0037.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0038.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0039.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0040.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0041.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0042.jpg  \n",
            "  inflating: Office-31/webcam/monitor/frame_0043.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0025.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0026.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0027.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0028.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0029.jpg  \n",
            "  inflating: Office-31/webcam/mouse/frame_0030.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0025.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0026.jpg  \n",
            "  inflating: Office-31/webcam/mug/frame_0027.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0025.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0026.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0027.jpg  \n",
            "  inflating: Office-31/webcam/paper_notebook/frame_0028.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0025.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0026.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0027.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0028.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0029.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0030.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0031.jpg  \n",
            "  inflating: Office-31/webcam/pen/frame_0032.jpg  \n",
            "  inflating: Office-31/webcam/phone/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/phone/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/phone/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/phone/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/phone/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/phone/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/phone/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/phone/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/phone/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/phone/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/phone/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/phone/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/phone/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/phone/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/phone/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/phone/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/printer/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0025.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0026.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0027.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0028.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0029.jpg  \n",
            "  inflating: Office-31/webcam/projector/frame_0030.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0025.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0026.jpg  \n",
            "  inflating: Office-31/webcam/punchers/frame_0027.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0025.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0026.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0027.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0028.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0029.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0030.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0031.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0032.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0033.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0034.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0035.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0036.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0037.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0038.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0039.jpg  \n",
            "  inflating: Office-31/webcam/ring_binder/frame_0040.jpg  \n",
            "  inflating: Office-31/webcam/ruler/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/ruler/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/ruler/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/ruler/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/ruler/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/ruler/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/ruler/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/ruler/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/ruler/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/ruler/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/ruler/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/scissors/frame_0025.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0025.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0026.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0027.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0028.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0029.jpg  \n",
            "  inflating: Office-31/webcam/speaker/frame_0030.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/stapler/frame_0024.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0021.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0022.jpg  \n",
            "  inflating: Office-31/webcam/tape_dispenser/frame_0023.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0001.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0002.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0003.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0004.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0005.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0006.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0007.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0008.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0009.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0010.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0011.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0012.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0013.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0014.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0015.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0016.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0017.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0018.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0019.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0020.jpg  \n",
            "  inflating: Office-31/webcam/trash_can/frame_0021.jpg  \n",
            "cp     LOG_DANN_amazon_to_dslr\t  LOG_DANN_dslr_to_amazon  LOG_DANN_webcam_to_amazon  Office-31\n",
            "drive  LOG_DANN_amazon_to_webcam  LOG_DANN_dslr_to_webcam  LOG_DANN_webcam_to_dslr    sample_data\n",
            "Amazon - Train: 2112, Test: 705\n",
            "DSLR - Train: 373, Test: 125\n",
            "Webcam - Train: 596, Test: 199\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "        [transforms.Resize((image_size, image_size)),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "drive.mount('./drive/')\n",
        "!unzip -o './drive/MyDrive/99H_datasets/Office-31.zip'\n",
        "\n",
        "dataset_path = './Office-31'\n",
        "!ls\n",
        "amazon_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'amazon'), transform=transform)\n",
        "dslr_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'dslr'), transform=transform)\n",
        "webcam_dataset = datasets.ImageFolder(root=os.path.join(dataset_path, 'webcam'), transform=transform)\n",
        "\n",
        "def split_dataset(dataset, train_ratio=0.75):\n",
        "    train_size = int(train_ratio * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "amazon_train, amazon_test = split_dataset(amazon_dataset)\n",
        "dslr_train, dslr_test = split_dataset(dslr_dataset)\n",
        "webcam_train, webcam_test = split_dataset(webcam_dataset)\n",
        "\n",
        "amazon_loader = []\n",
        "dslr_loader = []\n",
        "webcam_loader =[]\n",
        "\n",
        "amazon_tl = DataLoader(amazon_train, batch_size=batch_size, shuffle=True)\n",
        "amazon_vl = DataLoader(amazon_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "dslr_tl = DataLoader(dslr_train, batch_size=batch_size, shuffle=True)\n",
        "dslr_vl = DataLoader(dslr_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "webcam_tl = DataLoader(webcam_train, batch_size=batch_size, shuffle=True)\n",
        "webcam_vl = DataLoader(webcam_test, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "amazon_loader.append(amazon_tl)\n",
        "amazon_loader.append(amazon_vl)\n",
        "\n",
        "dslr_loader.append(dslr_tl)\n",
        "dslr_loader.append(dslr_vl)\n",
        "\n",
        "webcam_loader.append(webcam_tl)\n",
        "webcam_loader.append(webcam_vl)\n",
        "\n",
        "\n",
        "\n",
        "print(f'Amazon - Train: {len(amazon_train)}, Test: {len(amazon_test)}')\n",
        "print(f'DSLR - Train: {len(dslr_train)}, Test: {len(dslr_test)}')\n",
        "print(f'Webcam - Train: {len(webcam_train)}, Test: {len(webcam_test)}')\n",
        "\n",
        "def switch_ds(ds):\n",
        "  training_loader = ds[0]\n",
        "  validation_loader = ds[1]\n",
        "\n",
        "training_loader = amazon_loader[0]\n",
        "validation_loader = amazon_loader[1]\n",
        "\n",
        "switch_ds(amazon_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uXLV78uv4Ig"
      },
      "source": [
        "# Model Declaration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T9T2tEvfhu6"
      },
      "source": [
        "## MobileNetV3-Small with 31 class head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug8UUJ8ffqLZ"
      },
      "outputs": [],
      "source": [
        "def get_model_SO():\n",
        "    model = models.mobilenet_v3_small(pretrained=True)\n",
        "    print(\"Original Classifier:\", model.classifier)\n",
        "    model.classifier[-1] = nn.Linear(in_features=1024, out_features=31, bias=True)\n",
        "    print(\"Mod Classifier:\", model.classifier)\n",
        "    model.to(device)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhMAxNrk_vAV",
        "outputId": "7238d08b-9dcf-43cd-bbcf-13488da4e604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
            ")\n",
            "Mod Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=31, bias=True)\n",
            ")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MobileNetV3(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2dNormActivation(\n",
              "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "      (2): Hardswish()\n",
              "    )\n",
              "    (1): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
              "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
              "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
              "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
              "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
              "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (6): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
              "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
              "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (10): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (11): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (12): Conv2dNormActivation(\n",
              "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "      (2): Hardswish()\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
              "    (1): Hardswish()\n",
              "    (2): Dropout(p=0.2, inplace=True)\n",
              "    (3): Linear(in_features=1024, out_features=31, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_model_SO()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jpdn0ZZrgqXR"
      },
      "source": [
        "## MobileNetV3-Small w/ DANN (Teacher Ver.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loFcSSmGgxZs"
      },
      "outputs": [],
      "source": [
        "class GRL(Function):        #Gradient Reversal Layer Function\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lamda):\n",
        "        # store context for backprop\n",
        "        ctx.lamda = lamda\n",
        "\n",
        "        # forward pass is a no-op\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    # upstream end and result the loss then send back to backward pass\n",
        "    def backward(ctx, grad_output):\n",
        "        # grad_output is dL/dx (since our forward's output was x)\n",
        "\n",
        "        # Backward pass is just to -lamda the gradient\n",
        "        # This will become the dL/dx in the rest of the network\n",
        "        output = -ctx.lamda * grad_output\n",
        "\n",
        "        # Must return number of inputs to forward()\n",
        "        return output, None\n",
        "\n",
        "class DANNMBNv3s(nn.Module):\n",
        "    def __init__(self, n_C=31):\n",
        "        super(DANNMBNv3s, self).__init__()\n",
        "\n",
        "        self.feature_extractor = models.mobilenet_v3_small(pretrained=True)\n",
        "        self.feature_extractor.classifier = nn.Identity()\n",
        "\n",
        "        self.num_cnn_features = self._get_num_features()\n",
        "\n",
        "        # Label classification (blue section)\n",
        "        self.class_classifier = nn.Sequential(\n",
        "            nn.Linear(self.num_cnn_features, 1024),\n",
        "            nn.Hardswish(inplace=True),\n",
        "            nn.Dropout(p=0.2, inplace=True),\n",
        "            nn.Linear(1024, 31),\n",
        "            nn.LogSoftmax(dim=1),\n",
        "        )\n",
        "        self.domain_classifier = nn.Sequential(\n",
        "            nn.Linear(self.num_cnn_features, 1024),\n",
        "            nn.Hardswish(inplace=True),\n",
        "            nn.Linear(1024, 2),\n",
        "            nn.LogSoftmax(dim=1),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, grl_lambda=1.0):\n",
        "        if x.shape[1] == 1:\n",
        "            x = x.repeat(1, 3, 1, 1)\n",
        "        elif x.shape[1] != 3:\n",
        "            raise ValueError(\"Input images must have 3 channels\")\n",
        "\n",
        "        features = self.feature_extractor(x)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        reverse_features = GRL.apply(features, grl_lambda)\n",
        "        domain_prediction = self.domain_classifier(reverse_features)\n",
        "        class_prediction = self.class_classifier(features)\n",
        "        return class_prediction, domain_prediction\n",
        "    def _get_num_features(self):\n",
        "        \"\"\"Helper method to calculate the number of output features from the feature extractor.\"\"\"\n",
        "        dummy_input = torch.randn(1, 3, 224, 224)  # Assuming input size is 224x224\n",
        "        features = self.feature_extractor(dummy_input)\n",
        "        return features.view(1, -1).size(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Upqq8NnOLs_"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLoBd1xCgo6S"
      },
      "source": [
        "## Source Only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMURIYy0g5CZ"
      },
      "source": [
        "### Traing Loop Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5WdBJW6_zKc"
      },
      "outputs": [],
      "source": [
        "def create_balanced_dataloaders(src_dataloader, tar_dataloader, batch_size, blur_sigma=2):\n",
        "    src_size = len(src_dataloader.dataset)\n",
        "    tar_size = len(tar_dataloader.dataset)\n",
        "\n",
        "    # Define a blur transformation\n",
        "    blur_transform = transforms.GaussianBlur(kernel_size=(5, 5), sigma=blur_sigma)\n",
        "\n",
        "    def blur_images(batch):\n",
        "        images, labels = batch\n",
        "        # Apply blur to images\n",
        "        blurred_images = blur_transform(images)\n",
        "        return blurred_images, labels\n",
        "\n",
        "    # Ensure iterators are reset appropriately for smaller dataset\n",
        "    if src_size > tar_size:\n",
        "        Dl_source_iter = iter(src_dataloader)\n",
        "        Dl_target_iter = itertools.cycle(tar_dataloader)  # Cycle the smaller dataset\n",
        "        max_batches = len(src_dataloader)\n",
        "\n",
        "        # Apply blur to the cycled target images\n",
        "        Dl_target_iter = (blur_images(batch) for batch in Dl_target_iter)\n",
        "    else:\n",
        "        Dl_source_iter = itertools.cycle(src_dataloader)  # Cycle the smaller dataset\n",
        "        Dl_target_iter = iter(tar_dataloader)\n",
        "        max_batches = len(tar_dataloader)\n",
        "\n",
        "        # Apply blur to the cycled source images\n",
        "        Dl_source_iter = (blur_images(batch) for batch in Dl_source_iter)\n",
        "\n",
        "    return Dl_source_iter, Dl_target_iter, max_batches\n",
        "\n",
        "\n",
        "def train_model_with_balanced_batches(model, optimizer, loss_fn_class, src, tar, device, num_epochs=30, batch_size=64):\n",
        "    training_logs_so = {\"train_loss\": [],  \"train_src_acc\": [], \"train_tar_acc\": []}\n",
        "\n",
        "    src_dataloader, tar_dataloader = get_loader_DA(src, tar)\n",
        "    model_save_path = f\"SO_{src}_to_{tar}.pth\"\n",
        "    best_vloss = 1_000_000.\n",
        "    for epoch_idx in range(num_epochs):\n",
        "        train_loss, train_src_correct, train_tar_correct = 0, 0, 0\n",
        "        actual_src_samples, actual_tar_samples = 0, 0  # Track the actual number of processed samples\n",
        "        print(f'Epoch {epoch_idx+1:04d} / {num_epochs:04d}', end='\\n============\\n')\n",
        "\n",
        "        Dl_source_iter, Dl_target_iter, max_batches = create_balanced_dataloaders(src_dataloader, tar_dataloader, batch_size)\n",
        "\n",
        "        for batch_idx in range(max_batches):\n",
        "            # Get next batch from source and target\n",
        "            X_s, y_s = next(Dl_source_iter)\n",
        "            X_t, y_t = next(Dl_target_iter)\n",
        "\n",
        "            # Ensure the batches have the same size\n",
        "            if X_s.shape[0] != X_t.shape[0]:\n",
        "                min_bs = min(X_s.shape[0], X_t.shape[0])\n",
        "                X_s, y_s = X_s[:min_bs], y_s[:min_bs]\n",
        "                X_t, y_t = X_t[:min_bs], y_t[:min_bs]\n",
        "\n",
        "            # Track the number of samples processed\n",
        "            actual_src_samples += y_s.size(0)\n",
        "            actual_tar_samples += y_t.size(0)\n",
        "\n",
        "            # Send data to device (GPU or CPU)\n",
        "            X_s, y_s = X_s.to(device), y_s.to(device)\n",
        "            X_t, y_t = X_t.to(device), y_t.to(device)\n",
        "\n",
        "            # Reset gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            class_prediction_s = model(X_s)\n",
        "            loss = loss_fn_class(class_prediction_s, y_s)  # Calculate loss on source data\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Calculate source and target accuracy\n",
        "            with torch.no_grad():\n",
        "                class_prediction_t = model(X_t)\n",
        "                train_src_correct += (class_prediction_s.argmax(1) == y_s).float().sum().item()\n",
        "                train_tar_correct += (class_prediction_t.argmax(1) == y_t).float().sum().item()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Print batch info\n",
        "            print(f'[{batch_idx+1}/{max_batches}] class loss: {loss.item():.4f}')\n",
        "\n",
        "        # Calculate and store metrics for the epoch\n",
        "        training_logs_so[\"train_loss\"].append(train_loss / max_batches)\n",
        "        training_logs_so[\"train_src_acc\"].append(train_src_correct / actual_src_samples)  # Use actual number of samples\n",
        "        training_logs_so[\"train_tar_acc\"].append(train_tar_correct / actual_tar_samples)  # Use actual number of samples\n",
        "\n",
        "        print(f'Epoch: {epoch_idx+1} || '\n",
        "              f'Train_src_acc: {train_src_correct / actual_src_samples:.4f}, '  # Display as fraction\n",
        "              f'Train_tar_acc: {train_tar_correct / actual_tar_samples:.4f}, '\n",
        "              f'Train_loss: {train_loss / max_batches:.4f}')\n",
        "        if train_loss < best_vloss:\n",
        "            best_vloss = train_loss\n",
        "            path_save_cp = './cp/'\n",
        "            if not os.path.exists(path_save_cp): os.mkdir(path_save_cp)\n",
        "            torch.save(model.state_dict(), path_save_cp+model_save_path)\n",
        "\n",
        "\n",
        "    save_training_logs(training_logs_so, f\"LOG_SO_{src}_to_{tar}\")\n",
        "    return training_logs_so"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I47wrXNg91r"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "Luw7msF9gvxd",
        "outputId": "ee8a0d4e-74d4-47a4-9dbd-e4ad9b3f0d8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
            ")\n",
            "Mod Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=31, bias=True)\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 16, 112, 112]             432\n",
            "       BatchNorm2d-2         [-1, 16, 112, 112]              32\n",
            "         Hardswish-3         [-1, 16, 112, 112]               0\n",
            "            Conv2d-4           [-1, 16, 56, 56]             144\n",
            "       BatchNorm2d-5           [-1, 16, 56, 56]              32\n",
            "              ReLU-6           [-1, 16, 56, 56]               0\n",
            " AdaptiveAvgPool2d-7             [-1, 16, 1, 1]               0\n",
            "            Conv2d-8              [-1, 8, 1, 1]             136\n",
            "              ReLU-9              [-1, 8, 1, 1]               0\n",
            "           Conv2d-10             [-1, 16, 1, 1]             144\n",
            "      Hardsigmoid-11             [-1, 16, 1, 1]               0\n",
            "SqueezeExcitation-12           [-1, 16, 56, 56]               0\n",
            "           Conv2d-13           [-1, 16, 56, 56]             256\n",
            "      BatchNorm2d-14           [-1, 16, 56, 56]              32\n",
            " InvertedResidual-15           [-1, 16, 56, 56]               0\n",
            "           Conv2d-16           [-1, 72, 56, 56]           1,152\n",
            "      BatchNorm2d-17           [-1, 72, 56, 56]             144\n",
            "             ReLU-18           [-1, 72, 56, 56]               0\n",
            "           Conv2d-19           [-1, 72, 28, 28]             648\n",
            "      BatchNorm2d-20           [-1, 72, 28, 28]             144\n",
            "             ReLU-21           [-1, 72, 28, 28]               0\n",
            "           Conv2d-22           [-1, 24, 28, 28]           1,728\n",
            "      BatchNorm2d-23           [-1, 24, 28, 28]              48\n",
            " InvertedResidual-24           [-1, 24, 28, 28]               0\n",
            "           Conv2d-25           [-1, 88, 28, 28]           2,112\n",
            "      BatchNorm2d-26           [-1, 88, 28, 28]             176\n",
            "             ReLU-27           [-1, 88, 28, 28]               0\n",
            "           Conv2d-28           [-1, 88, 28, 28]             792\n",
            "      BatchNorm2d-29           [-1, 88, 28, 28]             176\n",
            "             ReLU-30           [-1, 88, 28, 28]               0\n",
            "           Conv2d-31           [-1, 24, 28, 28]           2,112\n",
            "      BatchNorm2d-32           [-1, 24, 28, 28]              48\n",
            " InvertedResidual-33           [-1, 24, 28, 28]               0\n",
            "           Conv2d-34           [-1, 96, 28, 28]           2,304\n",
            "      BatchNorm2d-35           [-1, 96, 28, 28]             192\n",
            "        Hardswish-36           [-1, 96, 28, 28]               0\n",
            "           Conv2d-37           [-1, 96, 14, 14]           2,400\n",
            "      BatchNorm2d-38           [-1, 96, 14, 14]             192\n",
            "        Hardswish-39           [-1, 96, 14, 14]               0\n",
            "AdaptiveAvgPool2d-40             [-1, 96, 1, 1]               0\n",
            "           Conv2d-41             [-1, 24, 1, 1]           2,328\n",
            "             ReLU-42             [-1, 24, 1, 1]               0\n",
            "           Conv2d-43             [-1, 96, 1, 1]           2,400\n",
            "      Hardsigmoid-44             [-1, 96, 1, 1]               0\n",
            "SqueezeExcitation-45           [-1, 96, 14, 14]               0\n",
            "           Conv2d-46           [-1, 40, 14, 14]           3,840\n",
            "      BatchNorm2d-47           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-48           [-1, 40, 14, 14]               0\n",
            "           Conv2d-49          [-1, 240, 14, 14]           9,600\n",
            "      BatchNorm2d-50          [-1, 240, 14, 14]             480\n",
            "        Hardswish-51          [-1, 240, 14, 14]               0\n",
            "           Conv2d-52          [-1, 240, 14, 14]           6,000\n",
            "      BatchNorm2d-53          [-1, 240, 14, 14]             480\n",
            "        Hardswish-54          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-55            [-1, 240, 1, 1]               0\n",
            "           Conv2d-56             [-1, 64, 1, 1]          15,424\n",
            "             ReLU-57             [-1, 64, 1, 1]               0\n",
            "           Conv2d-58            [-1, 240, 1, 1]          15,600\n",
            "      Hardsigmoid-59            [-1, 240, 1, 1]               0\n",
            "SqueezeExcitation-60          [-1, 240, 14, 14]               0\n",
            "           Conv2d-61           [-1, 40, 14, 14]           9,600\n",
            "      BatchNorm2d-62           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-63           [-1, 40, 14, 14]               0\n",
            "           Conv2d-64          [-1, 240, 14, 14]           9,600\n",
            "      BatchNorm2d-65          [-1, 240, 14, 14]             480\n",
            "        Hardswish-66          [-1, 240, 14, 14]               0\n",
            "           Conv2d-67          [-1, 240, 14, 14]           6,000\n",
            "      BatchNorm2d-68          [-1, 240, 14, 14]             480\n",
            "        Hardswish-69          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-70            [-1, 240, 1, 1]               0\n",
            "           Conv2d-71             [-1, 64, 1, 1]          15,424\n",
            "             ReLU-72             [-1, 64, 1, 1]               0\n",
            "           Conv2d-73            [-1, 240, 1, 1]          15,600\n",
            "      Hardsigmoid-74            [-1, 240, 1, 1]               0\n",
            "SqueezeExcitation-75          [-1, 240, 14, 14]               0\n",
            "           Conv2d-76           [-1, 40, 14, 14]           9,600\n",
            "      BatchNorm2d-77           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-78           [-1, 40, 14, 14]               0\n",
            "           Conv2d-79          [-1, 120, 14, 14]           4,800\n",
            "      BatchNorm2d-80          [-1, 120, 14, 14]             240\n",
            "        Hardswish-81          [-1, 120, 14, 14]               0\n",
            "           Conv2d-82          [-1, 120, 14, 14]           3,000\n",
            "      BatchNorm2d-83          [-1, 120, 14, 14]             240\n",
            "        Hardswish-84          [-1, 120, 14, 14]               0\n",
            "AdaptiveAvgPool2d-85            [-1, 120, 1, 1]               0\n",
            "           Conv2d-86             [-1, 32, 1, 1]           3,872\n",
            "             ReLU-87             [-1, 32, 1, 1]               0\n",
            "           Conv2d-88            [-1, 120, 1, 1]           3,960\n",
            "      Hardsigmoid-89            [-1, 120, 1, 1]               0\n",
            "SqueezeExcitation-90          [-1, 120, 14, 14]               0\n",
            "           Conv2d-91           [-1, 48, 14, 14]           5,760\n",
            "      BatchNorm2d-92           [-1, 48, 14, 14]              96\n",
            " InvertedResidual-93           [-1, 48, 14, 14]               0\n",
            "           Conv2d-94          [-1, 144, 14, 14]           6,912\n",
            "      BatchNorm2d-95          [-1, 144, 14, 14]             288\n",
            "        Hardswish-96          [-1, 144, 14, 14]               0\n",
            "           Conv2d-97          [-1, 144, 14, 14]           3,600\n",
            "      BatchNorm2d-98          [-1, 144, 14, 14]             288\n",
            "        Hardswish-99          [-1, 144, 14, 14]               0\n",
            "AdaptiveAvgPool2d-100            [-1, 144, 1, 1]               0\n",
            "          Conv2d-101             [-1, 40, 1, 1]           5,800\n",
            "            ReLU-102             [-1, 40, 1, 1]               0\n",
            "          Conv2d-103            [-1, 144, 1, 1]           5,904\n",
            "     Hardsigmoid-104            [-1, 144, 1, 1]               0\n",
            "SqueezeExcitation-105          [-1, 144, 14, 14]               0\n",
            "          Conv2d-106           [-1, 48, 14, 14]           6,912\n",
            "     BatchNorm2d-107           [-1, 48, 14, 14]              96\n",
            "InvertedResidual-108           [-1, 48, 14, 14]               0\n",
            "          Conv2d-109          [-1, 288, 14, 14]          13,824\n",
            "     BatchNorm2d-110          [-1, 288, 14, 14]             576\n",
            "       Hardswish-111          [-1, 288, 14, 14]               0\n",
            "          Conv2d-112            [-1, 288, 7, 7]           7,200\n",
            "     BatchNorm2d-113            [-1, 288, 7, 7]             576\n",
            "       Hardswish-114            [-1, 288, 7, 7]               0\n",
            "AdaptiveAvgPool2d-115            [-1, 288, 1, 1]               0\n",
            "          Conv2d-116             [-1, 72, 1, 1]          20,808\n",
            "            ReLU-117             [-1, 72, 1, 1]               0\n",
            "          Conv2d-118            [-1, 288, 1, 1]          21,024\n",
            "     Hardsigmoid-119            [-1, 288, 1, 1]               0\n",
            "SqueezeExcitation-120            [-1, 288, 7, 7]               0\n",
            "          Conv2d-121             [-1, 96, 7, 7]          27,648\n",
            "     BatchNorm2d-122             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-123             [-1, 96, 7, 7]               0\n",
            "          Conv2d-124            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-125            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-126            [-1, 576, 7, 7]               0\n",
            "          Conv2d-127            [-1, 576, 7, 7]          14,400\n",
            "     BatchNorm2d-128            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-129            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-130            [-1, 576, 1, 1]               0\n",
            "          Conv2d-131            [-1, 144, 1, 1]          83,088\n",
            "            ReLU-132            [-1, 144, 1, 1]               0\n",
            "          Conv2d-133            [-1, 576, 1, 1]          83,520\n",
            "     Hardsigmoid-134            [-1, 576, 1, 1]               0\n",
            "SqueezeExcitation-135            [-1, 576, 7, 7]               0\n",
            "          Conv2d-136             [-1, 96, 7, 7]          55,296\n",
            "     BatchNorm2d-137             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-138             [-1, 96, 7, 7]               0\n",
            "          Conv2d-139            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-140            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-141            [-1, 576, 7, 7]               0\n",
            "          Conv2d-142            [-1, 576, 7, 7]          14,400\n",
            "     BatchNorm2d-143            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-144            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-145            [-1, 576, 1, 1]               0\n",
            "          Conv2d-146            [-1, 144, 1, 1]          83,088\n",
            "            ReLU-147            [-1, 144, 1, 1]               0\n",
            "          Conv2d-148            [-1, 576, 1, 1]          83,520\n",
            "     Hardsigmoid-149            [-1, 576, 1, 1]               0\n",
            "SqueezeExcitation-150            [-1, 576, 7, 7]               0\n",
            "          Conv2d-151             [-1, 96, 7, 7]          55,296\n",
            "     BatchNorm2d-152             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-153             [-1, 96, 7, 7]               0\n",
            "          Conv2d-154            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-155            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-156            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-157            [-1, 576, 1, 1]               0\n",
            "          Linear-158                 [-1, 1024]         590,848\n",
            "       Hardswish-159                 [-1, 1024]               0\n",
            "         Dropout-160                 [-1, 1024]               0\n",
            "          Linear-161                   [-1, 31]          31,775\n",
            "================================================================\n",
            "Total params: 1,549,631\n",
            "Trainable params: 1,549,631\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 34.60\n",
            "Params size (MB): 5.91\n",
            "Estimated Total Size (MB): 41.09\n",
            "----------------------------------------------------------------\n",
            "Source and Target are the same. Skiping...\n",
            "Original Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
            ")\n",
            "Mod Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=31, bias=True)\n",
            ")\n",
            "Epoch 0001 / 0050\n",
            "============\n",
            "[1/33] class loss: 3.5767\n",
            "[2/33] class loss: 3.5533\n",
            "[3/33] class loss: 3.4511\n",
            "[4/33] class loss: 3.5127\n",
            "[5/33] class loss: 3.3671\n",
            "[6/33] class loss: 3.4298\n",
            "[7/33] class loss: 3.3734\n",
            "[8/33] class loss: 3.4078\n",
            "[9/33] class loss: 3.4549\n",
            "[10/33] class loss: 3.4535\n",
            "[11/33] class loss: 3.3720\n",
            "[12/33] class loss: 3.3394\n",
            "[13/33] class loss: 3.3907\n",
            "[14/33] class loss: 3.3465\n",
            "[15/33] class loss: 3.4210\n",
            "[16/33] class loss: 3.3622\n",
            "[17/33] class loss: 3.3455\n",
            "[18/33] class loss: 3.3098\n",
            "[19/33] class loss: 3.3528\n",
            "[20/33] class loss: 3.2898\n",
            "[21/33] class loss: 3.3068\n",
            "[22/33] class loss: 3.3686\n",
            "[23/33] class loss: 3.2637\n",
            "[24/33] class loss: 3.2369\n",
            "[25/33] class loss: 3.2651\n",
            "[26/33] class loss: 3.3462\n",
            "[27/33] class loss: 3.2224\n",
            "[28/33] class loss: 3.2799\n",
            "[29/33] class loss: 3.1918\n",
            "[30/33] class loss: 3.2760\n",
            "[31/33] class loss: 3.1971\n",
            "[32/33] class loss: 3.2097\n",
            "[33/33] class loss: 3.2629\n",
            "Epoch: 1 || Train_src_acc: 0.0924, Train_tar_acc: 0.0613, Train_loss: 3.3496\n",
            "Epoch 0002 / 0050\n",
            "============\n",
            "[1/33] class loss: 3.1900\n",
            "[2/33] class loss: 3.1700\n",
            "[3/33] class loss: 3.1944\n",
            "[4/33] class loss: 3.1816\n",
            "[5/33] class loss: 3.1038\n",
            "[6/33] class loss: 3.0425\n",
            "[7/33] class loss: 3.1012\n",
            "[8/33] class loss: 3.0661\n",
            "[9/33] class loss: 3.0101\n",
            "[10/33] class loss: 3.1564\n",
            "[11/33] class loss: 3.1463\n",
            "[12/33] class loss: 3.0814\n",
            "[13/33] class loss: 3.0520\n",
            "[14/33] class loss: 3.0497\n",
            "[15/33] class loss: 3.0475\n",
            "[16/33] class loss: 3.0389\n",
            "[17/33] class loss: 3.0288\n",
            "[18/33] class loss: 3.1100\n",
            "[19/33] class loss: 3.0005\n",
            "[20/33] class loss: 2.9828\n",
            "[21/33] class loss: 2.9483\n",
            "[22/33] class loss: 3.0194\n",
            "[23/33] class loss: 3.0661\n",
            "[24/33] class loss: 2.9595\n",
            "[25/33] class loss: 3.0351\n",
            "[26/33] class loss: 3.0414\n",
            "[27/33] class loss: 3.0193\n",
            "[28/33] class loss: 2.8317\n",
            "[29/33] class loss: 2.8917\n",
            "[30/33] class loss: 2.9315\n",
            "[31/33] class loss: 2.8729\n",
            "[32/33] class loss: 2.8537\n",
            "[33/33] class loss: 2.8463\n",
            "Epoch: 2 || Train_src_acc: 0.2645, Train_tar_acc: 0.1196, Train_loss: 3.0325\n",
            "Epoch 0003 / 0050\n",
            "============\n",
            "[1/33] class loss: 2.8908\n",
            "[2/33] class loss: 2.8564\n",
            "[3/33] class loss: 2.9339\n",
            "[4/33] class loss: 2.8571\n",
            "[5/33] class loss: 2.8670\n",
            "[6/33] class loss: 2.8805\n",
            "[7/33] class loss: 2.7987\n",
            "[8/33] class loss: 2.7609\n",
            "[9/33] class loss: 2.6144\n",
            "[10/33] class loss: 2.8312\n",
            "[11/33] class loss: 2.7435\n",
            "[12/33] class loss: 2.8209\n",
            "[13/33] class loss: 2.6880\n",
            "[14/33] class loss: 2.8252\n",
            "[15/33] class loss: 2.8089\n",
            "[16/33] class loss: 2.7646\n",
            "[17/33] class loss: 2.6681\n",
            "[18/33] class loss: 2.7747\n",
            "[19/33] class loss: 2.6564\n",
            "[20/33] class loss: 2.6817\n",
            "[21/33] class loss: 2.7290\n",
            "[22/33] class loss: 2.7155\n",
            "[23/33] class loss: 2.7388\n",
            "[24/33] class loss: 2.6837\n",
            "[25/33] class loss: 2.6067\n",
            "[26/33] class loss: 2.7615\n",
            "[27/33] class loss: 2.5569\n",
            "[28/33] class loss: 2.7303\n",
            "[29/33] class loss: 2.5695\n",
            "[30/33] class loss: 2.6056\n",
            "[31/33] class loss: 2.4686\n",
            "[32/33] class loss: 2.7264\n",
            "[33/33] class loss: 2.3869\n",
            "Epoch: 3 || Train_src_acc: 0.4283, Train_tar_acc: 0.1920, Train_loss: 2.7273\n",
            "Epoch 0004 / 0050\n",
            "============\n",
            "[1/33] class loss: 2.6080\n",
            "[2/33] class loss: 2.5062\n",
            "[3/33] class loss: 2.5314\n",
            "[4/33] class loss: 2.5070\n",
            "[5/33] class loss: 2.5572\n",
            "[6/33] class loss: 2.3683\n",
            "[7/33] class loss: 2.2628\n",
            "[8/33] class loss: 2.6057\n",
            "[9/33] class loss: 2.4069\n",
            "[10/33] class loss: 2.5379\n",
            "[11/33] class loss: 2.5378\n",
            "[12/33] class loss: 2.3278\n",
            "[13/33] class loss: 2.3945\n",
            "[14/33] class loss: 2.4836\n",
            "[15/33] class loss: 2.2197\n",
            "[16/33] class loss: 2.4014\n",
            "[17/33] class loss: 2.5195\n",
            "[18/33] class loss: 2.3783\n",
            "[19/33] class loss: 2.3375\n",
            "[20/33] class loss: 2.3170\n",
            "[21/33] class loss: 2.3376\n",
            "[22/33] class loss: 2.4205\n",
            "[23/33] class loss: 2.3969\n",
            "[24/33] class loss: 2.2805\n",
            "[25/33] class loss: 2.2868\n",
            "[26/33] class loss: 2.3828\n",
            "[27/33] class loss: 2.2641\n",
            "[28/33] class loss: 2.2496\n",
            "[29/33] class loss: 2.3676\n",
            "[30/33] class loss: 2.3258\n",
            "[31/33] class loss: 2.3905\n",
            "[32/33] class loss: 2.5046\n",
            "[33/33] class loss: 2.0178\n",
            "Epoch: 4 || Train_src_acc: 0.5338, Train_tar_acc: 0.2620, Train_loss: 2.3950\n",
            "Epoch 0005 / 0050\n",
            "============\n",
            "[1/33] class loss: 2.1323\n",
            "[2/33] class loss: 2.3143\n",
            "[3/33] class loss: 2.0251\n",
            "[4/33] class loss: 2.3118\n",
            "[5/33] class loss: 2.0193\n",
            "[6/33] class loss: 2.2052\n",
            "[7/33] class loss: 2.1779\n",
            "[8/33] class loss: 2.1163\n",
            "[9/33] class loss: 2.1837\n",
            "[10/33] class loss: 2.1681\n",
            "[11/33] class loss: 2.1649\n",
            "[12/33] class loss: 1.9325\n",
            "[13/33] class loss: 2.2444\n",
            "[14/33] class loss: 2.0663\n",
            "[15/33] class loss: 2.0043\n",
            "[16/33] class loss: 2.0643\n",
            "[17/33] class loss: 1.9473\n",
            "[18/33] class loss: 2.0933\n",
            "[19/33] class loss: 2.1153\n",
            "[20/33] class loss: 2.0357\n",
            "[21/33] class loss: 2.1226\n",
            "[22/33] class loss: 2.0496\n",
            "[23/33] class loss: 2.0381\n",
            "[24/33] class loss: 2.0578\n",
            "[25/33] class loss: 1.7848\n",
            "[26/33] class loss: 1.8758\n",
            "[27/33] class loss: 1.9931\n",
            "[28/33] class loss: 1.9596\n",
            "[29/33] class loss: 2.1361\n",
            "[30/33] class loss: 2.0037\n",
            "[31/33] class loss: 2.0325\n",
            "[32/33] class loss: 1.9780\n",
            "[33/33] class loss: 2.0368\n",
            "Epoch: 5 || Train_src_acc: 0.5950, Train_tar_acc: 0.3106, Train_loss: 2.0724\n",
            "Epoch 0006 / 0050\n",
            "============\n",
            "[1/33] class loss: 1.8406\n",
            "[2/33] class loss: 1.8839\n",
            "[3/33] class loss: 1.8310\n",
            "[4/33] class loss: 1.8243\n",
            "[5/33] class loss: 1.9101\n",
            "[6/33] class loss: 2.1253\n",
            "[7/33] class loss: 2.0775\n",
            "[8/33] class loss: 1.7648\n",
            "[9/33] class loss: 1.7565\n",
            "[10/33] class loss: 1.9356\n",
            "[11/33] class loss: 1.8574\n",
            "[12/33] class loss: 1.9293\n",
            "[13/33] class loss: 1.6527\n",
            "[14/33] class loss: 1.8738\n",
            "[15/33] class loss: 1.8987\n",
            "[16/33] class loss: 1.7041\n",
            "[17/33] class loss: 1.9440\n",
            "[18/33] class loss: 1.7000\n",
            "[19/33] class loss: 1.6657\n",
            "[20/33] class loss: 1.8090\n",
            "[21/33] class loss: 1.9375\n",
            "[22/33] class loss: 1.6402\n",
            "[23/33] class loss: 1.6119\n",
            "[24/33] class loss: 1.7263\n",
            "[25/33] class loss: 1.6509\n",
            "[26/33] class loss: 1.6479\n",
            "[27/33] class loss: 1.7394\n",
            "[28/33] class loss: 1.7382\n",
            "[29/33] class loss: 1.7172\n",
            "[30/33] class loss: 1.5519\n",
            "[31/33] class loss: 1.7228\n",
            "[32/33] class loss: 1.5199\n",
            "[33/33] class loss: 1.5000\n",
            "Epoch: 6 || Train_src_acc: 0.6558, Train_tar_acc: 0.3695, Train_loss: 1.7784\n",
            "Epoch 0007 / 0050\n",
            "============\n",
            "[1/33] class loss: 1.5005\n",
            "[2/33] class loss: 1.5974\n",
            "[3/33] class loss: 1.6283\n",
            "[4/33] class loss: 1.7169\n",
            "[5/33] class loss: 1.7419\n",
            "[6/33] class loss: 1.5816\n",
            "[7/33] class loss: 1.6170\n",
            "[8/33] class loss: 1.6065\n",
            "[9/33] class loss: 1.5860\n",
            "[10/33] class loss: 1.4593\n",
            "[11/33] class loss: 1.5317\n",
            "[12/33] class loss: 1.5197\n",
            "[13/33] class loss: 1.5568\n",
            "[14/33] class loss: 1.8063\n",
            "[15/33] class loss: 1.6290\n",
            "[16/33] class loss: 1.4785\n",
            "[17/33] class loss: 1.5254\n",
            "[18/33] class loss: 1.4212\n",
            "[19/33] class loss: 1.4969\n",
            "[20/33] class loss: 1.5853\n",
            "[21/33] class loss: 1.5362\n",
            "[22/33] class loss: 1.4126\n",
            "[23/33] class loss: 1.3949\n",
            "[24/33] class loss: 1.4619\n",
            "[25/33] class loss: 1.4590\n",
            "[26/33] class loss: 1.4345\n",
            "[27/33] class loss: 1.3837\n",
            "[28/33] class loss: 1.2502\n",
            "[29/33] class loss: 1.4258\n",
            "[30/33] class loss: 1.5335\n",
            "[31/33] class loss: 1.2327\n",
            "[32/33] class loss: 1.3278\n",
            "[33/33] class loss: 1.5644\n",
            "Epoch: 7 || Train_src_acc: 0.7000, Train_tar_acc: 0.3865, Train_loss: 1.5153\n",
            "Epoch 0008 / 0050\n",
            "============\n",
            "[1/33] class loss: 1.4246\n",
            "[2/33] class loss: 1.4372\n",
            "[3/33] class loss: 1.5709\n",
            "[4/33] class loss: 1.3234\n",
            "[5/33] class loss: 1.2998\n",
            "[6/33] class loss: 1.4228\n",
            "[7/33] class loss: 1.5377\n",
            "[8/33] class loss: 1.2530\n",
            "[9/33] class loss: 1.3834\n",
            "[10/33] class loss: 1.1795\n",
            "[11/33] class loss: 1.1589\n",
            "[12/33] class loss: 1.2099\n",
            "[13/33] class loss: 1.2294\n",
            "[14/33] class loss: 1.2756\n",
            "[15/33] class loss: 1.1760\n",
            "[16/33] class loss: 1.5199\n",
            "[17/33] class loss: 1.4503\n",
            "[18/33] class loss: 1.3810\n",
            "[19/33] class loss: 1.0893\n",
            "[20/33] class loss: 1.4086\n",
            "[21/33] class loss: 1.0428\n",
            "[22/33] class loss: 1.3228\n",
            "[23/33] class loss: 1.3248\n",
            "[24/33] class loss: 1.3952\n",
            "[25/33] class loss: 1.2016\n",
            "[26/33] class loss: 1.2776\n",
            "[27/33] class loss: 1.0528\n",
            "[28/33] class loss: 1.2370\n",
            "[29/33] class loss: 1.1156\n",
            "[30/33] class loss: 1.2464\n",
            "[31/33] class loss: 1.1992\n",
            "[32/33] class loss: 1.2066\n",
            "[33/33] class loss: 1.0725\n",
            "Epoch: 8 || Train_src_acc: 0.7399, Train_tar_acc: 0.4249, Train_loss: 1.2856\n",
            "Epoch 0009 / 0050\n",
            "============\n",
            "[1/33] class loss: 1.2059\n",
            "[2/33] class loss: 1.0191\n",
            "[3/33] class loss: 1.1508\n",
            "[4/33] class loss: 1.0370\n",
            "[5/33] class loss: 1.1919\n",
            "[6/33] class loss: 1.3065\n",
            "[7/33] class loss: 1.1296\n",
            "[8/33] class loss: 1.0523\n",
            "[9/33] class loss: 1.0566\n",
            "[10/33] class loss: 1.3663\n",
            "[11/33] class loss: 1.1276\n",
            "[12/33] class loss: 0.9740\n",
            "[13/33] class loss: 0.9399\n",
            "[14/33] class loss: 1.0938\n",
            "[15/33] class loss: 1.2003\n",
            "[16/33] class loss: 1.1073\n",
            "[17/33] class loss: 1.3028\n",
            "[18/33] class loss: 1.0705\n",
            "[19/33] class loss: 1.2053\n",
            "[20/33] class loss: 1.0456\n",
            "[21/33] class loss: 1.1582\n",
            "[22/33] class loss: 0.9851\n",
            "[23/33] class loss: 1.0760\n",
            "[24/33] class loss: 1.1904\n",
            "[25/33] class loss: 1.0093\n",
            "[26/33] class loss: 1.2097\n",
            "[27/33] class loss: 1.3057\n",
            "[28/33] class loss: 1.1640\n",
            "[29/33] class loss: 1.1471\n",
            "[30/33] class loss: 1.0236\n",
            "[31/33] class loss: 0.9909\n",
            "[32/33] class loss: 1.1049\n",
            "[33/33] class loss: 0.8879\n",
            "Epoch: 9 || Train_src_acc: 0.7623, Train_tar_acc: 0.4084, Train_loss: 1.1162\n",
            "Epoch 0010 / 0050\n",
            "============\n",
            "[1/33] class loss: 1.0006\n",
            "[2/33] class loss: 1.2247\n",
            "[3/33] class loss: 1.1819\n",
            "[4/33] class loss: 1.0797\n",
            "[5/33] class loss: 0.8033\n",
            "[6/33] class loss: 0.9548\n",
            "[7/33] class loss: 0.9573\n",
            "[8/33] class loss: 0.9375\n",
            "[9/33] class loss: 0.9725\n",
            "[10/33] class loss: 0.9026\n",
            "[11/33] class loss: 1.0237\n",
            "[12/33] class loss: 1.1276\n",
            "[13/33] class loss: 0.9535\n",
            "[14/33] class loss: 0.8828\n",
            "[15/33] class loss: 0.7289\n",
            "[16/33] class loss: 0.8440\n",
            "[17/33] class loss: 1.0666\n",
            "[18/33] class loss: 0.9455\n",
            "[19/33] class loss: 1.2031\n",
            "[20/33] class loss: 0.8602\n",
            "[21/33] class loss: 0.8688\n",
            "[22/33] class loss: 0.8389\n",
            "[23/33] class loss: 1.0040\n",
            "[24/33] class loss: 0.9997\n",
            "[25/33] class loss: 0.9364\n",
            "[26/33] class loss: 1.0338\n",
            "[27/33] class loss: 0.8162\n",
            "[28/33] class loss: 0.9367\n",
            "[29/33] class loss: 0.8081\n",
            "[30/33] class loss: 1.0806\n",
            "[31/33] class loss: 0.9314\n",
            "[32/33] class loss: 0.9806\n",
            "[33/33] class loss: 0.9316\n",
            "Epoch: 10 || Train_src_acc: 0.7963, Train_tar_acc: 0.4516, Train_loss: 0.9642\n",
            "Epoch 0011 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.7752\n",
            "[2/33] class loss: 0.9679\n",
            "[3/33] class loss: 1.0995\n",
            "[4/33] class loss: 0.8705\n",
            "[5/33] class loss: 0.9516\n",
            "[6/33] class loss: 0.7615\n",
            "[7/33] class loss: 1.0449\n",
            "[8/33] class loss: 0.8527\n",
            "[9/33] class loss: 0.7225\n",
            "[10/33] class loss: 0.8780\n",
            "[11/33] class loss: 0.9202\n",
            "[12/33] class loss: 0.9567\n",
            "[13/33] class loss: 0.8925\n",
            "[14/33] class loss: 0.6886\n",
            "[15/33] class loss: 0.9728\n",
            "[16/33] class loss: 0.8136\n",
            "[17/33] class loss: 0.7480\n",
            "[18/33] class loss: 0.9583\n",
            "[19/33] class loss: 0.6570\n",
            "[20/33] class loss: 0.7793\n",
            "[21/33] class loss: 0.9953\n",
            "[22/33] class loss: 0.6603\n",
            "[23/33] class loss: 1.0383\n",
            "[24/33] class loss: 0.8107\n",
            "[25/33] class loss: 0.8806\n",
            "[26/33] class loss: 0.8348\n",
            "[27/33] class loss: 0.8618\n",
            "[28/33] class loss: 0.9083\n",
            "[29/33] class loss: 0.8887\n",
            "[30/33] class loss: 0.6960\n",
            "[31/33] class loss: 0.7882\n",
            "[32/33] class loss: 0.7492\n",
            "[33/33] class loss: 0.7075\n",
            "Epoch: 11 || Train_src_acc: 0.8172, Train_tar_acc: 0.4531, Train_loss: 0.8525\n",
            "Epoch 0012 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.8113\n",
            "[2/33] class loss: 0.6067\n",
            "[3/33] class loss: 0.8100\n",
            "[4/33] class loss: 0.6643\n",
            "[5/33] class loss: 0.7609\n",
            "[6/33] class loss: 1.2206\n",
            "[7/33] class loss: 0.6214\n",
            "[8/33] class loss: 0.7704\n",
            "[9/33] class loss: 0.9803\n",
            "[10/33] class loss: 0.5689\n",
            "[11/33] class loss: 0.8137\n",
            "[12/33] class loss: 0.8645\n",
            "[13/33] class loss: 0.8502\n",
            "[14/33] class loss: 0.8648\n",
            "[15/33] class loss: 0.7985\n",
            "[16/33] class loss: 0.7503\n",
            "[17/33] class loss: 0.7433\n",
            "[18/33] class loss: 0.7862\n",
            "[19/33] class loss: 0.7095\n",
            "[20/33] class loss: 0.8482\n",
            "[21/33] class loss: 0.8344\n",
            "[22/33] class loss: 0.6762\n",
            "[23/33] class loss: 0.6320\n",
            "[24/33] class loss: 0.5917\n",
            "[25/33] class loss: 0.6762\n",
            "[26/33] class loss: 0.6673\n",
            "[27/33] class loss: 0.7868\n",
            "[28/33] class loss: 0.8113\n",
            "[29/33] class loss: 0.5599\n",
            "[30/33] class loss: 0.7302\n",
            "[31/33] class loss: 0.6036\n",
            "[32/33] class loss: 0.7306\n",
            "[33/33] class loss: 0.9324\n",
            "Epoch: 12 || Train_src_acc: 0.8323, Train_tar_acc: 0.4657, Train_loss: 0.7599\n",
            "Epoch 0013 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.5609\n",
            "[2/33] class loss: 0.6822\n",
            "[3/33] class loss: 0.5980\n",
            "[4/33] class loss: 0.7575\n",
            "[5/33] class loss: 0.6446\n",
            "[6/33] class loss: 0.8212\n",
            "[7/33] class loss: 0.8082\n",
            "[8/33] class loss: 0.7165\n",
            "[9/33] class loss: 0.7559\n",
            "[10/33] class loss: 0.5400\n",
            "[11/33] class loss: 0.7148\n",
            "[12/33] class loss: 0.5685\n",
            "[13/33] class loss: 0.6346\n",
            "[14/33] class loss: 0.7186\n",
            "[15/33] class loss: 0.7461\n",
            "[16/33] class loss: 0.6440\n",
            "[17/33] class loss: 0.7869\n",
            "[18/33] class loss: 0.7512\n",
            "[19/33] class loss: 0.7982\n",
            "[20/33] class loss: 0.6284\n",
            "[21/33] class loss: 0.8587\n",
            "[22/33] class loss: 0.5074\n",
            "[23/33] class loss: 0.6761\n",
            "[24/33] class loss: 0.5441\n",
            "[25/33] class loss: 0.5394\n",
            "[26/33] class loss: 0.7549\n",
            "[27/33] class loss: 0.7859\n",
            "[28/33] class loss: 0.5645\n",
            "[29/33] class loss: 0.6858\n",
            "[30/33] class loss: 0.5203\n",
            "[31/33] class loss: 0.5065\n",
            "[32/33] class loss: 0.7636\n",
            "[33/33] class loss: 0.7405\n",
            "Epoch: 13 || Train_src_acc: 0.8508, Train_tar_acc: 0.4711, Train_loss: 0.6765\n",
            "Epoch 0014 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.6952\n",
            "[2/33] class loss: 0.8296\n",
            "[3/33] class loss: 0.6909\n",
            "[4/33] class loss: 0.5658\n",
            "[5/33] class loss: 0.8008\n",
            "[6/33] class loss: 0.5573\n",
            "[7/33] class loss: 0.4838\n",
            "[8/33] class loss: 0.6198\n",
            "[9/33] class loss: 0.6231\n",
            "[10/33] class loss: 0.6867\n",
            "[11/33] class loss: 0.7000\n",
            "[12/33] class loss: 0.6295\n",
            "[13/33] class loss: 0.6622\n",
            "[14/33] class loss: 0.6547\n",
            "[15/33] class loss: 0.6164\n",
            "[16/33] class loss: 0.4444\n",
            "[17/33] class loss: 0.6628\n",
            "[18/33] class loss: 0.7478\n",
            "[19/33] class loss: 0.4043\n",
            "[20/33] class loss: 0.7354\n",
            "[21/33] class loss: 0.5718\n",
            "[22/33] class loss: 0.5337\n",
            "[23/33] class loss: 0.4979\n",
            "[24/33] class loss: 0.7168\n",
            "[25/33] class loss: 0.5089\n",
            "[26/33] class loss: 0.8117\n",
            "[27/33] class loss: 0.5633\n",
            "[28/33] class loss: 0.7580\n",
            "[29/33] class loss: 0.4334\n",
            "[30/33] class loss: 0.5510\n",
            "[31/33] class loss: 0.8038\n",
            "[32/33] class loss: 0.4746\n",
            "[33/33] class loss: 0.6892\n",
            "Epoch: 14 || Train_src_acc: 0.8508, Train_tar_acc: 0.4837, Train_loss: 0.6280\n",
            "Epoch 0015 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.5970\n",
            "[2/33] class loss: 0.8224\n",
            "[3/33] class loss: 0.4899\n",
            "[4/33] class loss: 0.5691\n",
            "[5/33] class loss: 0.4144\n",
            "[6/33] class loss: 0.6350\n",
            "[7/33] class loss: 0.6059\n",
            "[8/33] class loss: 0.6306\n",
            "[9/33] class loss: 0.4510\n",
            "[10/33] class loss: 0.5218\n",
            "[11/33] class loss: 0.5224\n",
            "[12/33] class loss: 0.6196\n",
            "[13/33] class loss: 0.5321\n",
            "[14/33] class loss: 0.6579\n",
            "[15/33] class loss: 0.5967\n",
            "[16/33] class loss: 0.6084\n",
            "[17/33] class loss: 0.5363\n",
            "[18/33] class loss: 0.4529\n",
            "[19/33] class loss: 0.8044\n",
            "[20/33] class loss: 0.4190\n",
            "[21/33] class loss: 0.6123\n",
            "[22/33] class loss: 0.6268\n",
            "[23/33] class loss: 0.5692\n",
            "[24/33] class loss: 0.4320\n",
            "[25/33] class loss: 0.6945\n",
            "[26/33] class loss: 0.4919\n",
            "[27/33] class loss: 0.4817\n",
            "[28/33] class loss: 0.5318\n",
            "[29/33] class loss: 0.5498\n",
            "[30/33] class loss: 0.5549\n",
            "[31/33] class loss: 0.5309\n",
            "[32/33] class loss: 0.5597\n",
            "[33/33] class loss: 0.4665\n",
            "Epoch: 15 || Train_src_acc: 0.8697, Train_tar_acc: 0.4842, Train_loss: 0.5633\n",
            "Epoch 0016 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.5076\n",
            "[2/33] class loss: 0.4394\n",
            "[3/33] class loss: 0.6932\n",
            "[4/33] class loss: 0.5960\n",
            "[5/33] class loss: 0.6833\n",
            "[6/33] class loss: 0.4968\n",
            "[7/33] class loss: 0.5196\n",
            "[8/33] class loss: 0.4760\n",
            "[9/33] class loss: 0.5987\n",
            "[10/33] class loss: 0.3852\n",
            "[11/33] class loss: 0.5708\n",
            "[12/33] class loss: 0.6644\n",
            "[13/33] class loss: 0.4587\n",
            "[14/33] class loss: 0.5982\n",
            "[15/33] class loss: 0.3123\n",
            "[16/33] class loss: 0.4795\n",
            "[17/33] class loss: 0.4836\n",
            "[18/33] class loss: 0.4256\n",
            "[19/33] class loss: 0.5799\n",
            "[20/33] class loss: 0.5522\n",
            "[21/33] class loss: 0.5335\n",
            "[22/33] class loss: 0.4937\n",
            "[23/33] class loss: 0.5121\n",
            "[24/33] class loss: 0.4503\n",
            "[25/33] class loss: 0.5428\n",
            "[26/33] class loss: 0.5135\n",
            "[27/33] class loss: 0.5549\n",
            "[28/33] class loss: 0.6444\n",
            "[29/33] class loss: 0.4354\n",
            "[30/33] class loss: 0.4701\n",
            "[31/33] class loss: 0.3995\n",
            "[32/33] class loss: 0.5256\n",
            "[33/33] class loss: 0.5223\n",
            "Epoch: 16 || Train_src_acc: 0.8751, Train_tar_acc: 0.4827, Train_loss: 0.5188\n",
            "Epoch 0017 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.3864\n",
            "[2/33] class loss: 0.6137\n",
            "[3/33] class loss: 0.3530\n",
            "[4/33] class loss: 0.6560\n",
            "[5/33] class loss: 0.3876\n",
            "[6/33] class loss: 0.3407\n",
            "[7/33] class loss: 0.4473\n",
            "[8/33] class loss: 0.4864\n",
            "[9/33] class loss: 0.6019\n",
            "[10/33] class loss: 0.4719\n",
            "[11/33] class loss: 0.4062\n",
            "[12/33] class loss: 0.4027\n",
            "[13/33] class loss: 0.7922\n",
            "[14/33] class loss: 0.3776\n",
            "[15/33] class loss: 0.3234\n",
            "[16/33] class loss: 0.4242\n",
            "[17/33] class loss: 0.4251\n",
            "[18/33] class loss: 0.5321\n",
            "[19/33] class loss: 0.5576\n",
            "[20/33] class loss: 0.4468\n",
            "[21/33] class loss: 0.3497\n",
            "[22/33] class loss: 0.4116\n",
            "[23/33] class loss: 0.3779\n",
            "[24/33] class loss: 0.5159\n",
            "[25/33] class loss: 0.6941\n",
            "[26/33] class loss: 0.4904\n",
            "[27/33] class loss: 0.3991\n",
            "[28/33] class loss: 0.4823\n",
            "[29/33] class loss: 0.5046\n",
            "[30/33] class loss: 0.3780\n",
            "[31/33] class loss: 0.4737\n",
            "[32/33] class loss: 0.4486\n",
            "[33/33] class loss: 0.4023\n",
            "Epoch: 17 || Train_src_acc: 0.8940, Train_tar_acc: 0.4861, Train_loss: 0.4655\n",
            "Epoch 0018 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.4176\n",
            "[2/33] class loss: 0.3196\n",
            "[3/33] class loss: 0.3647\n",
            "[4/33] class loss: 0.3250\n",
            "[5/33] class loss: 0.3557\n",
            "[6/33] class loss: 0.4037\n",
            "[7/33] class loss: 0.4438\n",
            "[8/33] class loss: 0.4354\n",
            "[9/33] class loss: 0.4928\n",
            "[10/33] class loss: 0.3396\n",
            "[11/33] class loss: 0.3825\n",
            "[12/33] class loss: 0.6891\n",
            "[13/33] class loss: 0.4876\n",
            "[14/33] class loss: 0.3475\n",
            "[15/33] class loss: 0.5311\n",
            "[16/33] class loss: 0.3306\n",
            "[17/33] class loss: 0.3043\n",
            "[18/33] class loss: 0.5557\n",
            "[19/33] class loss: 0.5037\n",
            "[20/33] class loss: 0.4749\n",
            "[21/33] class loss: 0.3227\n",
            "[22/33] class loss: 0.4853\n",
            "[23/33] class loss: 0.3414\n",
            "[24/33] class loss: 0.5184\n",
            "[25/33] class loss: 0.5648\n",
            "[26/33] class loss: 0.5176\n",
            "[27/33] class loss: 0.5102\n",
            "[28/33] class loss: 0.3652\n",
            "[29/33] class loss: 0.3233\n",
            "[30/33] class loss: 0.2904\n",
            "[31/33] class loss: 0.4449\n",
            "[32/33] class loss: 0.3690\n",
            "[33/33] class loss: 0.6144\n",
            "Epoch: 18 || Train_src_acc: 0.9076, Train_tar_acc: 0.5109, Train_loss: 0.4295\n",
            "Epoch 0019 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.6671\n",
            "[2/33] class loss: 0.2716\n",
            "[3/33] class loss: 0.2053\n",
            "[4/33] class loss: 0.2389\n",
            "[5/33] class loss: 0.3836\n",
            "[6/33] class loss: 0.3323\n",
            "[7/33] class loss: 0.4566\n",
            "[8/33] class loss: 0.3922\n",
            "[9/33] class loss: 0.2919\n",
            "[10/33] class loss: 0.3692\n",
            "[11/33] class loss: 0.3234\n",
            "[12/33] class loss: 0.3351\n",
            "[13/33] class loss: 0.3350\n",
            "[14/33] class loss: 0.4647\n",
            "[15/33] class loss: 0.4074\n",
            "[16/33] class loss: 0.2755\n",
            "[17/33] class loss: 0.5317\n",
            "[18/33] class loss: 0.3826\n",
            "[19/33] class loss: 0.5085\n",
            "[20/33] class loss: 0.3815\n",
            "[21/33] class loss: 0.5420\n",
            "[22/33] class loss: 0.3888\n",
            "[23/33] class loss: 0.3411\n",
            "[24/33] class loss: 0.3903\n",
            "[25/33] class loss: 0.5313\n",
            "[26/33] class loss: 0.4074\n",
            "[27/33] class loss: 0.2744\n",
            "[28/33] class loss: 0.3648\n",
            "[29/33] class loss: 0.3675\n",
            "[30/33] class loss: 0.3085\n",
            "[31/33] class loss: 0.4344\n",
            "[32/33] class loss: 0.4121\n",
            "[33/33] class loss: 0.2970\n",
            "Epoch: 19 || Train_src_acc: 0.9140, Train_tar_acc: 0.4993, Train_loss: 0.3822\n",
            "Epoch 0020 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.4965\n",
            "[2/33] class loss: 0.3378\n",
            "[3/33] class loss: 0.2031\n",
            "[4/33] class loss: 0.2842\n",
            "[5/33] class loss: 0.3059\n",
            "[6/33] class loss: 0.3368\n",
            "[7/33] class loss: 0.3459\n",
            "[8/33] class loss: 0.2351\n",
            "[9/33] class loss: 0.2932\n",
            "[10/33] class loss: 0.3548\n",
            "[11/33] class loss: 0.3257\n",
            "[12/33] class loss: 0.3095\n",
            "[13/33] class loss: 0.1963\n",
            "[14/33] class loss: 0.3305\n",
            "[15/33] class loss: 0.3024\n",
            "[16/33] class loss: 0.2491\n",
            "[17/33] class loss: 0.3880\n",
            "[18/33] class loss: 0.3595\n",
            "[19/33] class loss: 0.3491\n",
            "[20/33] class loss: 0.3725\n",
            "[21/33] class loss: 0.3209\n",
            "[22/33] class loss: 0.2605\n",
            "[23/33] class loss: 0.2419\n",
            "[24/33] class loss: 0.3089\n",
            "[25/33] class loss: 0.6466\n",
            "[26/33] class loss: 0.5256\n",
            "[27/33] class loss: 0.4376\n",
            "[28/33] class loss: 0.5334\n",
            "[29/33] class loss: 0.4418\n",
            "[30/33] class loss: 0.3758\n",
            "[31/33] class loss: 0.4252\n",
            "[32/33] class loss: 0.2649\n",
            "[33/33] class loss: 0.3503\n",
            "Epoch: 20 || Train_src_acc: 0.9227, Train_tar_acc: 0.4910, Train_loss: 0.3488\n",
            "Epoch 0021 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.2342\n",
            "[2/33] class loss: 0.2004\n",
            "[3/33] class loss: 0.3675\n",
            "[4/33] class loss: 0.3837\n",
            "[5/33] class loss: 0.2536\n",
            "[6/33] class loss: 0.3280\n",
            "[7/33] class loss: 0.3794\n",
            "[8/33] class loss: 0.2616\n",
            "[9/33] class loss: 0.3053\n",
            "[10/33] class loss: 0.3423\n",
            "[11/33] class loss: 0.1850\n",
            "[12/33] class loss: 0.3018\n",
            "[13/33] class loss: 0.3544\n",
            "[14/33] class loss: 0.4548\n",
            "[15/33] class loss: 0.2239\n",
            "[16/33] class loss: 0.3655\n",
            "[17/33] class loss: 0.2183\n",
            "[18/33] class loss: 0.1809\n",
            "[19/33] class loss: 0.3242\n",
            "[20/33] class loss: 0.2840\n",
            "[21/33] class loss: 0.2740\n",
            "[22/33] class loss: 0.2726\n",
            "[23/33] class loss: 0.2815\n",
            "[24/33] class loss: 0.3980\n",
            "[25/33] class loss: 0.4598\n",
            "[26/33] class loss: 0.3063\n",
            "[27/33] class loss: 0.2441\n",
            "[28/33] class loss: 0.3253\n",
            "[29/33] class loss: 0.4568\n",
            "[30/33] class loss: 0.3379\n",
            "[31/33] class loss: 0.2581\n",
            "[32/33] class loss: 0.3816\n",
            "[33/33] class loss: 0.2481\n",
            "Epoch: 21 || Train_src_acc: 0.9334, Train_tar_acc: 0.4964, Train_loss: 0.3089\n",
            "Epoch 0022 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.2201\n",
            "[2/33] class loss: 0.2983\n",
            "[3/33] class loss: 0.2133\n",
            "[4/33] class loss: 0.2965\n",
            "[5/33] class loss: 0.2455\n",
            "[6/33] class loss: 0.3257\n",
            "[7/33] class loss: 0.3153\n",
            "[8/33] class loss: 0.3389\n",
            "[9/33] class loss: 0.2738\n",
            "[10/33] class loss: 0.2641\n",
            "[11/33] class loss: 0.2663\n",
            "[12/33] class loss: 0.2604\n",
            "[13/33] class loss: 0.3353\n",
            "[14/33] class loss: 0.2791\n",
            "[15/33] class loss: 0.1774\n",
            "[16/33] class loss: 0.3118\n",
            "[17/33] class loss: 0.3722\n",
            "[18/33] class loss: 0.4443\n",
            "[19/33] class loss: 0.2122\n",
            "[20/33] class loss: 0.3469\n",
            "[21/33] class loss: 0.2703\n",
            "[22/33] class loss: 0.4158\n",
            "[23/33] class loss: 0.3174\n",
            "[24/33] class loss: 0.3318\n",
            "[25/33] class loss: 0.2620\n",
            "[26/33] class loss: 0.3263\n",
            "[27/33] class loss: 0.1944\n",
            "[28/33] class loss: 0.2659\n",
            "[29/33] class loss: 0.3257\n",
            "[30/33] class loss: 0.2490\n",
            "[31/33] class loss: 0.3795\n",
            "[32/33] class loss: 0.3885\n",
            "[33/33] class loss: 0.2501\n",
            "Epoch: 22 || Train_src_acc: 0.9383, Train_tar_acc: 0.4895, Train_loss: 0.2962\n",
            "Epoch 0023 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.2153\n",
            "[2/33] class loss: 0.2914\n",
            "[3/33] class loss: 0.2432\n",
            "[4/33] class loss: 0.3485\n",
            "[5/33] class loss: 0.2929\n",
            "[6/33] class loss: 0.1965\n",
            "[7/33] class loss: 0.3071\n",
            "[8/33] class loss: 0.2121\n",
            "[9/33] class loss: 0.2463\n",
            "[10/33] class loss: 0.2515\n",
            "[11/33] class loss: 0.2207\n",
            "[12/33] class loss: 0.0980\n",
            "[13/33] class loss: 0.2229\n",
            "[14/33] class loss: 0.2724\n",
            "[15/33] class loss: 0.1985\n",
            "[16/33] class loss: 0.3229\n",
            "[17/33] class loss: 0.3482\n",
            "[18/33] class loss: 0.2459\n",
            "[19/33] class loss: 0.3937\n",
            "[20/33] class loss: 0.1960\n",
            "[21/33] class loss: 0.2936\n",
            "[22/33] class loss: 0.3119\n",
            "[23/33] class loss: 0.2723\n",
            "[24/33] class loss: 0.3310\n",
            "[25/33] class loss: 0.2767\n",
            "[26/33] class loss: 0.2936\n",
            "[27/33] class loss: 0.3901\n",
            "[28/33] class loss: 0.1477\n",
            "[29/33] class loss: 0.2080\n",
            "[30/33] class loss: 0.2626\n",
            "[31/33] class loss: 0.3298\n",
            "[32/33] class loss: 0.3041\n",
            "[33/33] class loss: 0.1834\n",
            "Epoch: 23 || Train_src_acc: 0.9509, Train_tar_acc: 0.4983, Train_loss: 0.2645\n",
            "Epoch 0024 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.2546\n",
            "[2/33] class loss: 0.2715\n",
            "[3/33] class loss: 0.3455\n",
            "[4/33] class loss: 0.2584\n",
            "[5/33] class loss: 0.2110\n",
            "[6/33] class loss: 0.2033\n",
            "[7/33] class loss: 0.1932\n",
            "[8/33] class loss: 0.2806\n",
            "[9/33] class loss: 0.2817\n",
            "[10/33] class loss: 0.2174\n",
            "[11/33] class loss: 0.1446\n",
            "[12/33] class loss: 0.2512\n",
            "[13/33] class loss: 0.1254\n",
            "[14/33] class loss: 0.3252\n",
            "[15/33] class loss: 0.3640\n",
            "[16/33] class loss: 0.2345\n",
            "[17/33] class loss: 0.2645\n",
            "[18/33] class loss: 0.1576\n",
            "[19/33] class loss: 0.1955\n",
            "[20/33] class loss: 0.3035\n",
            "[21/33] class loss: 0.2443\n",
            "[22/33] class loss: 0.3052\n",
            "[23/33] class loss: 0.2334\n",
            "[24/33] class loss: 0.1838\n",
            "[25/33] class loss: 0.1744\n",
            "[26/33] class loss: 0.2030\n",
            "[27/33] class loss: 0.3008\n",
            "[28/33] class loss: 0.2024\n",
            "[29/33] class loss: 0.1820\n",
            "[30/33] class loss: 0.2515\n",
            "[31/33] class loss: 0.1865\n",
            "[32/33] class loss: 0.1794\n",
            "[33/33] class loss: 0.2401\n",
            "Epoch: 24 || Train_src_acc: 0.9538, Train_tar_acc: 0.4944, Train_loss: 0.2355\n",
            "Epoch 0025 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.1496\n",
            "[2/33] class loss: 0.2410\n",
            "[3/33] class loss: 0.1829\n",
            "[4/33] class loss: 0.1925\n",
            "[5/33] class loss: 0.1471\n",
            "[6/33] class loss: 0.3778\n",
            "[7/33] class loss: 0.2599\n",
            "[8/33] class loss: 0.2304\n",
            "[9/33] class loss: 0.2392\n",
            "[10/33] class loss: 0.1658\n",
            "[11/33] class loss: 0.1853\n",
            "[12/33] class loss: 0.2559\n",
            "[13/33] class loss: 0.3064\n",
            "[14/33] class loss: 0.2296\n",
            "[15/33] class loss: 0.1028\n",
            "[16/33] class loss: 0.1557\n",
            "[17/33] class loss: 0.1585\n",
            "[18/33] class loss: 0.1695\n",
            "[19/33] class loss: 0.2061\n",
            "[20/33] class loss: 0.3398\n",
            "[21/33] class loss: 0.1289\n",
            "[22/33] class loss: 0.1441\n",
            "[23/33] class loss: 0.1257\n",
            "[24/33] class loss: 0.2646\n",
            "[25/33] class loss: 0.2455\n",
            "[26/33] class loss: 0.2328\n",
            "[27/33] class loss: 0.2673\n",
            "[28/33] class loss: 0.2141\n",
            "[29/33] class loss: 0.1908\n",
            "[30/33] class loss: 0.2721\n",
            "[31/33] class loss: 0.2688\n",
            "[32/33] class loss: 0.1330\n",
            "[33/33] class loss: 0.2221\n",
            "Epoch: 25 || Train_src_acc: 0.9596, Train_tar_acc: 0.4842, Train_loss: 0.2123\n",
            "Epoch 0026 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.2560\n",
            "[2/33] class loss: 0.1449\n",
            "[3/33] class loss: 0.1888\n",
            "[4/33] class loss: 0.1613\n",
            "[5/33] class loss: 0.2236\n",
            "[6/33] class loss: 0.1959\n",
            "[7/33] class loss: 0.1052\n",
            "[8/33] class loss: 0.1337\n",
            "[9/33] class loss: 0.2543\n",
            "[10/33] class loss: 0.2537\n",
            "[11/33] class loss: 0.1315\n",
            "[12/33] class loss: 0.3501\n",
            "[13/33] class loss: 0.1569\n",
            "[14/33] class loss: 0.2119\n",
            "[15/33] class loss: 0.2466\n",
            "[16/33] class loss: 0.2319\n",
            "[17/33] class loss: 0.1177\n",
            "[18/33] class loss: 0.1286\n",
            "[19/33] class loss: 0.1748\n",
            "[20/33] class loss: 0.1620\n",
            "[21/33] class loss: 0.2440\n",
            "[22/33] class loss: 0.1227\n",
            "[23/33] class loss: 0.1696\n",
            "[24/33] class loss: 0.1853\n",
            "[25/33] class loss: 0.1627\n",
            "[26/33] class loss: 0.3001\n",
            "[27/33] class loss: 0.1705\n",
            "[28/33] class loss: 0.2249\n",
            "[29/33] class loss: 0.2115\n",
            "[30/33] class loss: 0.1806\n",
            "[31/33] class loss: 0.1835\n",
            "[32/33] class loss: 0.1448\n",
            "[33/33] class loss: 0.2048\n",
            "Epoch: 26 || Train_src_acc: 0.9669, Train_tar_acc: 0.5017, Train_loss: 0.1919\n",
            "Epoch 0027 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.1271\n",
            "[2/33] class loss: 0.1507\n",
            "[3/33] class loss: 0.1867\n",
            "[4/33] class loss: 0.1612\n",
            "[5/33] class loss: 0.1240\n",
            "[6/33] class loss: 0.3352\n",
            "[7/33] class loss: 0.1259\n",
            "[8/33] class loss: 0.2605\n",
            "[9/33] class loss: 0.2034\n",
            "[10/33] class loss: 0.0951\n",
            "[11/33] class loss: 0.1635\n",
            "[12/33] class loss: 0.1485\n",
            "[13/33] class loss: 0.1902\n",
            "[14/33] class loss: 0.1276\n",
            "[15/33] class loss: 0.1291\n",
            "[16/33] class loss: 0.1187\n",
            "[17/33] class loss: 0.1918\n",
            "[18/33] class loss: 0.0925\n",
            "[19/33] class loss: 0.1760\n",
            "[20/33] class loss: 0.1601\n",
            "[21/33] class loss: 0.1551\n",
            "[22/33] class loss: 0.3009\n",
            "[23/33] class loss: 0.1388\n",
            "[24/33] class loss: 0.2121\n",
            "[25/33] class loss: 0.2753\n",
            "[26/33] class loss: 0.1517\n",
            "[27/33] class loss: 0.1546\n",
            "[28/33] class loss: 0.2038\n",
            "[29/33] class loss: 0.1551\n",
            "[30/33] class loss: 0.1836\n",
            "[31/33] class loss: 0.1611\n",
            "[32/33] class loss: 0.1370\n",
            "[33/33] class loss: 0.1243\n",
            "Epoch: 27 || Train_src_acc: 0.9723, Train_tar_acc: 0.4939, Train_loss: 0.1703\n",
            "Epoch 0028 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.1538\n",
            "[2/33] class loss: 0.2094\n",
            "[3/33] class loss: 0.1057\n",
            "[4/33] class loss: 0.1149\n",
            "[5/33] class loss: 0.1757\n",
            "[6/33] class loss: 0.1874\n",
            "[7/33] class loss: 0.1174\n",
            "[8/33] class loss: 0.1979\n",
            "[9/33] class loss: 0.1253\n",
            "[10/33] class loss: 0.1493\n",
            "[11/33] class loss: 0.0905\n",
            "[12/33] class loss: 0.1263\n",
            "[13/33] class loss: 0.1655\n",
            "[14/33] class loss: 0.1609\n",
            "[15/33] class loss: 0.2355\n",
            "[16/33] class loss: 0.1954\n",
            "[17/33] class loss: 0.1078\n",
            "[18/33] class loss: 0.1256\n",
            "[19/33] class loss: 0.1686\n",
            "[20/33] class loss: 0.1568\n",
            "[21/33] class loss: 0.2551\n",
            "[22/33] class loss: 0.1488\n",
            "[23/33] class loss: 0.1577\n",
            "[24/33] class loss: 0.0872\n",
            "[25/33] class loss: 0.1117\n",
            "[26/33] class loss: 0.2046\n",
            "[27/33] class loss: 0.1164\n",
            "[28/33] class loss: 0.2821\n",
            "[29/33] class loss: 0.1208\n",
            "[30/33] class loss: 0.1810\n",
            "[31/33] class loss: 0.1716\n",
            "[32/33] class loss: 0.0716\n",
            "[33/33] class loss: 0.1406\n",
            "Epoch: 28 || Train_src_acc: 0.9767, Train_tar_acc: 0.5027, Train_loss: 0.1551\n",
            "Epoch 0029 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.1420\n",
            "[2/33] class loss: 0.1055\n",
            "[3/33] class loss: 0.1644\n",
            "[4/33] class loss: 0.1231\n",
            "[5/33] class loss: 0.1201\n",
            "[6/33] class loss: 0.1792\n",
            "[7/33] class loss: 0.0918\n",
            "[8/33] class loss: 0.1335\n",
            "[9/33] class loss: 0.1796\n",
            "[10/33] class loss: 0.1603\n",
            "[11/33] class loss: 0.1246\n",
            "[12/33] class loss: 0.2363\n",
            "[13/33] class loss: 0.2201\n",
            "[14/33] class loss: 0.1180\n",
            "[15/33] class loss: 0.1342\n",
            "[16/33] class loss: 0.1834\n",
            "[17/33] class loss: 0.1267\n",
            "[18/33] class loss: 0.0833\n",
            "[19/33] class loss: 0.0778\n",
            "[20/33] class loss: 0.1180\n",
            "[21/33] class loss: 0.0717\n",
            "[22/33] class loss: 0.1831\n",
            "[23/33] class loss: 0.1954\n",
            "[24/33] class loss: 0.1927\n",
            "[25/33] class loss: 0.1672\n",
            "[26/33] class loss: 0.1129\n",
            "[27/33] class loss: 0.1692\n",
            "[28/33] class loss: 0.1154\n",
            "[29/33] class loss: 0.0937\n",
            "[30/33] class loss: 0.1494\n",
            "[31/33] class loss: 0.1118\n",
            "[32/33] class loss: 0.1781\n",
            "[33/33] class loss: 0.1549\n",
            "Epoch: 29 || Train_src_acc: 0.9854, Train_tar_acc: 0.4871, Train_loss: 0.1429\n",
            "Epoch 0030 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.1035\n",
            "[2/33] class loss: 0.0745\n",
            "[3/33] class loss: 0.0836\n",
            "[4/33] class loss: 0.0902\n",
            "[5/33] class loss: 0.0736\n",
            "[6/33] class loss: 0.1189\n",
            "[7/33] class loss: 0.0591\n",
            "[8/33] class loss: 0.1633\n",
            "[9/33] class loss: 0.1535\n",
            "[10/33] class loss: 0.0508\n",
            "[11/33] class loss: 0.2035\n",
            "[12/33] class loss: 0.0734\n",
            "[13/33] class loss: 0.1056\n",
            "[14/33] class loss: 0.1799\n",
            "[15/33] class loss: 0.1097\n",
            "[16/33] class loss: 0.1247\n",
            "[17/33] class loss: 0.2074\n",
            "[18/33] class loss: 0.1640\n",
            "[19/33] class loss: 0.1130\n",
            "[20/33] class loss: 0.0964\n",
            "[21/33] class loss: 0.1002\n",
            "[22/33] class loss: 0.1701\n",
            "[23/33] class loss: 0.0941\n",
            "[24/33] class loss: 0.1449\n",
            "[25/33] class loss: 0.1043\n",
            "[26/33] class loss: 0.1223\n",
            "[27/33] class loss: 0.1793\n",
            "[28/33] class loss: 0.1787\n",
            "[29/33] class loss: 0.1054\n",
            "[30/33] class loss: 0.1889\n",
            "[31/33] class loss: 0.1832\n",
            "[32/33] class loss: 0.1512\n",
            "[33/33] class loss: 0.1166\n",
            "Epoch: 30 || Train_src_acc: 0.9864, Train_tar_acc: 0.5036, Train_loss: 0.1269\n",
            "Epoch 0031 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.1201\n",
            "[2/33] class loss: 0.0801\n",
            "[3/33] class loss: 0.1908\n",
            "[4/33] class loss: 0.1652\n",
            "[5/33] class loss: 0.1992\n",
            "[6/33] class loss: 0.1902\n",
            "[7/33] class loss: 0.1474\n",
            "[8/33] class loss: 0.1422\n",
            "[9/33] class loss: 0.1079\n",
            "[10/33] class loss: 0.1357\n",
            "[11/33] class loss: 0.1109\n",
            "[12/33] class loss: 0.1337\n",
            "[13/33] class loss: 0.1044\n",
            "[14/33] class loss: 0.1815\n",
            "[15/33] class loss: 0.0523\n",
            "[16/33] class loss: 0.0902\n",
            "[17/33] class loss: 0.0740\n",
            "[18/33] class loss: 0.1415\n",
            "[19/33] class loss: 0.1145\n",
            "[20/33] class loss: 0.0779\n",
            "[21/33] class loss: 0.0708\n",
            "[22/33] class loss: 0.0769\n",
            "[23/33] class loss: 0.1048\n",
            "[24/33] class loss: 0.0710\n",
            "[25/33] class loss: 0.0877\n",
            "[26/33] class loss: 0.0895\n",
            "[27/33] class loss: 0.0845\n",
            "[28/33] class loss: 0.1647\n",
            "[29/33] class loss: 0.1034\n",
            "[30/33] class loss: 0.1153\n",
            "[31/33] class loss: 0.1308\n",
            "[32/33] class loss: 0.1085\n",
            "[33/33] class loss: 0.0779\n",
            "Epoch: 31 || Train_src_acc: 0.9835, Train_tar_acc: 0.5017, Train_loss: 0.1165\n",
            "Epoch 0032 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.0782\n",
            "[2/33] class loss: 0.0886\n",
            "[3/33] class loss: 0.0634\n",
            "[4/33] class loss: 0.1144\n",
            "[5/33] class loss: 0.0822\n",
            "[6/33] class loss: 0.0978\n",
            "[7/33] class loss: 0.0553\n",
            "[8/33] class loss: 0.1408\n",
            "[9/33] class loss: 0.0892\n",
            "[10/33] class loss: 0.0851\n",
            "[11/33] class loss: 0.0727\n",
            "[12/33] class loss: 0.0579\n",
            "[13/33] class loss: 0.1358\n",
            "[14/33] class loss: 0.1468\n",
            "[15/33] class loss: 0.1067\n",
            "[16/33] class loss: 0.1479\n",
            "[17/33] class loss: 0.0760\n",
            "[18/33] class loss: 0.1991\n",
            "[19/33] class loss: 0.0866\n",
            "[20/33] class loss: 0.1323\n",
            "[21/33] class loss: 0.0620\n",
            "[22/33] class loss: 0.0883\n",
            "[23/33] class loss: 0.0608\n",
            "[24/33] class loss: 0.0967\n",
            "[25/33] class loss: 0.0956\n",
            "[26/33] class loss: 0.1371\n",
            "[27/33] class loss: 0.0783\n",
            "[28/33] class loss: 0.1421\n",
            "[29/33] class loss: 0.0666\n",
            "[30/33] class loss: 0.1126\n",
            "[31/33] class loss: 0.1195\n",
            "[32/33] class loss: 0.1399\n",
            "[33/33] class loss: 0.0613\n",
            "Epoch: 32 || Train_src_acc: 0.9917, Train_tar_acc: 0.4934, Train_loss: 0.1005\n",
            "Epoch 0033 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.1231\n",
            "[2/33] class loss: 0.0884\n",
            "[3/33] class loss: 0.1213\n",
            "[4/33] class loss: 0.0552\n",
            "[5/33] class loss: 0.1011\n",
            "[6/33] class loss: 0.0889\n",
            "[7/33] class loss: 0.1353\n",
            "[8/33] class loss: 0.1451\n",
            "[9/33] class loss: 0.0712\n",
            "[10/33] class loss: 0.0938\n",
            "[11/33] class loss: 0.0992\n",
            "[12/33] class loss: 0.1352\n",
            "[13/33] class loss: 0.1182\n",
            "[14/33] class loss: 0.0680\n",
            "[15/33] class loss: 0.0749\n",
            "[16/33] class loss: 0.1454\n",
            "[17/33] class loss: 0.0795\n",
            "[18/33] class loss: 0.0727\n",
            "[19/33] class loss: 0.1078\n",
            "[20/33] class loss: 0.0498\n",
            "[21/33] class loss: 0.1660\n",
            "[22/33] class loss: 0.1288\n",
            "[23/33] class loss: 0.0749\n",
            "[24/33] class loss: 0.0705\n",
            "[25/33] class loss: 0.1341\n",
            "[26/33] class loss: 0.1047\n",
            "[27/33] class loss: 0.0773\n",
            "[28/33] class loss: 0.0495\n",
            "[29/33] class loss: 0.0690\n",
            "[30/33] class loss: 0.1435\n",
            "[31/33] class loss: 0.0929\n",
            "[32/33] class loss: 0.1011\n",
            "[33/33] class loss: 0.1379\n",
            "Epoch: 33 || Train_src_acc: 0.9883, Train_tar_acc: 0.5090, Train_loss: 0.1007\n",
            "Epoch 0034 / 0050\n",
            "============\n",
            "[1/33] class loss: 0.1189\n",
            "[2/33] class loss: 0.1011\n",
            "[3/33] class loss: 0.0710\n",
            "[4/33] class loss: 0.0857\n",
            "[5/33] class loss: 0.1153\n",
            "[6/33] class loss: 0.0841\n",
            "[7/33] class loss: 0.1745\n",
            "[8/33] class loss: 0.1295\n",
            "[9/33] class loss: 0.0818\n",
            "[10/33] class loss: 0.0546\n",
            "[11/33] class loss: 0.0554\n",
            "[12/33] class loss: 0.0598\n",
            "[13/33] class loss: 0.1036\n",
            "[14/33] class loss: 0.0770\n",
            "[15/33] class loss: 0.0858\n",
            "[16/33] class loss: 0.0629\n",
            "[17/33] class loss: 0.1338\n",
            "[18/33] class loss: 0.0352\n",
            "[19/33] class loss: 0.1285\n",
            "[20/33] class loss: 0.0916\n",
            "[21/33] class loss: 0.0714\n",
            "[22/33] class loss: 0.1048\n",
            "[23/33] class loss: 0.0777\n",
            "[24/33] class loss: 0.0692\n",
            "[25/33] class loss: 0.0560\n",
            "[26/33] class loss: 0.0847\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a61d90f5c788>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0manneal_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m  \u001b[0;31m# Can also try 'cos' for cosine annealing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             )\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mplot_graphDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_model_with_balanced_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"SO {src} -> {tar}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Source and Target are the same. Skiping...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-3f813992334d>\u001b[0m in \u001b[0;36mtrain_model_with_balanced_batches\u001b[0;34m(model, optimizer, loss_fn_class, src, tar, device, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# Get next batch from source and target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mX_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDl_source_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mX_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDl_target_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[1;32m    244\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3375\u001b[0;31m def open(\n\u001b[0m\u001b[1;32m   3376\u001b[0m     \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mStrOrBytesPath\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3377\u001b[0m     \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "summary(get_model_SO(), input_size=(channel_size, image_size, image_size))\n",
        "num_epochs = 50\n",
        "lr = 1e-3\n",
        "loss_fn_class = torch.nn.CrossEntropyLoss()\n",
        "srcs = [\"amazon\", \"dslr\", \"webcam\"]\n",
        "tars = [\"amazon\", \"dslr\", \"webcam\"]\n",
        "for src in srcs:\n",
        "    for tar in tars:\n",
        "        if src != tar:\n",
        "            model = get_model_SO()\n",
        "            src_dataloader, tar_dataloader = get_loader_DA(src, tar)\n",
        "            optimizer = optim.Adam(model.parameters(), lr)\n",
        "            steps_per_epoch = len(src_dataloader)\n",
        "            scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "                optimizer,\n",
        "                max_lr=lr,  # Max learning rate during the cycle\n",
        "                steps_per_epoch=steps_per_epoch,\n",
        "                epochs=num_epochs,\n",
        "                anneal_strategy='linear'  # Can also try 'cos' for cosine annealing\n",
        "            )\n",
        "            plot_graphDA(train_model_with_balanced_batches(model, optimizer, loss_fn_class, src, tar, device, num_epochs=num_epochs), f\"SO {src} -> {tar}\")\n",
        "        else: print(\"Source and Target are the same. Skiping...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aLU_yowMkd-"
      },
      "outputs": [],
      "source": [
        "zip_directory('./cp', 'SO_Model.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhnSl5aXNAFB",
        "outputId": "34177aa8-86d1-4c74-9e68-ede5d15e492e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added LOG_SO_amazon_to_webcam to the zip\n",
            "Added LOG_SO_webcam_to_dslr to the zip\n",
            "Added LOG_SO_dslr_to_webcam to the zip\n",
            "Added LOG_SO_amazon_to_dslr to the zip\n",
            "Added LOG_SO_webcam_to_amazon to the zip\n",
            "Added LOG_SO_dslr_to_amazon to the zip\n",
            "All files starting with 'LOG_' have been zipped into SO_LOG.zip\n"
          ]
        }
      ],
      "source": [
        "zip_log_files(\"SO_LOG.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rz3CrPk8hHew"
      },
      "source": [
        "## DANN (Teacher Ver.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFgQ8ptShOaZ"
      },
      "source": [
        "### Training Loop Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms9mbNNig4H1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def create_balanced_dataloaders(src_dataloader, tar_dataloader, batch_size, blur_sigma=2):\n",
        "    src_size = len(src_dataloader.dataset)\n",
        "    tar_size = len(tar_dataloader.dataset)\n",
        "\n",
        "    # Define a blur transformation\n",
        "    blur_transform = transforms.GaussianBlur(kernel_size=(5, 5), sigma=blur_sigma)\n",
        "\n",
        "    def blur_images(batch):\n",
        "        images, labels = batch\n",
        "        # Apply blur to images\n",
        "        blurred_images = blur_transform(images)\n",
        "        return blurred_images, labels\n",
        "\n",
        "    # Ensure iterators are reset appropriately for smaller dataset\n",
        "    if src_size > tar_size:\n",
        "        Dl_source_iter = iter(src_dataloader)\n",
        "        Dl_target_iter = itertools.cycle(tar_dataloader)  # Cycle the smaller dataset\n",
        "        max_batches = len(src_dataloader)\n",
        "\n",
        "        # Apply blur to the cycled target images\n",
        "        Dl_target_iter = (blur_images(batch) for batch in Dl_target_iter)\n",
        "    else:\n",
        "        Dl_source_iter = itertools.cycle(src_dataloader)  # Cycle the smaller dataset\n",
        "        Dl_target_iter = iter(tar_dataloader)\n",
        "        max_batches = len(tar_dataloader)\n",
        "\n",
        "        # Apply blur to the cycled source images\n",
        "        Dl_source_iter = (blur_images(batch) for batch in Dl_source_iter)\n",
        "\n",
        "    return Dl_source_iter, Dl_target_iter, max_batches\n",
        "\n",
        "\n",
        "\n",
        "def train_model_with_balanced_batches_DANN(model, optimizer, scheduler, loss_fn_class, loss_fn_domain,\n",
        "                                           src, tar, device,\n",
        "                                           num_epochs=30, batch_size=64):\n",
        "    \"\"\"\n",
        "    Train the DANN model using balanced batches from both source and target domain dataloaders.\n",
        "    Args:\n",
        "        model: The DANN model (including feature extractor and domain classifier).\n",
        "        optimizer: Optimizer for the model.\n",
        "        loss_fn_class: Loss function for classification (e.g., cross-entropy).\n",
        "        loss_fn_domain: Loss function for domain classification (e.g., binary cross-entropy).\n",
        "        src_dataloader: DataLoader for the source domain.\n",
        "        tar_dataloader: DataLoader for the target domain.\n",
        "        device: Device to run the training on (e.g., 'cuda' or 'cpu').\n",
        "        num_epochs: Number of training epochs.\n",
        "        batch_size: Batch size used in training.\n",
        "\n",
        "    Returns:\n",
        "        training_logs_so: A dictionary with training loss and accuracies for source and target domains.\n",
        "    \"\"\"\n",
        "\n",
        "    # Logs to track training performance\n",
        "    training_logs_so = {\"train_loss\": [], \"train_src_acc\": [], \"train_tar_acc\": []}\n",
        "    src_dataloader, tar_dataloader = get_loader_DA(src, tar)\n",
        "    model_save_path = f\"DANN(Teacher)_{src}_to_{tar}.pth\"\n",
        "    best_vloss = 1_000_000.\n",
        "\n",
        "    for epoch_idx in range(num_epochs):\n",
        "        train_loss, train_src_correct, train_tar_correct = 0, 0, 0\n",
        "        actual_src_samples, actual_tar_samples = 0, 0  # Track the actual number of processed samples\n",
        "\n",
        "        print(f'Epoch {epoch_idx+1:04d} / {num_epochs:04d}', end='\\n============\\n')\n",
        "\n",
        "        # Create balanced iterators for source and target domain\n",
        "        Dl_source_iter, Dl_target_iter, max_batches = create_balanced_dataloaders(src_dataloader, tar_dataloader, batch_size)\n",
        "\n",
        "        for batch_idx in range(max_batches):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get next batch from source and target datasets\n",
        "            X_s, y_s = next(Dl_source_iter)\n",
        "            X_t, y_t = next(Dl_target_iter)\n",
        "\n",
        "            # Ensure the batches have the same size by taking the minimum batch size\n",
        "            if X_s.shape[0] != X_t.shape[0]:\n",
        "                min_bs = min(X_s.shape[0], X_t.shape[0])\n",
        "                X_s, y_s = X_s[:min_bs], y_s[:min_bs]\n",
        "                X_t, y_t = X_t[:min_bs], y_t[:min_bs]\n",
        "\n",
        "            # Track the number of samples processed\n",
        "            actual_src_samples += y_s.size(0)\n",
        "            actual_tar_samples += y_t.size(0)\n",
        "\n",
        "            # Send data to device (GPU or CPU)\n",
        "            X_s, y_s = X_s.to(device), y_s.to(device)\n",
        "            X_t, y_t = X_t.to(device), y_t.to(device)\n",
        "\n",
        "            # Dynamic adjustment of grl_lambda\n",
        "            p = float(batch_idx + epoch_idx * max_batches) / (num_epochs * max_batches)\n",
        "            grl_lambda = (2 / (1 + np.exp(-10 * p)) - 1)\n",
        "\n",
        "            # Prepare labels for domain classification (source domain = 0, target domain = 1)\n",
        "            y_s_domain = torch.zeros(X_s.size(0), dtype=torch.long).to(device)\n",
        "            y_t_domain = torch.ones(X_t.size(0), dtype=torch.long).to(device)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            class_prediction_s, domain_prediction_s = model(X_s, grl_lambda)\n",
        "            _, domain_prediction_t = model(X_t, grl_lambda)\n",
        "\n",
        "            # Calculate classification and domain losses\n",
        "            loss_s_label = loss_fn_class(class_prediction_s, y_s)  # Source classification loss\n",
        "            loss_s_domain = loss_fn_domain(domain_prediction_s, y_s_domain)  # Source domain loss\n",
        "            loss_t_domain = loss_fn_domain(domain_prediction_t, y_t_domain)  # Target domain loss\n",
        "\n",
        "            # Combine the losses with a weight for domain loss\n",
        "            loss = loss_s_label + grl_lambda * (loss_s_domain + loss_t_domain)\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Accumulate the training loss\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy for source domain classification\n",
        "            train_src_correct += (class_prediction_s.argmax(1) == y_s).float().sum().item()\n",
        "\n",
        "            # Compute accuracy for target domain classification (even though target labels are not used in training)\n",
        "            with torch.no_grad():\n",
        "                class_prediction_t, _ = model(X_t, grl_lambda)\n",
        "            train_tar_correct += (class_prediction_t.argmax(1) == y_t).float().sum().item()\n",
        "\n",
        "            # Print batch info\n",
        "            print(f'[{batch_idx+1}/{max_batches}] Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Calculate and store metrics for the epoch\n",
        "        training_logs_so[\"train_loss\"].append(train_loss / max_batches)\n",
        "        training_logs_so[\"train_src_acc\"].append(train_src_correct / actual_src_samples)\n",
        "        training_logs_so[\"train_tar_acc\"].append(train_tar_correct / actual_tar_samples)\n",
        "\n",
        "        print(f'Epoch: {epoch_idx+1} | '\n",
        "              f'Source Accuracy: {train_src_correct / actual_src_samples:.4f}, '\n",
        "              f'Target Accuracy: {train_tar_correct / actual_tar_samples:.4f}, '\n",
        "              f'Loss: {train_loss / max_batches:.4f}')\n",
        "        if train_loss < best_vloss:\n",
        "            best_vloss = train_loss\n",
        "            path_save_cp = './cp/'\n",
        "            if not os.path.exists(path_save_cp): os.mkdir(path_save_cp)\n",
        "            torch.save(model.state_dict(), path_save_cp+model_save_path)\n",
        "\n",
        "\n",
        "    save_training_logs(training_logs_so, f\"LOG_DANN_{src}_to_{tar}\")\n",
        "\n",
        "    return training_logs_so"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SswggKTcmzN8"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "G_DzHU4zhUQD",
        "outputId": "5dace70d-febd-4726-ff90-44e789889217"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[14/33] Loss: 1.3432\n",
            "[15/33] Loss: 1.4044\n",
            "[16/33] Loss: 1.4881\n",
            "[17/33] Loss: 1.3512\n",
            "[18/33] Loss: 1.2544\n",
            "[19/33] Loss: 1.4965\n",
            "[20/33] Loss: 1.3022\n",
            "[21/33] Loss: 1.2550\n",
            "[22/33] Loss: 1.3485\n",
            "[23/33] Loss: 1.3157\n",
            "[24/33] Loss: 1.3171\n",
            "[25/33] Loss: 1.3430\n",
            "[26/33] Loss: 1.3305\n",
            "[27/33] Loss: 1.3705\n",
            "[28/33] Loss: 1.2899\n",
            "[29/33] Loss: 1.5059\n",
            "[30/33] Loss: 1.3082\n",
            "[31/33] Loss: 1.2883\n",
            "[32/33] Loss: 1.3414\n",
            "[33/33] Loss: 1.4896\n",
            "Epoch: 48 | Source Accuracy: 0.9985, Target Accuracy: 0.5207, Loss: 1.3696\n",
            "Epoch 0049 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3821\n",
            "[2/33] Loss: 1.4603\n",
            "[3/33] Loss: 1.3449\n",
            "[4/33] Loss: 1.3193\n",
            "[5/33] Loss: 1.3540\n",
            "[6/33] Loss: 1.3097\n",
            "[7/33] Loss: 1.2994\n",
            "[8/33] Loss: 1.3497\n",
            "[9/33] Loss: 1.2594\n",
            "[10/33] Loss: 1.4231\n",
            "[11/33] Loss: 1.5293\n",
            "[12/33] Loss: 1.3237\n",
            "[13/33] Loss: 1.2836\n",
            "[14/33] Loss: 1.3267\n",
            "[15/33] Loss: 1.3580\n",
            "[16/33] Loss: 1.5088\n",
            "[17/33] Loss: 1.3139\n",
            "[18/33] Loss: 1.3432\n",
            "[19/33] Loss: 1.2935\n",
            "[20/33] Loss: 1.3283\n",
            "[21/33] Loss: 1.3593\n",
            "[22/33] Loss: 1.3315\n",
            "[23/33] Loss: 1.2938\n",
            "[24/33] Loss: 1.3043\n",
            "[25/33] Loss: 1.4080\n",
            "[26/33] Loss: 1.3453\n",
            "[27/33] Loss: 1.4899\n",
            "[28/33] Loss: 1.5172\n",
            "[29/33] Loss: 1.3129\n",
            "[30/33] Loss: 1.4858\n",
            "[31/33] Loss: 1.3847\n",
            "[32/33] Loss: 1.3219\n",
            "[33/33] Loss: 1.2974\n",
            "Epoch: 49 | Source Accuracy: 0.9995, Target Accuracy: 0.5263, Loss: 1.3625\n",
            "Epoch 0050 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3895\n",
            "[2/33] Loss: 1.3603\n",
            "[3/33] Loss: 1.3754\n",
            "[4/33] Loss: 1.4855\n",
            "[5/33] Loss: 1.3275\n",
            "[6/33] Loss: 1.5986\n",
            "[7/33] Loss: 1.2989\n",
            "[8/33] Loss: 1.2444\n",
            "[9/33] Loss: 1.2032\n",
            "[10/33] Loss: 1.1342\n",
            "[11/33] Loss: 1.4029\n",
            "[12/33] Loss: 1.3534\n",
            "[13/33] Loss: 1.3230\n",
            "[14/33] Loss: 1.4851\n",
            "[15/33] Loss: 1.2903\n",
            "[16/33] Loss: 1.4308\n",
            "[17/33] Loss: 1.3264\n",
            "[18/33] Loss: 1.2721\n",
            "[19/33] Loss: 1.2518\n",
            "[20/33] Loss: 1.2633\n",
            "[21/33] Loss: 1.4091\n",
            "[22/33] Loss: 1.3909\n",
            "[23/33] Loss: 1.4112\n",
            "[24/33] Loss: 1.3489\n",
            "[25/33] Loss: 1.4864\n",
            "[26/33] Loss: 1.4354\n",
            "[27/33] Loss: 1.6250\n",
            "[28/33] Loss: 1.2861\n",
            "[29/33] Loss: 1.2138\n",
            "[30/33] Loss: 1.3322\n",
            "[31/33] Loss: 1.4433\n",
            "[32/33] Loss: 1.4535\n",
            "[33/33] Loss: 1.4598\n",
            "Epoch: 50 | Source Accuracy: 0.9990, Target Accuracy: 0.5348, Loss: 1.3670\n",
            "Training logs saved to LOG_DANN_amazon_to_webcam\n",
            "Epoch 0001 / 0050\n",
            "============\n",
            "[1/33] Loss: 3.4272\n",
            "[2/33] Loss: 3.4377\n",
            "[3/33] Loss: 3.4456\n",
            "[4/33] Loss: 3.4357\n",
            "[5/33] Loss: 3.4291\n",
            "[6/33] Loss: 3.4050\n",
            "[7/33] Loss: 3.3796\n",
            "[8/33] Loss: 3.3964\n",
            "[9/33] Loss: 3.4037\n",
            "[10/33] Loss: 3.4173\n",
            "[11/33] Loss: 3.4032\n",
            "[12/33] Loss: 3.3775\n",
            "[13/33] Loss: 3.3501\n",
            "[14/33] Loss: 3.3630\n",
            "[15/33] Loss: 3.4072\n",
            "[16/33] Loss: 3.3885\n",
            "[17/33] Loss: 3.3779\n",
            "[18/33] Loss: 3.3361\n",
            "[19/33] Loss: 3.3092\n",
            "[20/33] Loss: 3.3213\n",
            "[21/33] Loss: 3.3659\n",
            "[22/33] Loss: 3.3525\n",
            "[23/33] Loss: 3.3311\n",
            "[24/33] Loss: 3.2908\n",
            "[25/33] Loss: 3.2566\n",
            "[26/33] Loss: 3.2870\n",
            "[27/33] Loss: 3.3418\n",
            "[28/33] Loss: 3.3239\n",
            "[29/33] Loss: 3.2884\n",
            "[30/33] Loss: 3.2427\n",
            "[31/33] Loss: 3.2142\n",
            "[32/33] Loss: 3.2520\n",
            "[33/33] Loss: 3.3121\n",
            "Epoch: 1 | Source Accuracy: 0.2324, Target Accuracy: 0.0749, Loss: 3.3537\n",
            "Epoch 0002 / 0050\n",
            "============\n",
            "[1/33] Loss: 3.2359\n",
            "[2/33] Loss: 3.2510\n",
            "[3/33] Loss: 3.2762\n",
            "[4/33] Loss: 3.1589\n",
            "[5/33] Loss: 3.2235\n",
            "[6/33] Loss: 3.1783\n",
            "[7/33] Loss: 3.1692\n",
            "[8/33] Loss: 3.1952\n",
            "[9/33] Loss: 3.2165\n",
            "[10/33] Loss: 3.0888\n",
            "[11/33] Loss: 3.1674\n",
            "[12/33] Loss: 3.0959\n",
            "[13/33] Loss: 3.0879\n",
            "[14/33] Loss: 3.1295\n",
            "[15/33] Loss: 3.1574\n",
            "[16/33] Loss: 2.9928\n",
            "[17/33] Loss: 3.0983\n",
            "[18/33] Loss: 3.0026\n",
            "[19/33] Loss: 3.0043\n",
            "[20/33] Loss: 3.0466\n",
            "[21/33] Loss: 3.0863\n",
            "[22/33] Loss: 2.8757\n",
            "[23/33] Loss: 2.9970\n",
            "[24/33] Loss: 2.8757\n",
            "[25/33] Loss: 2.9022\n",
            "[26/33] Loss: 2.9462\n",
            "[27/33] Loss: 2.9809\n",
            "[28/33] Loss: 2.7402\n",
            "[29/33] Loss: 2.8755\n",
            "[30/33] Loss: 2.7471\n",
            "[31/33] Loss: 2.7945\n",
            "[32/33] Loss: 2.8138\n",
            "[33/33] Loss: 2.8618\n",
            "Epoch: 2 | Source Accuracy: 0.4069, Target Accuracy: 0.1094, Loss: 3.0386\n",
            "Epoch 0003 / 0050\n",
            "============\n",
            "[1/33] Loss: 2.7012\n",
            "[2/33] Loss: 2.7033\n",
            "[3/33] Loss: 2.5404\n",
            "[4/33] Loss: 2.8299\n",
            "[5/33] Loss: 2.6822\n",
            "[6/33] Loss: 2.5825\n",
            "[7/33] Loss: 2.5218\n",
            "[8/33] Loss: 2.5608\n",
            "[9/33] Loss: 2.3446\n",
            "[10/33] Loss: 2.6625\n",
            "[11/33] Loss: 2.5324\n",
            "[12/33] Loss: 2.3893\n",
            "[13/33] Loss: 2.3197\n",
            "[14/33] Loss: 2.3890\n",
            "[15/33] Loss: 2.1400\n",
            "[16/33] Loss: 2.4782\n",
            "[17/33] Loss: 2.3695\n",
            "[18/33] Loss: 2.2017\n",
            "[19/33] Loss: 2.1139\n",
            "[20/33] Loss: 2.2160\n",
            "[21/33] Loss: 1.9335\n",
            "[22/33] Loss: 2.2613\n",
            "[23/33] Loss: 2.1923\n",
            "[24/33] Loss: 2.0182\n",
            "[25/33] Loss: 1.9082\n",
            "[26/33] Loss: 1.9948\n",
            "[27/33] Loss: 1.7149\n",
            "[28/33] Loss: 2.0557\n",
            "[29/33] Loss: 1.9833\n",
            "[30/33] Loss: 1.7512\n",
            "[31/33] Loss: 1.6930\n",
            "[32/33] Loss: 1.7893\n",
            "[33/33] Loss: 1.5071\n",
            "Epoch: 3 | Source Accuracy: 0.6208, Target Accuracy: 0.1570, Loss: 2.2449\n",
            "Epoch 0004 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.6091\n",
            "[2/33] Loss: 1.6061\n",
            "[3/33] Loss: 1.5907\n",
            "[4/33] Loss: 1.4575\n",
            "[5/33] Loss: 1.6220\n",
            "[6/33] Loss: 1.6506\n",
            "[7/33] Loss: 1.3610\n",
            "[8/33] Loss: 1.3795\n",
            "[9/33] Loss: 1.3660\n",
            "[10/33] Loss: 1.2955\n",
            "[11/33] Loss: 1.3788\n",
            "[12/33] Loss: 1.4197\n",
            "[13/33] Loss: 1.1072\n",
            "[14/33] Loss: 1.1979\n",
            "[15/33] Loss: 1.1571\n",
            "[16/33] Loss: 1.0880\n",
            "[17/33] Loss: 1.1568\n",
            "[18/33] Loss: 1.1663\n",
            "[19/33] Loss: 0.9189\n",
            "[20/33] Loss: 1.0017\n",
            "[21/33] Loss: 0.9718\n",
            "[22/33] Loss: 0.8997\n",
            "[23/33] Loss: 0.9481\n",
            "[24/33] Loss: 0.9443\n",
            "[25/33] Loss: 0.7970\n",
            "[26/33] Loss: 0.8729\n",
            "[27/33] Loss: 0.7597\n",
            "[28/33] Loss: 0.7495\n",
            "[29/33] Loss: 0.8086\n",
            "[30/33] Loss: 0.7639\n",
            "[31/33] Loss: 0.6899\n",
            "[32/33] Loss: 0.7140\n",
            "[33/33] Loss: 0.6622\n",
            "Epoch: 4 | Source Accuracy: 0.8751, Target Accuracy: 0.2397, Loss: 1.1246\n",
            "Epoch 0005 / 0050\n",
            "============\n",
            "[1/33] Loss: 0.5829\n",
            "[2/33] Loss: 0.6962\n",
            "[3/33] Loss: 0.7202\n",
            "[4/33] Loss: 0.6400\n",
            "[5/33] Loss: 0.6944\n",
            "[6/33] Loss: 0.5595\n",
            "[7/33] Loss: 0.5266\n",
            "[8/33] Loss: 0.5902\n",
            "[9/33] Loss: 0.6344\n",
            "[10/33] Loss: 0.6092\n",
            "[11/33] Loss: 0.6653\n",
            "[12/33] Loss: 0.4889\n",
            "[13/33] Loss: 0.5077\n",
            "[14/33] Loss: 0.5790\n",
            "[15/33] Loss: 0.5953\n",
            "[16/33] Loss: 0.5202\n",
            "[17/33] Loss: 0.6195\n",
            "[18/33] Loss: 0.5202\n",
            "[19/33] Loss: 0.5062\n",
            "[20/33] Loss: 0.5542\n",
            "[21/33] Loss: 0.5801\n",
            "[22/33] Loss: 0.5349\n",
            "[23/33] Loss: 0.6172\n",
            "[24/33] Loss: 0.6079\n",
            "[25/33] Loss: 0.5092\n",
            "[26/33] Loss: 0.6071\n",
            "[27/33] Loss: 0.6409\n",
            "[28/33] Loss: 0.6357\n",
            "[29/33] Loss: 0.6178\n",
            "[30/33] Loss: 0.6300\n",
            "[31/33] Loss: 0.5686\n",
            "[32/33] Loss: 0.6520\n",
            "[33/33] Loss: 0.8461\n",
            "Epoch: 5 | Source Accuracy: 0.9723, Target Accuracy: 0.2645, Loss: 0.6017\n",
            "Epoch 0006 / 0050\n",
            "============\n",
            "[1/33] Loss: 0.6974\n",
            "[2/33] Loss: 0.6613\n",
            "[3/33] Loss: 0.8999\n",
            "[4/33] Loss: 0.8692\n",
            "[5/33] Loss: 0.7711\n",
            "[6/33] Loss: 0.7698\n",
            "[7/33] Loss: 0.8465\n",
            "[8/33] Loss: 0.7944\n",
            "[9/33] Loss: 1.0040\n",
            "[10/33] Loss: 0.9888\n",
            "[11/33] Loss: 0.9270\n",
            "[12/33] Loss: 0.9647\n",
            "[13/33] Loss: 0.9911\n",
            "[14/33] Loss: 0.9097\n",
            "[15/33] Loss: 1.0486\n",
            "[16/33] Loss: 1.0396\n",
            "[17/33] Loss: 0.9632\n",
            "[18/33] Loss: 0.8911\n",
            "[19/33] Loss: 0.9877\n",
            "[20/33] Loss: 1.0263\n",
            "[21/33] Loss: 1.1603\n",
            "[22/33] Loss: 1.0959\n",
            "[23/33] Loss: 0.9144\n",
            "[24/33] Loss: 0.9466\n",
            "[25/33] Loss: 0.9595\n",
            "[26/33] Loss: 0.9937\n",
            "[27/33] Loss: 1.0351\n",
            "[28/33] Loss: 0.9499\n",
            "[29/33] Loss: 0.8665\n",
            "[30/33] Loss: 0.9408\n",
            "[31/33] Loss: 0.8136\n",
            "[32/33] Loss: 0.8614\n",
            "[33/33] Loss: 1.0131\n",
            "Epoch: 6 | Source Accuracy: 0.9893, Target Accuracy: 0.2309, Loss: 0.9273\n",
            "Epoch 0007 / 0050\n",
            "============\n",
            "[1/33] Loss: 0.8591\n",
            "[2/33] Loss: 0.9202\n",
            "[3/33] Loss: 0.9655\n",
            "[4/33] Loss: 0.9400\n",
            "[5/33] Loss: 0.8859\n",
            "[6/33] Loss: 0.8530\n",
            "[7/33] Loss: 0.8904\n",
            "[8/33] Loss: 0.9456\n",
            "[9/33] Loss: 0.9697\n",
            "[10/33] Loss: 0.9302\n",
            "[11/33] Loss: 0.9159\n",
            "[12/33] Loss: 0.9654\n",
            "[13/33] Loss: 0.9830\n",
            "[14/33] Loss: 1.0130\n",
            "[15/33] Loss: 1.0425\n",
            "[16/33] Loss: 0.9275\n",
            "[17/33] Loss: 0.8921\n",
            "[18/33] Loss: 0.9201\n",
            "[19/33] Loss: 0.8964\n",
            "[20/33] Loss: 0.9342\n",
            "[21/33] Loss: 0.9412\n",
            "[22/33] Loss: 0.9127\n",
            "[23/33] Loss: 0.9378\n",
            "[24/33] Loss: 0.8373\n",
            "[25/33] Loss: 0.8381\n",
            "[26/33] Loss: 0.7899\n",
            "[27/33] Loss: 0.8065\n",
            "[28/33] Loss: 0.8704\n",
            "[29/33] Loss: 0.8102\n",
            "[30/33] Loss: 0.7744\n",
            "[31/33] Loss: 0.8083\n",
            "[32/33] Loss: 0.8052\n",
            "[33/33] Loss: 0.8219\n",
            "Epoch: 7 | Source Accuracy: 0.9956, Target Accuracy: 0.2377, Loss: 0.8971\n",
            "Epoch 0008 / 0050\n",
            "============\n",
            "[1/33] Loss: 0.8094\n",
            "[2/33] Loss: 0.8576\n",
            "[3/33] Loss: 0.7731\n",
            "[4/33] Loss: 0.8866\n",
            "[5/33] Loss: 0.8101\n",
            "[6/33] Loss: 0.9680\n",
            "[7/33] Loss: 0.8605\n",
            "[8/33] Loss: 0.8982\n",
            "[9/33] Loss: 0.8340\n",
            "[10/33] Loss: 0.8846\n",
            "[11/33] Loss: 0.8644\n",
            "[12/33] Loss: 0.9185\n",
            "[13/33] Loss: 0.9161\n",
            "[14/33] Loss: 0.9633\n",
            "[15/33] Loss: 0.9558\n",
            "[16/33] Loss: 0.8793\n",
            "[17/33] Loss: 0.9169\n",
            "[18/33] Loss: 0.9164\n",
            "[19/33] Loss: 0.9194\n",
            "[20/33] Loss: 0.9063\n",
            "[21/33] Loss: 0.9317\n",
            "[22/33] Loss: 0.8751\n",
            "[23/33] Loss: 0.8921\n",
            "[24/33] Loss: 0.8159\n",
            "[25/33] Loss: 0.8874\n",
            "[26/33] Loss: 0.9383\n",
            "[27/33] Loss: 0.9311\n",
            "[28/33] Loss: 0.8523\n",
            "[29/33] Loss: 0.9343\n",
            "[30/33] Loss: 0.8291\n",
            "[31/33] Loss: 0.9506\n",
            "[32/33] Loss: 0.8459\n",
            "[33/33] Loss: 0.9305\n",
            "Epoch: 8 | Source Accuracy: 0.9971, Target Accuracy: 0.2309, Loss: 0.8895\n",
            "Epoch 0009 / 0050\n",
            "============\n",
            "[1/33] Loss: 0.9275\n",
            "[2/33] Loss: 0.9662\n",
            "[3/33] Loss: 0.9668\n",
            "[4/33] Loss: 0.8978\n",
            "[5/33] Loss: 0.9490\n",
            "[6/33] Loss: 0.9559\n",
            "[7/33] Loss: 0.9495\n",
            "[8/33] Loss: 0.9543\n",
            "[9/33] Loss: 0.9434\n",
            "[10/33] Loss: 0.9316\n",
            "[11/33] Loss: 0.8978\n",
            "[12/33] Loss: 0.9540\n",
            "[13/33] Loss: 0.8950\n",
            "[14/33] Loss: 0.9128\n",
            "[15/33] Loss: 0.9412\n",
            "[16/33] Loss: 0.8713\n",
            "[17/33] Loss: 0.8799\n",
            "[18/33] Loss: 0.8723\n",
            "[19/33] Loss: 0.9188\n",
            "[20/33] Loss: 0.9412\n",
            "[21/33] Loss: 0.8922\n",
            "[22/33] Loss: 0.8612\n",
            "[23/33] Loss: 0.9743\n",
            "[24/33] Loss: 0.9200\n",
            "[25/33] Loss: 0.8504\n",
            "[26/33] Loss: 0.8689\n",
            "[27/33] Loss: 0.9198\n",
            "[28/33] Loss: 0.9633\n",
            "[29/33] Loss: 1.0099\n",
            "[30/33] Loss: 1.0293\n",
            "[31/33] Loss: 0.9229\n",
            "[32/33] Loss: 0.9447\n",
            "[33/33] Loss: 1.1143\n",
            "Epoch: 9 | Source Accuracy: 0.9903, Target Accuracy: 0.2659, Loss: 0.9333\n",
            "Epoch 0010 / 0050\n",
            "============\n",
            "[1/33] Loss: 0.9943\n",
            "[2/33] Loss: 0.9885\n",
            "[3/33] Loss: 1.0852\n",
            "[4/33] Loss: 1.1697\n",
            "[5/33] Loss: 1.0813\n",
            "[6/33] Loss: 0.9974\n",
            "[7/33] Loss: 0.9740\n",
            "[8/33] Loss: 0.9522\n",
            "[9/33] Loss: 1.2837\n",
            "[10/33] Loss: 1.0532\n",
            "[11/33] Loss: 1.0805\n",
            "[12/33] Loss: 1.0080\n",
            "[13/33] Loss: 1.0193\n",
            "[14/33] Loss: 1.0503\n",
            "[15/33] Loss: 1.1570\n",
            "[16/33] Loss: 1.0594\n",
            "[17/33] Loss: 1.0217\n",
            "[18/33] Loss: 1.1119\n",
            "[19/33] Loss: 1.1860\n",
            "[20/33] Loss: 1.1081\n",
            "[21/33] Loss: 1.2243\n",
            "[22/33] Loss: 1.0209\n",
            "[23/33] Loss: 0.9752\n",
            "[24/33] Loss: 1.1984\n",
            "[25/33] Loss: 1.0118\n",
            "[26/33] Loss: 1.0967\n",
            "[27/33] Loss: 1.1096\n",
            "[28/33] Loss: 1.0980\n",
            "[29/33] Loss: 1.0057\n",
            "[30/33] Loss: 0.9943\n",
            "[31/33] Loss: 1.0060\n",
            "[32/33] Loss: 1.0281\n",
            "[33/33] Loss: 1.0610\n",
            "Epoch: 10 | Source Accuracy: 0.9830, Target Accuracy: 0.2684, Loss: 1.0670\n",
            "Epoch 0011 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.1290\n",
            "[2/33] Loss: 1.1617\n",
            "[3/33] Loss: 1.2144\n",
            "[4/33] Loss: 1.2094\n",
            "[5/33] Loss: 1.0864\n",
            "[6/33] Loss: 1.1175\n",
            "[7/33] Loss: 1.0518\n",
            "[8/33] Loss: 1.2472\n",
            "[9/33] Loss: 1.1217\n",
            "[10/33] Loss: 1.1903\n",
            "[11/33] Loss: 1.1634\n",
            "[12/33] Loss: 1.1755\n",
            "[13/33] Loss: 1.1468\n",
            "[14/33] Loss: 1.1570\n",
            "[15/33] Loss: 1.1186\n",
            "[16/33] Loss: 1.3317\n",
            "[17/33] Loss: 1.1477\n",
            "[18/33] Loss: 1.1831\n",
            "[19/33] Loss: 1.1937\n",
            "[20/33] Loss: 1.1256\n",
            "[21/33] Loss: 1.2487\n",
            "[22/33] Loss: 1.3503\n",
            "[23/33] Loss: 1.2930\n",
            "[24/33] Loss: 1.2816\n",
            "[25/33] Loss: 1.5607\n",
            "[26/33] Loss: 1.1433\n",
            "[27/33] Loss: 1.1282\n",
            "[28/33] Loss: 1.4199\n",
            "[29/33] Loss: 1.1802\n",
            "[30/33] Loss: 1.0985\n",
            "[31/33] Loss: 1.0648\n",
            "[32/33] Loss: 1.1877\n",
            "[33/33] Loss: 1.4431\n",
            "Epoch: 11 | Source Accuracy: 0.9844, Target Accuracy: 0.2173, Loss: 1.2022\n",
            "Epoch 0012 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.0985\n",
            "[2/33] Loss: 1.2075\n",
            "[3/33] Loss: 1.1286\n",
            "[4/33] Loss: 1.1542\n",
            "[5/33] Loss: 1.1851\n",
            "[6/33] Loss: 1.0982\n",
            "[7/33] Loss: 1.2019\n",
            "[8/33] Loss: 1.1668\n",
            "[9/33] Loss: 1.4559\n",
            "[10/33] Loss: 1.1737\n",
            "[11/33] Loss: 1.2302\n",
            "[12/33] Loss: 1.3058\n",
            "[13/33] Loss: 1.3711\n",
            "[14/33] Loss: 1.3430\n",
            "[15/33] Loss: 1.3105\n",
            "[16/33] Loss: 1.2215\n",
            "[17/33] Loss: 1.3627\n",
            "[18/33] Loss: 1.2262\n",
            "[19/33] Loss: 1.3015\n",
            "[20/33] Loss: 1.2115\n",
            "[21/33] Loss: 1.1161\n",
            "[22/33] Loss: 1.2385\n",
            "[23/33] Loss: 1.2829\n",
            "[24/33] Loss: 1.1969\n",
            "[25/33] Loss: 1.3196\n",
            "[26/33] Loss: 1.1358\n",
            "[27/33] Loss: 1.1849\n",
            "[28/33] Loss: 1.2766\n",
            "[29/33] Loss: 1.1193\n",
            "[30/33] Loss: 1.1149\n",
            "[31/33] Loss: 1.3470\n",
            "[32/33] Loss: 1.1743\n",
            "[33/33] Loss: 1.1899\n",
            "Epoch: 12 | Source Accuracy: 0.9713, Target Accuracy: 0.2528, Loss: 1.2258\n",
            "Epoch 0013 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.1403\n",
            "[2/33] Loss: 1.3201\n",
            "[3/33] Loss: 1.3152\n",
            "[4/33] Loss: 1.3676\n",
            "[5/33] Loss: 1.1316\n",
            "[6/33] Loss: 1.1953\n",
            "[7/33] Loss: 1.1634\n",
            "[8/33] Loss: 1.3195\n",
            "[9/33] Loss: 1.1294\n",
            "[10/33] Loss: 1.1276\n",
            "[11/33] Loss: 1.0090\n",
            "[12/33] Loss: 0.9691\n",
            "[13/33] Loss: 1.0595\n",
            "[14/33] Loss: 1.2206\n",
            "[15/33] Loss: 1.0086\n",
            "[16/33] Loss: 1.0346\n",
            "[17/33] Loss: 1.1105\n",
            "[18/33] Loss: 1.3151\n",
            "[19/33] Loss: 1.0925\n",
            "[20/33] Loss: 1.0048\n",
            "[21/33] Loss: 1.6354\n",
            "[22/33] Loss: 1.5069\n",
            "[23/33] Loss: 1.1308\n",
            "[24/33] Loss: 1.0859\n",
            "[25/33] Loss: 1.2018\n",
            "[26/33] Loss: 1.1805\n",
            "[27/33] Loss: 1.2493\n",
            "[28/33] Loss: 1.1149\n",
            "[29/33] Loss: 1.3346\n",
            "[30/33] Loss: 1.2539\n",
            "[31/33] Loss: 1.1739\n",
            "[32/33] Loss: 1.0419\n",
            "[33/33] Loss: 1.1236\n",
            "Epoch: 13 | Source Accuracy: 0.9737, Target Accuracy: 0.2620, Loss: 1.1839\n",
            "Epoch 0014 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3137\n",
            "[2/33] Loss: 1.3125\n",
            "[3/33] Loss: 1.5449\n",
            "[4/33] Loss: 1.1803\n",
            "[5/33] Loss: 1.2525\n",
            "[6/33] Loss: 1.4332\n",
            "[7/33] Loss: 1.2670\n",
            "[8/33] Loss: 1.3828\n",
            "[9/33] Loss: 1.2489\n",
            "[10/33] Loss: 1.1823\n",
            "[11/33] Loss: 1.3605\n",
            "[12/33] Loss: 1.5184\n",
            "[13/33] Loss: 1.1245\n",
            "[14/33] Loss: 1.3115\n",
            "[15/33] Loss: 1.3229\n",
            "[16/33] Loss: 1.2718\n",
            "[17/33] Loss: 1.2510\n",
            "[18/33] Loss: 1.4472\n",
            "[19/33] Loss: 1.4445\n",
            "[20/33] Loss: 1.1543\n",
            "[21/33] Loss: 1.2050\n",
            "[22/33] Loss: 1.1490\n",
            "[23/33] Loss: 1.5716\n",
            "[24/33] Loss: 1.2650\n",
            "[25/33] Loss: 1.2467\n",
            "[26/33] Loss: 1.1860\n",
            "[27/33] Loss: 1.1400\n",
            "[28/33] Loss: 1.1359\n",
            "[29/33] Loss: 1.4010\n",
            "[30/33] Loss: 1.4183\n",
            "[31/33] Loss: 1.2090\n",
            "[32/33] Loss: 1.1698\n",
            "[33/33] Loss: 1.2234\n",
            "Epoch: 14 | Source Accuracy: 0.9669, Target Accuracy: 0.2504, Loss: 1.2923\n",
            "Epoch 0015 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.1917\n",
            "[2/33] Loss: 1.3082\n",
            "[3/33] Loss: 1.2495\n",
            "[4/33] Loss: 1.2205\n",
            "[5/33] Loss: 1.4264\n",
            "[6/33] Loss: 1.4963\n",
            "[7/33] Loss: 1.1424\n",
            "[8/33] Loss: 1.2939\n",
            "[9/33] Loss: 1.3597\n",
            "[10/33] Loss: 1.2462\n",
            "[11/33] Loss: 1.2078\n",
            "[12/33] Loss: 1.3665\n",
            "[13/33] Loss: 1.4311\n",
            "[14/33] Loss: 1.3269\n",
            "[15/33] Loss: 1.3454\n",
            "[16/33] Loss: 1.4201\n",
            "[17/33] Loss: 1.2806\n",
            "[18/33] Loss: 1.2149\n",
            "[19/33] Loss: 1.3413\n",
            "[20/33] Loss: 1.4757\n",
            "[21/33] Loss: 1.3279\n",
            "[22/33] Loss: 1.2944\n",
            "[23/33] Loss: 1.2550\n",
            "[24/33] Loss: 1.2705\n",
            "[25/33] Loss: 1.3059\n",
            "[26/33] Loss: 1.0341\n",
            "[27/33] Loss: 1.1949\n",
            "[28/33] Loss: 1.5381\n",
            "[29/33] Loss: 1.6189\n",
            "[30/33] Loss: 1.1893\n",
            "[31/33] Loss: 1.2840\n",
            "[32/33] Loss: 1.1316\n",
            "[33/33] Loss: 1.3402\n",
            "Epoch: 15 | Source Accuracy: 0.9728, Target Accuracy: 0.2659, Loss: 1.3070\n",
            "Epoch 0016 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.5185\n",
            "[2/33] Loss: 1.3631\n",
            "[3/33] Loss: 1.2249\n",
            "[4/33] Loss: 1.2572\n",
            "[5/33] Loss: 1.5394\n",
            "[6/33] Loss: 1.3814\n",
            "[7/33] Loss: 1.4219\n",
            "[8/33] Loss: 1.4449\n",
            "[9/33] Loss: 1.4962\n",
            "[10/33] Loss: 1.1802\n",
            "[11/33] Loss: 1.3715\n",
            "[12/33] Loss: 1.3261\n",
            "[13/33] Loss: 1.2675\n",
            "[14/33] Loss: 1.2100\n",
            "[15/33] Loss: 1.5671\n",
            "[16/33] Loss: 1.3847\n",
            "[17/33] Loss: 1.2779\n",
            "[18/33] Loss: 1.1931\n",
            "[19/33] Loss: 1.2198\n",
            "[20/33] Loss: 1.0859\n",
            "[21/33] Loss: 1.2253\n",
            "[22/33] Loss: 1.4779\n",
            "[23/33] Loss: 1.3652\n",
            "[24/33] Loss: 1.3809\n",
            "[25/33] Loss: 1.4283\n",
            "[26/33] Loss: 1.1766\n",
            "[27/33] Loss: 1.1988\n",
            "[28/33] Loss: 1.4009\n",
            "[29/33] Loss: 1.2750\n",
            "[30/33] Loss: 1.2281\n",
            "[31/33] Loss: 1.2587\n",
            "[32/33] Loss: 1.1610\n",
            "[33/33] Loss: 1.3325\n",
            "Epoch: 16 | Source Accuracy: 0.9757, Target Accuracy: 0.2591, Loss: 1.3224\n",
            "Epoch 0017 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.4197\n",
            "[2/33] Loss: 1.3045\n",
            "[3/33] Loss: 1.2924\n",
            "[4/33] Loss: 1.3131\n",
            "[5/33] Loss: 1.6461\n",
            "[6/33] Loss: 1.4998\n",
            "[7/33] Loss: 1.3184\n",
            "[8/33] Loss: 1.3233\n",
            "[9/33] Loss: 1.2587\n",
            "[10/33] Loss: 1.3216\n",
            "[11/33] Loss: 1.1936\n",
            "[12/33] Loss: 1.2395\n",
            "[13/33] Loss: 1.3121\n",
            "[14/33] Loss: 1.4603\n",
            "[15/33] Loss: 1.4995\n",
            "[16/33] Loss: 1.3378\n",
            "[17/33] Loss: 1.2414\n",
            "[18/33] Loss: 1.2722\n",
            "[19/33] Loss: 1.3054\n",
            "[20/33] Loss: 1.2961\n",
            "[21/33] Loss: 1.3669\n",
            "[22/33] Loss: 1.3218\n",
            "[23/33] Loss: 1.2273\n",
            "[24/33] Loss: 1.4925\n",
            "[25/33] Loss: 1.3482\n",
            "[26/33] Loss: 1.3096\n",
            "[27/33] Loss: 1.6810\n",
            "[28/33] Loss: 1.3318\n",
            "[29/33] Loss: 1.3393\n",
            "[30/33] Loss: 1.5109\n",
            "[31/33] Loss: 1.3892\n",
            "[32/33] Loss: 1.4220\n",
            "[33/33] Loss: 1.3646\n",
            "Epoch: 17 | Source Accuracy: 0.9810, Target Accuracy: 0.2543, Loss: 1.3624\n",
            "Epoch 0018 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.5927\n",
            "[2/33] Loss: 1.3968\n",
            "[3/33] Loss: 1.5096\n",
            "[4/33] Loss: 1.6181\n",
            "[5/33] Loss: 1.3032\n",
            "[6/33] Loss: 1.3276\n",
            "[7/33] Loss: 1.4693\n",
            "[8/33] Loss: 1.2184\n",
            "[9/33] Loss: 1.3656\n",
            "[10/33] Loss: 1.5651\n",
            "[11/33] Loss: 1.4808\n",
            "[12/33] Loss: 1.3880\n",
            "[13/33] Loss: 1.3099\n",
            "[14/33] Loss: 1.3953\n",
            "[15/33] Loss: 1.2733\n",
            "[16/33] Loss: 1.4779\n",
            "[17/33] Loss: 1.3858\n",
            "[18/33] Loss: 1.3318\n",
            "[19/33] Loss: 1.2770\n",
            "[20/33] Loss: 1.2630\n",
            "[21/33] Loss: 1.2577\n",
            "[22/33] Loss: 1.5048\n",
            "[23/33] Loss: 1.6403\n",
            "[24/33] Loss: 1.3950\n",
            "[25/33] Loss: 1.4434\n",
            "[26/33] Loss: 1.8260\n",
            "[27/33] Loss: 1.3312\n",
            "[28/33] Loss: 1.5529\n",
            "[29/33] Loss: 1.6685\n",
            "[30/33] Loss: 1.3652\n",
            "[31/33] Loss: 1.6506\n",
            "[32/33] Loss: 1.6825\n",
            "[33/33] Loss: 1.7058\n",
            "Epoch: 18 | Source Accuracy: 0.9903, Target Accuracy: 0.2567, Loss: 1.4537\n",
            "Epoch 0019 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.6109\n",
            "[2/33] Loss: 1.7118\n",
            "[3/33] Loss: 1.6332\n",
            "[4/33] Loss: 1.5077\n",
            "[5/33] Loss: 1.4490\n",
            "[6/33] Loss: 2.6684\n",
            "[7/33] Loss: 1.5812\n",
            "[8/33] Loss: 1.3176\n",
            "[9/33] Loss: 3.2648\n",
            "[10/33] Loss: 1.1505\n",
            "[11/33] Loss: 1.6507\n",
            "[12/33] Loss: 1.7948\n",
            "[13/33] Loss: 1.9149\n",
            "[14/33] Loss: 1.5301\n",
            "[15/33] Loss: 1.8335\n",
            "[16/33] Loss: 1.4265\n",
            "[17/33] Loss: 1.2758\n",
            "[18/33] Loss: 1.4207\n",
            "[19/33] Loss: 1.3661\n",
            "[20/33] Loss: 1.6003\n",
            "[21/33] Loss: 1.4867\n",
            "[22/33] Loss: 1.4797\n",
            "[23/33] Loss: 1.2014\n",
            "[24/33] Loss: 1.2707\n",
            "[25/33] Loss: 1.3966\n",
            "[26/33] Loss: 1.3322\n",
            "[27/33] Loss: 1.4260\n",
            "[28/33] Loss: 1.4462\n",
            "[29/33] Loss: 1.7735\n",
            "[30/33] Loss: 1.3649\n",
            "[31/33] Loss: 1.2200\n",
            "[32/33] Loss: 1.4660\n",
            "[33/33] Loss: 1.3755\n",
            "Epoch: 19 | Source Accuracy: 0.9806, Target Accuracy: 0.2518, Loss: 1.5742\n",
            "Epoch 0020 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.5613\n",
            "[2/33] Loss: 1.5289\n",
            "[3/33] Loss: 1.4339\n",
            "[4/33] Loss: 1.9227\n",
            "[5/33] Loss: 1.5489\n",
            "[6/33] Loss: 1.4431\n",
            "[7/33] Loss: 1.7227\n",
            "[8/33] Loss: 1.2995\n",
            "[9/33] Loss: 1.4277\n",
            "[10/33] Loss: 1.3563\n",
            "[11/33] Loss: 1.4407\n",
            "[12/33] Loss: 1.2246\n",
            "[13/33] Loss: 1.8054\n",
            "[14/33] Loss: 1.2403\n",
            "[15/33] Loss: 1.4492\n",
            "[16/33] Loss: 1.3049\n",
            "[17/33] Loss: 1.5700\n",
            "[18/33] Loss: 1.2257\n",
            "[19/33] Loss: 1.4270\n",
            "[20/33] Loss: 1.4024\n",
            "[21/33] Loss: 1.4092\n",
            "[22/33] Loss: 1.4268\n",
            "[23/33] Loss: 1.4440\n",
            "[24/33] Loss: 1.3360\n",
            "[25/33] Loss: 1.3020\n",
            "[26/33] Loss: 1.1961\n",
            "[27/33] Loss: 1.1554\n",
            "[28/33] Loss: 1.4930\n",
            "[29/33] Loss: 1.3924\n",
            "[30/33] Loss: 1.2045\n",
            "[31/33] Loss: 1.3370\n",
            "[32/33] Loss: 1.2545\n",
            "[33/33] Loss: 1.2172\n",
            "Epoch: 20 | Source Accuracy: 0.9830, Target Accuracy: 0.2722, Loss: 1.4092\n",
            "Epoch 0021 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.4784\n",
            "[2/33] Loss: 1.2584\n",
            "[3/33] Loss: 1.5535\n",
            "[4/33] Loss: 1.5470\n",
            "[5/33] Loss: 1.9363\n",
            "[6/33] Loss: 1.9480\n",
            "[7/33] Loss: 1.6336\n",
            "[8/33] Loss: 1.5774\n",
            "[9/33] Loss: 1.7296\n",
            "[10/33] Loss: 1.5992\n",
            "[11/33] Loss: 1.9481\n",
            "[12/33] Loss: 1.5678\n",
            "[13/33] Loss: 1.6390\n",
            "[14/33] Loss: 1.4619\n",
            "[15/33] Loss: 1.6481\n",
            "[16/33] Loss: 1.4238\n",
            "[17/33] Loss: 1.2979\n",
            "[18/33] Loss: 1.3529\n",
            "[19/33] Loss: 1.2035\n",
            "[20/33] Loss: 1.0368\n",
            "[21/33] Loss: 1.2258\n",
            "[22/33] Loss: 1.1383\n",
            "[23/33] Loss: 1.0498\n",
            "[24/33] Loss: 0.9244\n",
            "[25/33] Loss: 1.3458\n",
            "[26/33] Loss: 1.1463\n",
            "[27/33] Loss: 0.9505\n",
            "[28/33] Loss: 1.1725\n",
            "[29/33] Loss: 1.4941\n",
            "[30/33] Loss: 1.4431\n",
            "[31/33] Loss: 1.4014\n",
            "[32/33] Loss: 2.0176\n",
            "[33/33] Loss: 1.8870\n",
            "Epoch: 21 | Source Accuracy: 0.9767, Target Accuracy: 0.2630, Loss: 1.4557\n",
            "Epoch 0022 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.5501\n",
            "[2/33] Loss: 1.7670\n",
            "[3/33] Loss: 1.4375\n",
            "[4/33] Loss: 1.6255\n",
            "[5/33] Loss: 1.4596\n",
            "[6/33] Loss: 1.7522\n",
            "[7/33] Loss: 1.6039\n",
            "[8/33] Loss: 2.0073\n",
            "[9/33] Loss: 1.6743\n",
            "[10/33] Loss: 1.5816\n",
            "[11/33] Loss: 1.4688\n",
            "[12/33] Loss: 1.5721\n",
            "[13/33] Loss: 1.5220\n",
            "[14/33] Loss: 1.6808\n",
            "[15/33] Loss: 1.4224\n",
            "[16/33] Loss: 1.6641\n",
            "[17/33] Loss: 1.2920\n",
            "[18/33] Loss: 1.3019\n",
            "[19/33] Loss: 1.3985\n",
            "[20/33] Loss: 1.5040\n",
            "[21/33] Loss: 1.2935\n",
            "[22/33] Loss: 1.3618\n",
            "[23/33] Loss: 1.3634\n",
            "[24/33] Loss: 1.5361\n",
            "[25/33] Loss: 1.4690\n",
            "[26/33] Loss: 1.7133\n",
            "[27/33] Loss: 1.3834\n",
            "[28/33] Loss: 1.3095\n",
            "[29/33] Loss: 1.4589\n",
            "[30/33] Loss: 1.1920\n",
            "[31/33] Loss: 1.4006\n",
            "[32/33] Loss: 1.8305\n",
            "[33/33] Loss: 1.3690\n",
            "Epoch: 22 | Source Accuracy: 0.9786, Target Accuracy: 0.2727, Loss: 1.5141\n",
            "Epoch 0023 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3912\n",
            "[2/33] Loss: 1.3867\n",
            "[3/33] Loss: 1.2980\n",
            "[4/33] Loss: 1.4650\n",
            "[5/33] Loss: 1.1626\n",
            "[6/33] Loss: 1.4037\n",
            "[7/33] Loss: 1.3733\n",
            "[8/33] Loss: 1.8033\n",
            "[9/33] Loss: 1.4931\n",
            "[10/33] Loss: 1.2198\n",
            "[11/33] Loss: 1.2520\n",
            "[12/33] Loss: 1.4963\n",
            "[13/33] Loss: 2.1180\n",
            "[14/33] Loss: 1.2576\n",
            "[15/33] Loss: 1.4702\n",
            "[16/33] Loss: 1.5606\n",
            "[17/33] Loss: 1.4672\n",
            "[18/33] Loss: 2.1509\n",
            "[19/33] Loss: 1.5034\n",
            "[20/33] Loss: 1.5494\n",
            "[21/33] Loss: 1.8107\n",
            "[22/33] Loss: 1.5710\n",
            "[23/33] Loss: 1.7844\n",
            "[24/33] Loss: 1.7081\n",
            "[25/33] Loss: 1.6539\n",
            "[26/33] Loss: 1.6732\n",
            "[27/33] Loss: 1.5898\n",
            "[28/33] Loss: 1.4990\n",
            "[29/33] Loss: 1.4808\n",
            "[30/33] Loss: 1.4198\n",
            "[31/33] Loss: 1.5091\n",
            "[32/33] Loss: 1.2632\n",
            "[33/33] Loss: 1.1835\n",
            "Epoch: 23 | Source Accuracy: 0.9878, Target Accuracy: 0.2615, Loss: 1.5142\n",
            "Epoch 0024 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.2974\n",
            "[2/33] Loss: 1.8960\n",
            "[3/33] Loss: 1.1471\n",
            "[4/33] Loss: 1.7110\n",
            "[5/33] Loss: 1.5963\n",
            "[6/33] Loss: 1.8023\n",
            "[7/33] Loss: 1.3655\n",
            "[8/33] Loss: 1.5579\n",
            "[9/33] Loss: 1.6286\n",
            "[10/33] Loss: 1.6894\n",
            "[11/33] Loss: 1.7225\n",
            "[12/33] Loss: 1.3845\n",
            "[13/33] Loss: 1.7819\n",
            "[14/33] Loss: 1.4252\n",
            "[15/33] Loss: 2.8493\n",
            "[16/33] Loss: 1.4639\n",
            "[17/33] Loss: 1.9040\n",
            "[18/33] Loss: 1.5475\n",
            "[19/33] Loss: 1.6307\n",
            "[20/33] Loss: 1.5764\n",
            "[21/33] Loss: 1.5715\n",
            "[22/33] Loss: 1.4779\n",
            "[23/33] Loss: 1.3664\n",
            "[24/33] Loss: 1.3737\n",
            "[25/33] Loss: 1.5198\n",
            "[26/33] Loss: 1.5937\n",
            "[27/33] Loss: 1.3732\n",
            "[28/33] Loss: 1.3031\n",
            "[29/33] Loss: 1.3978\n",
            "[30/33] Loss: 1.3417\n",
            "[31/33] Loss: 1.4182\n",
            "[32/33] Loss: 1.3416\n",
            "[33/33] Loss: 1.4170\n",
            "Epoch: 24 | Source Accuracy: 0.9893, Target Accuracy: 0.2615, Loss: 1.5598\n",
            "Epoch 0025 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3864\n",
            "[2/33] Loss: 1.4345\n",
            "[3/33] Loss: 1.3361\n",
            "[4/33] Loss: 1.4015\n",
            "[5/33] Loss: 1.2317\n",
            "[6/33] Loss: 1.4185\n",
            "[7/33] Loss: 1.2609\n",
            "[8/33] Loss: 1.3812\n",
            "[9/33] Loss: 1.3580\n",
            "[10/33] Loss: 1.2700\n",
            "[11/33] Loss: 1.4446\n",
            "[12/33] Loss: 1.2993\n",
            "[13/33] Loss: 1.4199\n",
            "[14/33] Loss: 1.2992\n",
            "[15/33] Loss: 1.2501\n",
            "[16/33] Loss: 1.1885\n",
            "[17/33] Loss: 1.3341\n",
            "[18/33] Loss: 1.2396\n",
            "[19/33] Loss: 1.4765\n",
            "[20/33] Loss: 1.4998\n",
            "[21/33] Loss: 1.3675\n",
            "[22/33] Loss: 1.2970\n",
            "[23/33] Loss: 1.4360\n",
            "[24/33] Loss: 1.2853\n",
            "[25/33] Loss: 1.2965\n",
            "[26/33] Loss: 1.4001\n",
            "[27/33] Loss: 1.3373\n",
            "[28/33] Loss: 1.2956\n",
            "[29/33] Loss: 1.3008\n",
            "[30/33] Loss: 1.4358\n",
            "[31/33] Loss: 1.3884\n",
            "[32/33] Loss: 1.3672\n",
            "[33/33] Loss: 1.2979\n",
            "Epoch: 25 | Source Accuracy: 0.9990, Target Accuracy: 0.2737, Loss: 1.3465\n",
            "Epoch 0026 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3546\n",
            "[2/33] Loss: 1.4051\n",
            "[3/33] Loss: 1.3362\n",
            "[4/33] Loss: 1.6263\n",
            "[5/33] Loss: 1.5104\n",
            "[6/33] Loss: 1.3895\n",
            "[7/33] Loss: 1.5643\n",
            "[8/33] Loss: 1.3981\n",
            "[9/33] Loss: 1.3268\n",
            "[10/33] Loss: 1.5180\n",
            "[11/33] Loss: 1.4214\n",
            "[12/33] Loss: 1.4783\n",
            "[13/33] Loss: 1.4074\n",
            "[14/33] Loss: 1.3556\n",
            "[15/33] Loss: 1.4261\n",
            "[16/33] Loss: 1.4475\n",
            "[17/33] Loss: 1.3969\n",
            "[18/33] Loss: 1.3711\n",
            "[19/33] Loss: 1.4068\n",
            "[20/33] Loss: 1.3658\n",
            "[21/33] Loss: 1.3502\n",
            "[22/33] Loss: 1.6580\n",
            "[23/33] Loss: 1.3488\n",
            "[24/33] Loss: 1.3239\n",
            "[25/33] Loss: 1.3713\n",
            "[26/33] Loss: 1.4025\n",
            "[27/33] Loss: 1.3533\n",
            "[28/33] Loss: 1.3336\n",
            "[29/33] Loss: 1.4014\n",
            "[30/33] Loss: 1.5309\n",
            "[31/33] Loss: 1.3919\n",
            "[32/33] Loss: 1.3453\n",
            "[33/33] Loss: 1.3446\n",
            "Epoch: 26 | Source Accuracy: 0.9966, Target Accuracy: 0.2742, Loss: 1.4140\n",
            "Epoch 0027 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3778\n",
            "[2/33] Loss: 1.3694\n",
            "[3/33] Loss: 1.2898\n",
            "[4/33] Loss: 1.2903\n",
            "[5/33] Loss: 1.3812\n",
            "[6/33] Loss: 1.4501\n",
            "[7/33] Loss: 1.4149\n",
            "[8/33] Loss: 1.3378\n",
            "[9/33] Loss: 1.3188\n",
            "[10/33] Loss: 1.3293\n",
            "[11/33] Loss: 1.3616\n",
            "[12/33] Loss: 1.3622\n",
            "[13/33] Loss: 1.3938\n",
            "[14/33] Loss: 1.3209\n",
            "[15/33] Loss: 1.3395\n",
            "[16/33] Loss: 1.2899\n",
            "[17/33] Loss: 1.3569\n",
            "[18/33] Loss: 1.3996\n",
            "[19/33] Loss: 1.3254\n",
            "[20/33] Loss: 1.3597\n",
            "[21/33] Loss: 1.2855\n",
            "[22/33] Loss: 1.3567\n",
            "[23/33] Loss: 1.3651\n",
            "[24/33] Loss: 1.3808\n",
            "[25/33] Loss: 1.4143\n",
            "[26/33] Loss: 1.3556\n",
            "[27/33] Loss: 1.3555\n",
            "[28/33] Loss: 1.3851\n",
            "[29/33] Loss: 1.3616\n",
            "[30/33] Loss: 1.4444\n",
            "[31/33] Loss: 1.4068\n",
            "[32/33] Loss: 1.4128\n",
            "[33/33] Loss: 1.3686\n",
            "Epoch: 27 | Source Accuracy: 0.9985, Target Accuracy: 0.2917, Loss: 1.3625\n",
            "Epoch 0028 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3498\n",
            "[2/33] Loss: 1.4162\n",
            "[3/33] Loss: 1.3660\n",
            "[4/33] Loss: 1.4114\n",
            "[5/33] Loss: 1.4452\n",
            "[6/33] Loss: 1.4290\n",
            "[7/33] Loss: 1.2975\n",
            "[8/33] Loss: 1.3264\n",
            "[9/33] Loss: 1.3358\n",
            "[10/33] Loss: 1.4027\n",
            "[11/33] Loss: 1.3448\n",
            "[12/33] Loss: 1.3386\n",
            "[13/33] Loss: 1.3364\n",
            "[14/33] Loss: 1.2900\n",
            "[15/33] Loss: 1.3175\n",
            "[16/33] Loss: 1.3874\n",
            "[17/33] Loss: 1.2675\n",
            "[18/33] Loss: 1.3662\n",
            "[19/33] Loss: 1.3176\n",
            "[20/33] Loss: 1.2940\n",
            "[21/33] Loss: 1.3545\n",
            "[22/33] Loss: 1.3981\n",
            "[23/33] Loss: 1.3028\n",
            "[24/33] Loss: 1.2482\n",
            "[25/33] Loss: 1.3186\n",
            "[26/33] Loss: 1.3086\n",
            "[27/33] Loss: 1.3408\n",
            "[28/33] Loss: 1.3571\n",
            "[29/33] Loss: 1.4027\n",
            "[30/33] Loss: 1.2522\n",
            "[31/33] Loss: 1.3823\n",
            "[32/33] Loss: 1.4282\n",
            "[33/33] Loss: 1.4266\n",
            "Epoch: 28 | Source Accuracy: 0.9985, Target Accuracy: 0.2917, Loss: 1.3503\n",
            "Epoch 0029 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.4391\n",
            "[2/33] Loss: 1.4403\n",
            "[3/33] Loss: 1.5847\n",
            "[4/33] Loss: 1.5262\n",
            "[5/33] Loss: 1.5202\n",
            "[6/33] Loss: 1.4401\n",
            "[7/33] Loss: 1.2538\n",
            "[8/33] Loss: 1.4742\n",
            "[9/33] Loss: 1.4964\n",
            "[10/33] Loss: 1.4616\n",
            "[11/33] Loss: 1.6066\n",
            "[12/33] Loss: 1.7016\n",
            "[13/33] Loss: 1.5661\n",
            "[14/33] Loss: 1.5606\n",
            "[15/33] Loss: 1.4761\n",
            "[16/33] Loss: 1.5663\n",
            "[17/33] Loss: 1.4248\n",
            "[18/33] Loss: 1.4059\n",
            "[19/33] Loss: 1.4585\n",
            "[20/33] Loss: 1.3517\n",
            "[21/33] Loss: 1.4344\n",
            "[22/33] Loss: 1.3844\n",
            "[23/33] Loss: 1.4269\n",
            "[24/33] Loss: 1.5262\n",
            "[25/33] Loss: 1.4616\n",
            "[26/33] Loss: 1.3712\n",
            "[27/33] Loss: 1.5003\n",
            "[28/33] Loss: 1.4941\n",
            "[29/33] Loss: 1.4124\n",
            "[30/33] Loss: 1.4140\n",
            "[31/33] Loss: 1.5079\n",
            "[32/33] Loss: 1.4056\n",
            "[33/33] Loss: 1.4349\n",
            "Epoch: 29 | Source Accuracy: 0.9985, Target Accuracy: 0.2708, Loss: 1.4706\n",
            "Epoch 0030 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3711\n",
            "[2/33] Loss: 1.4995\n",
            "[3/33] Loss: 1.3847\n",
            "[4/33] Loss: 1.5044\n",
            "[5/33] Loss: 1.4685\n",
            "[6/33] Loss: 1.4439\n",
            "[7/33] Loss: 1.3921\n",
            "[8/33] Loss: 1.5055\n",
            "[9/33] Loss: 1.4273\n",
            "[10/33] Loss: 1.5090\n",
            "[11/33] Loss: 1.4379\n",
            "[12/33] Loss: 1.5268\n",
            "[13/33] Loss: 1.4412\n",
            "[14/33] Loss: 1.5936\n",
            "[15/33] Loss: 1.5075\n",
            "[16/33] Loss: 1.5394\n",
            "[17/33] Loss: 1.4330\n",
            "[18/33] Loss: 1.3699\n",
            "[19/33] Loss: 1.5605\n",
            "[20/33] Loss: 1.4902\n",
            "[21/33] Loss: 1.4687\n",
            "[22/33] Loss: 1.4614\n",
            "[23/33] Loss: 1.5847\n",
            "[24/33] Loss: 1.4196\n",
            "[25/33] Loss: 1.5203\n",
            "[26/33] Loss: 1.5168\n",
            "[27/33] Loss: 1.4562\n",
            "[28/33] Loss: 1.4310\n",
            "[29/33] Loss: 1.4721\n",
            "[30/33] Loss: 1.4164\n",
            "[31/33] Loss: 1.4295\n",
            "[32/33] Loss: 1.4510\n",
            "[33/33] Loss: 1.3552\n",
            "Epoch: 30 | Source Accuracy: 1.0000, Target Accuracy: 0.2713, Loss: 1.4663\n",
            "Epoch 0031 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3712\n",
            "[2/33] Loss: 1.4104\n",
            "[3/33] Loss: 1.3994\n",
            "[4/33] Loss: 1.4531\n",
            "[5/33] Loss: 1.3492\n",
            "[6/33] Loss: 1.3789\n",
            "[7/33] Loss: 1.2566\n",
            "[8/33] Loss: 1.4182\n",
            "[9/33] Loss: 1.3134\n",
            "[10/33] Loss: 1.3673\n",
            "[11/33] Loss: 1.3711\n",
            "[12/33] Loss: 1.2985\n",
            "[13/33] Loss: 1.3292\n",
            "[14/33] Loss: 1.3812\n",
            "[15/33] Loss: 1.2895\n",
            "[16/33] Loss: 1.4754\n",
            "[17/33] Loss: 1.4084\n",
            "[18/33] Loss: 1.4031\n",
            "[19/33] Loss: 1.4205\n",
            "[20/33] Loss: 1.4142\n",
            "[21/33] Loss: 1.5099\n",
            "[22/33] Loss: 1.4273\n",
            "[23/33] Loss: 1.5585\n",
            "[24/33] Loss: 1.3675\n",
            "[25/33] Loss: 1.4760\n",
            "[26/33] Loss: 1.5082\n",
            "[27/33] Loss: 1.3936\n",
            "[28/33] Loss: 1.3947\n",
            "[29/33] Loss: 1.5805\n",
            "[30/33] Loss: 1.4553\n",
            "[31/33] Loss: 1.4549\n",
            "[32/33] Loss: 1.4163\n",
            "[33/33] Loss: 1.4748\n",
            "Epoch: 31 | Source Accuracy: 0.9995, Target Accuracy: 0.2897, Loss: 1.4099\n",
            "Epoch 0032 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3801\n",
            "[2/33] Loss: 1.4425\n",
            "[3/33] Loss: 1.4643\n",
            "[4/33] Loss: 1.4887\n",
            "[5/33] Loss: 1.4742\n",
            "[6/33] Loss: 1.5529\n",
            "[7/33] Loss: 1.4373\n",
            "[8/33] Loss: 1.4147\n",
            "[9/33] Loss: 1.4409\n",
            "[10/33] Loss: 1.4489\n",
            "[11/33] Loss: 1.4779\n",
            "[12/33] Loss: 1.4522\n",
            "[13/33] Loss: 1.4366\n",
            "[14/33] Loss: 1.4823\n",
            "[15/33] Loss: 1.4690\n",
            "[16/33] Loss: 1.4473\n",
            "[17/33] Loss: 1.3633\n",
            "[18/33] Loss: 1.4604\n",
            "[19/33] Loss: 1.4614\n",
            "[20/33] Loss: 1.4497\n",
            "[21/33] Loss: 1.4907\n",
            "[22/33] Loss: 1.4983\n",
            "[23/33] Loss: 1.4032\n",
            "[24/33] Loss: 1.4011\n",
            "[25/33] Loss: 1.3498\n",
            "[26/33] Loss: 1.4242\n",
            "[27/33] Loss: 1.3951\n",
            "[28/33] Loss: 1.4305\n",
            "[29/33] Loss: 1.3655\n",
            "[30/33] Loss: 1.3468\n",
            "[31/33] Loss: 1.3966\n",
            "[32/33] Loss: 1.3408\n",
            "[33/33] Loss: 1.4452\n",
            "Epoch: 32 | Source Accuracy: 0.9990, Target Accuracy: 0.2844, Loss: 1.4343\n",
            "Epoch 0033 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3705\n",
            "[2/33] Loss: 1.6197\n",
            "[3/33] Loss: 1.4079\n",
            "[4/33] Loss: 2.0879\n",
            "[5/33] Loss: 1.3329\n",
            "[6/33] Loss: 1.5235\n",
            "[7/33] Loss: 1.6541\n",
            "[8/33] Loss: 1.3758\n",
            "[9/33] Loss: 1.5316\n",
            "[10/33] Loss: 1.4150\n",
            "[11/33] Loss: 1.3537\n",
            "[12/33] Loss: 1.3924\n",
            "[13/33] Loss: 1.5236\n",
            "[14/33] Loss: 1.5161\n",
            "[15/33] Loss: 1.4937\n",
            "[16/33] Loss: 1.3207\n",
            "[17/33] Loss: 1.6341\n",
            "[18/33] Loss: 1.5926\n",
            "[19/33] Loss: 2.5733\n",
            "[20/33] Loss: 1.4148\n",
            "[21/33] Loss: 1.4719\n",
            "[22/33] Loss: 1.5049\n",
            "[23/33] Loss: 1.6023\n",
            "[24/33] Loss: 1.5616\n",
            "[25/33] Loss: 1.7403\n",
            "[26/33] Loss: 1.6318\n",
            "[27/33] Loss: 1.3996\n",
            "[28/33] Loss: 1.4733\n",
            "[29/33] Loss: 1.4826\n",
            "[30/33] Loss: 1.5283\n",
            "[31/33] Loss: 1.4837\n",
            "[32/33] Loss: 1.6590\n",
            "[33/33] Loss: 1.8748\n",
            "Epoch: 33 | Source Accuracy: 0.9903, Target Accuracy: 0.2421, Loss: 1.5621\n",
            "Epoch 0034 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.7060\n",
            "[2/33] Loss: 1.4511\n",
            "[3/33] Loss: 1.5915\n",
            "[4/33] Loss: 1.4229\n",
            "[5/33] Loss: 1.5282\n",
            "[6/33] Loss: 1.3983\n",
            "[7/33] Loss: 1.4970\n",
            "[8/33] Loss: 1.4698\n",
            "[9/33] Loss: 1.3797\n",
            "[10/33] Loss: 1.5015\n",
            "[11/33] Loss: 1.3489\n",
            "[12/33] Loss: 1.3178\n",
            "[13/33] Loss: 1.4881\n",
            "[14/33] Loss: 1.4073\n",
            "[15/33] Loss: 1.3765\n",
            "[16/33] Loss: 1.3391\n",
            "[17/33] Loss: 1.3375\n",
            "[18/33] Loss: 1.3269\n",
            "[19/33] Loss: 1.4415\n",
            "[20/33] Loss: 1.3911\n",
            "[21/33] Loss: 1.3759\n",
            "[22/33] Loss: 1.4081\n",
            "[23/33] Loss: 1.3595\n",
            "[24/33] Loss: 1.2729\n",
            "[25/33] Loss: 1.4378\n",
            "[26/33] Loss: 1.4049\n",
            "[27/33] Loss: 1.3999\n",
            "[28/33] Loss: 1.3648\n",
            "[29/33] Loss: 1.3782\n",
            "[30/33] Loss: 1.3374\n",
            "[31/33] Loss: 1.4526\n",
            "[32/33] Loss: 1.3569\n",
            "[33/33] Loss: 1.4531\n",
            "Epoch: 34 | Source Accuracy: 0.9951, Target Accuracy: 0.2363, Loss: 1.4158\n",
            "Epoch 0035 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3924\n",
            "[2/33] Loss: 1.3921\n",
            "[3/33] Loss: 1.4104\n",
            "[4/33] Loss: 1.3819\n",
            "[5/33] Loss: 1.5680\n",
            "[6/33] Loss: 1.4640\n",
            "[7/33] Loss: 1.4047\n",
            "[8/33] Loss: 1.3699\n",
            "[9/33] Loss: 1.4614\n",
            "[10/33] Loss: 1.3550\n",
            "[11/33] Loss: 1.3786\n",
            "[12/33] Loss: 1.3769\n",
            "[13/33] Loss: 1.3985\n",
            "[14/33] Loss: 1.4246\n",
            "[15/33] Loss: 1.4383\n",
            "[16/33] Loss: 1.3360\n",
            "[17/33] Loss: 1.3705\n",
            "[18/33] Loss: 1.4285\n",
            "[19/33] Loss: 1.4154\n",
            "[20/33] Loss: 1.3610\n",
            "[21/33] Loss: 1.4022\n",
            "[22/33] Loss: 1.2777\n",
            "[23/33] Loss: 1.3448\n",
            "[24/33] Loss: 1.3837\n",
            "[25/33] Loss: 1.3496\n",
            "[26/33] Loss: 1.3691\n",
            "[27/33] Loss: 1.3861\n",
            "[28/33] Loss: 1.2956\n",
            "[29/33] Loss: 1.3100\n",
            "[30/33] Loss: 1.3651\n",
            "[31/33] Loss: 1.3020\n",
            "[32/33] Loss: 1.4135\n",
            "[33/33] Loss: 1.3718\n",
            "Epoch: 35 | Source Accuracy: 0.9985, Target Accuracy: 0.2533, Loss: 1.3848\n",
            "Epoch 0036 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3326\n",
            "[2/33] Loss: 1.2781\n",
            "[3/33] Loss: 1.4021\n",
            "[4/33] Loss: 1.2649\n",
            "[5/33] Loss: 1.3614\n",
            "[6/33] Loss: 1.4104\n",
            "[7/33] Loss: 1.2797\n",
            "[8/33] Loss: 1.3059\n",
            "[9/33] Loss: 1.3871\n",
            "[10/33] Loss: 1.2739\n",
            "[11/33] Loss: 1.2806\n",
            "[12/33] Loss: 1.4145\n",
            "[13/33] Loss: 1.2621\n",
            "[14/33] Loss: 1.2930\n",
            "[15/33] Loss: 1.3484\n",
            "[16/33] Loss: 1.3139\n",
            "[17/33] Loss: 1.3257\n",
            "[18/33] Loss: 1.3215\n",
            "[19/33] Loss: 1.3369\n",
            "[20/33] Loss: 1.3342\n",
            "[21/33] Loss: 1.3487\n",
            "[22/33] Loss: 1.3620\n",
            "[23/33] Loss: 1.2776\n",
            "[24/33] Loss: 1.4328\n",
            "[25/33] Loss: 1.3711\n",
            "[26/33] Loss: 1.3314\n",
            "[27/33] Loss: 1.3696\n",
            "[28/33] Loss: 1.3129\n",
            "[29/33] Loss: 1.3600\n",
            "[30/33] Loss: 1.4358\n",
            "[31/33] Loss: 1.3506\n",
            "[32/33] Loss: 1.4073\n",
            "[33/33] Loss: 1.3488\n",
            "Epoch: 36 | Source Accuracy: 0.9981, Target Accuracy: 0.2518, Loss: 1.3405\n",
            "Epoch 0037 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.4286\n",
            "[2/33] Loss: 1.5434\n",
            "[3/33] Loss: 1.4638\n",
            "[4/33] Loss: 1.3858\n",
            "[5/33] Loss: 1.3727\n",
            "[6/33] Loss: 1.4239\n",
            "[7/33] Loss: 1.3954\n",
            "[8/33] Loss: 1.3878\n",
            "[9/33] Loss: 1.3732\n",
            "[10/33] Loss: 1.3774\n",
            "[11/33] Loss: 1.4651\n",
            "[12/33] Loss: 1.4152\n",
            "[13/33] Loss: 1.3915\n",
            "[14/33] Loss: 1.4134\n",
            "[15/33] Loss: 1.4275\n",
            "[16/33] Loss: 1.3901\n",
            "[17/33] Loss: 1.4269\n",
            "[18/33] Loss: 1.3427\n",
            "[19/33] Loss: 1.4377\n",
            "[20/33] Loss: 1.4273\n",
            "[21/33] Loss: 1.3550\n",
            "[22/33] Loss: 1.3058\n",
            "[23/33] Loss: 1.3476\n",
            "[24/33] Loss: 1.3237\n",
            "[25/33] Loss: 1.3662\n",
            "[26/33] Loss: 1.3559\n",
            "[27/33] Loss: 1.2689\n",
            "[28/33] Loss: 1.3675\n",
            "[29/33] Loss: 1.3455\n",
            "[30/33] Loss: 1.3410\n",
            "[31/33] Loss: 1.3450\n",
            "[32/33] Loss: 1.3404\n",
            "[33/33] Loss: 1.2920\n",
            "Epoch: 37 | Source Accuracy: 0.9985, Target Accuracy: 0.2640, Loss: 1.3831\n",
            "Epoch 0038 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3860\n",
            "[2/33] Loss: 1.3380\n",
            "[3/33] Loss: 1.4828\n",
            "[4/33] Loss: 1.2689\n",
            "[5/33] Loss: 1.3891\n",
            "[6/33] Loss: 1.6206\n",
            "[7/33] Loss: 1.3820\n",
            "[8/33] Loss: 1.3932\n",
            "[9/33] Loss: 1.3907\n",
            "[10/33] Loss: 1.3558\n",
            "[11/33] Loss: 1.3754\n",
            "[12/33] Loss: 1.3336\n",
            "[13/33] Loss: 1.3508\n",
            "[14/33] Loss: 1.4326\n",
            "[15/33] Loss: 1.3966\n",
            "[16/33] Loss: 1.2959\n",
            "[17/33] Loss: 1.4110\n",
            "[18/33] Loss: 1.3523\n",
            "[19/33] Loss: 1.3947\n",
            "[20/33] Loss: 1.4644\n",
            "[21/33] Loss: 1.4368\n",
            "[22/33] Loss: 1.3019\n",
            "[23/33] Loss: 1.4254\n",
            "[24/33] Loss: 1.4747\n",
            "[25/33] Loss: 1.4783\n",
            "[26/33] Loss: 1.3850\n",
            "[27/33] Loss: 1.4284\n",
            "[28/33] Loss: 1.4049\n",
            "[29/33] Loss: 1.4545\n",
            "[30/33] Loss: 1.3623\n",
            "[31/33] Loss: 1.5519\n",
            "[32/33] Loss: 1.4322\n",
            "[33/33] Loss: 1.6121\n",
            "Epoch: 38 | Source Accuracy: 0.9951, Target Accuracy: 0.2718, Loss: 1.4110\n",
            "Epoch 0039 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.4879\n",
            "[2/33] Loss: 1.8540\n",
            "[3/33] Loss: 1.4980\n",
            "[4/33] Loss: 1.4911\n",
            "[5/33] Loss: 1.4149\n",
            "[6/33] Loss: 1.4221\n",
            "[7/33] Loss: 1.4124\n",
            "[8/33] Loss: 1.6378\n",
            "[9/33] Loss: 1.4334\n",
            "[10/33] Loss: 1.5415\n",
            "[11/33] Loss: 1.3673\n",
            "[12/33] Loss: 1.4620\n",
            "[13/33] Loss: 1.4263\n",
            "[14/33] Loss: 1.5682\n",
            "[15/33] Loss: 1.4799\n",
            "[16/33] Loss: 1.4443\n",
            "[17/33] Loss: 1.4483\n",
            "[18/33] Loss: 1.4754\n",
            "[19/33] Loss: 1.4963\n",
            "[20/33] Loss: 1.5222\n",
            "[21/33] Loss: 1.4153\n",
            "[22/33] Loss: 1.4208\n",
            "[23/33] Loss: 1.4102\n",
            "[24/33] Loss: 1.3489\n",
            "[25/33] Loss: 1.4288\n",
            "[26/33] Loss: 1.4863\n",
            "[27/33] Loss: 1.3819\n",
            "[28/33] Loss: 1.3368\n",
            "[29/33] Loss: 1.3003\n",
            "[30/33] Loss: 1.3045\n",
            "[31/33] Loss: 1.3880\n",
            "[32/33] Loss: 1.4760\n",
            "[33/33] Loss: 1.2068\n",
            "Epoch: 39 | Source Accuracy: 0.9985, Target Accuracy: 0.2897, Loss: 1.4481\n",
            "Epoch 0040 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.2750\n",
            "[2/33] Loss: 1.3337\n",
            "[3/33] Loss: 1.2197\n",
            "[4/33] Loss: 1.4465\n",
            "[5/33] Loss: 1.1172\n",
            "[6/33] Loss: 1.2455\n",
            "[7/33] Loss: 1.2048\n",
            "[8/33] Loss: 1.2166\n",
            "[9/33] Loss: 1.2452\n",
            "[10/33] Loss: 1.3653\n",
            "[11/33] Loss: 1.1571\n",
            "[12/33] Loss: 1.2629\n",
            "[13/33] Loss: 1.1155\n",
            "[14/33] Loss: 1.2320\n",
            "[15/33] Loss: 1.2420\n",
            "[16/33] Loss: 1.3969\n",
            "[17/33] Loss: 1.2731\n",
            "[18/33] Loss: 1.3605\n",
            "[19/33] Loss: 1.3714\n",
            "[20/33] Loss: 1.4177\n",
            "[21/33] Loss: 1.2748\n",
            "[22/33] Loss: 1.4451\n",
            "[23/33] Loss: 1.2839\n",
            "[24/33] Loss: 1.3628\n",
            "[25/33] Loss: 1.2993\n",
            "[26/33] Loss: 1.3728\n",
            "[27/33] Loss: 1.4691\n",
            "[28/33] Loss: 1.4042\n",
            "[29/33] Loss: 1.2600\n",
            "[30/33] Loss: 1.3174\n",
            "[31/33] Loss: 1.3773\n",
            "[32/33] Loss: 1.3768\n",
            "[33/33] Loss: 1.4305\n",
            "Epoch: 40 | Source Accuracy: 0.9981, Target Accuracy: 0.2834, Loss: 1.3083\n",
            "Epoch 0041 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3608\n",
            "[2/33] Loss: 1.3578\n",
            "[3/33] Loss: 1.3152\n",
            "[4/33] Loss: 1.5557\n",
            "[5/33] Loss: 1.4653\n",
            "[6/33] Loss: 1.4725\n",
            "[7/33] Loss: 1.3462\n",
            "[8/33] Loss: 1.4350\n",
            "[9/33] Loss: 1.4125\n",
            "[10/33] Loss: 1.3908\n",
            "[11/33] Loss: 1.4764\n",
            "[12/33] Loss: 1.4906\n",
            "[13/33] Loss: 1.3469\n",
            "[14/33] Loss: 1.3354\n",
            "[15/33] Loss: 1.4542\n",
            "[16/33] Loss: 1.5409\n",
            "[17/33] Loss: 1.4871\n",
            "[18/33] Loss: 1.4604\n",
            "[19/33] Loss: 1.4626\n",
            "[20/33] Loss: 1.4006\n",
            "[21/33] Loss: 1.4961\n",
            "[22/33] Loss: 1.3873\n",
            "[23/33] Loss: 1.4321\n",
            "[24/33] Loss: 1.4309\n",
            "[25/33] Loss: 1.4805\n",
            "[26/33] Loss: 1.4447\n",
            "[27/33] Loss: 1.4979\n",
            "[28/33] Loss: 1.5333\n",
            "[29/33] Loss: 1.5113\n",
            "[30/33] Loss: 1.4529\n",
            "[31/33] Loss: 1.5199\n",
            "[32/33] Loss: 1.4238\n",
            "[33/33] Loss: 1.4812\n",
            "Epoch: 41 | Source Accuracy: 0.9985, Target Accuracy: 0.2790, Loss: 1.4442\n",
            "Epoch 0042 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.5236\n",
            "[2/33] Loss: 1.5013\n",
            "[3/33] Loss: 1.4921\n",
            "[4/33] Loss: 1.4375\n",
            "[5/33] Loss: 1.6719\n",
            "[6/33] Loss: 1.5119\n",
            "[7/33] Loss: 1.4881\n",
            "[8/33] Loss: 1.3779\n",
            "[9/33] Loss: 1.6106\n",
            "[10/33] Loss: 1.3675\n",
            "[11/33] Loss: 1.5308\n",
            "[12/33] Loss: 1.5209\n",
            "[13/33] Loss: 1.5521\n",
            "[14/33] Loss: 1.4072\n",
            "[15/33] Loss: 1.5984\n",
            "[16/33] Loss: 1.5457\n",
            "[17/33] Loss: 1.5359\n",
            "[18/33] Loss: 1.5439\n",
            "[19/33] Loss: 1.4873\n",
            "[20/33] Loss: 1.5119\n",
            "[21/33] Loss: 1.5435\n",
            "[22/33] Loss: 1.4823\n",
            "[23/33] Loss: 1.5549\n",
            "[24/33] Loss: 1.6725\n",
            "[25/33] Loss: 1.5062\n",
            "[26/33] Loss: 1.5469\n",
            "[27/33] Loss: 1.7049\n",
            "[28/33] Loss: 1.3941\n",
            "[29/33] Loss: 1.5666\n",
            "[30/33] Loss: 1.5491\n",
            "[31/33] Loss: 1.5635\n",
            "[32/33] Loss: 1.4485\n",
            "[33/33] Loss: 1.6213\n",
            "Epoch: 42 | Source Accuracy: 0.9971, Target Accuracy: 0.2990, Loss: 1.5264\n",
            "Epoch 0043 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.5306\n",
            "[2/33] Loss: 1.4769\n",
            "[3/33] Loss: 1.4188\n",
            "[4/33] Loss: 1.6192\n",
            "[5/33] Loss: 1.4535\n",
            "[6/33] Loss: 1.5067\n",
            "[7/33] Loss: 1.4935\n",
            "[8/33] Loss: 1.5656\n",
            "[9/33] Loss: 1.3860\n",
            "[10/33] Loss: 1.5002\n",
            "[11/33] Loss: 1.5252\n",
            "[12/33] Loss: 1.6102\n",
            "[13/33] Loss: 1.4355\n",
            "[14/33] Loss: 1.4982\n",
            "[15/33] Loss: 1.4040\n",
            "[16/33] Loss: 1.4988\n",
            "[17/33] Loss: 1.3920\n",
            "[18/33] Loss: 1.5346\n",
            "[19/33] Loss: 1.3641\n",
            "[20/33] Loss: 1.3440\n",
            "[21/33] Loss: 1.4857\n",
            "[22/33] Loss: 1.4839\n",
            "[23/33] Loss: 1.4470\n",
            "[24/33] Loss: 1.4138\n",
            "[25/33] Loss: 1.2775\n",
            "[26/33] Loss: 1.3611\n",
            "[27/33] Loss: 1.3153\n",
            "[28/33] Loss: 1.5313\n",
            "[29/33] Loss: 1.3690\n",
            "[30/33] Loss: 1.2256\n",
            "[31/33] Loss: 1.2595\n",
            "[32/33] Loss: 1.4351\n",
            "[33/33] Loss: 1.4039\n",
            "Epoch: 43 | Source Accuracy: 0.9971, Target Accuracy: 0.2888, Loss: 1.4414\n",
            "Epoch 0044 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3970\n",
            "[2/33] Loss: 1.3889\n",
            "[3/33] Loss: 1.1358\n",
            "[4/33] Loss: 1.4856\n",
            "[5/33] Loss: 1.4436\n",
            "[6/33] Loss: 1.5161\n",
            "[7/33] Loss: 1.4925\n",
            "[8/33] Loss: 1.4793\n",
            "[9/33] Loss: 1.4035\n",
            "[10/33] Loss: 1.4958\n",
            "[11/33] Loss: 1.5027\n",
            "[12/33] Loss: 1.5794\n",
            "[13/33] Loss: 1.5516\n",
            "[14/33] Loss: 1.0842\n",
            "[15/33] Loss: 1.5339\n",
            "[16/33] Loss: 1.4804\n",
            "[17/33] Loss: 1.4889\n",
            "[18/33] Loss: 1.6133\n",
            "[19/33] Loss: 1.6392\n",
            "[20/33] Loss: 1.0965\n",
            "[21/33] Loss: 1.5203\n",
            "[22/33] Loss: 1.2957\n",
            "[23/33] Loss: 1.6705\n",
            "[24/33] Loss: 1.7100\n",
            "[25/33] Loss: 1.6504\n",
            "[26/33] Loss: 1.5380\n",
            "[27/33] Loss: 1.1744\n",
            "[28/33] Loss: 1.2175\n",
            "[29/33] Loss: 1.5907\n",
            "[30/33] Loss: 1.4188\n",
            "[31/33] Loss: 1.5196\n",
            "[32/33] Loss: 1.5507\n",
            "[33/33] Loss: 1.2565\n",
            "Epoch: 44 | Source Accuracy: 0.9971, Target Accuracy: 0.2834, Loss: 1.4522\n",
            "Epoch 0045 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.5084\n",
            "[2/33] Loss: 1.4763\n",
            "[3/33] Loss: 1.4039\n",
            "[4/33] Loss: 1.3941\n",
            "[5/33] Loss: 1.6248\n",
            "[6/33] Loss: 1.4119\n",
            "[7/33] Loss: 1.5491\n",
            "[8/33] Loss: 1.5457\n",
            "[9/33] Loss: 1.3576\n",
            "[10/33] Loss: 1.4708\n",
            "[11/33] Loss: 1.4318\n",
            "[12/33] Loss: 1.3007\n",
            "[13/33] Loss: 1.5538\n",
            "[14/33] Loss: 1.3420\n",
            "[15/33] Loss: 1.4217\n",
            "[16/33] Loss: 1.5015\n",
            "[17/33] Loss: 1.4888\n",
            "[18/33] Loss: 1.4888\n",
            "[19/33] Loss: 1.5508\n",
            "[20/33] Loss: 1.4112\n",
            "[21/33] Loss: 1.4089\n",
            "[22/33] Loss: 1.4270\n",
            "[23/33] Loss: 1.5690\n",
            "[24/33] Loss: 1.3928\n",
            "[25/33] Loss: 1.5117\n",
            "[26/33] Loss: 1.3930\n",
            "[27/33] Loss: 1.4708\n",
            "[28/33] Loss: 1.3749\n",
            "[29/33] Loss: 1.4891\n",
            "[30/33] Loss: 1.4892\n",
            "[31/33] Loss: 1.4845\n",
            "[32/33] Loss: 1.4459\n",
            "[33/33] Loss: 1.4816\n",
            "Epoch: 45 | Source Accuracy: 1.0000, Target Accuracy: 0.2912, Loss: 1.4598\n",
            "Epoch 0046 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.4404\n",
            "[2/33] Loss: 1.4589\n",
            "[3/33] Loss: 1.3554\n",
            "[4/33] Loss: 1.4155\n",
            "[5/33] Loss: 1.4244\n",
            "[6/33] Loss: 2.2900\n",
            "[7/33] Loss: 1.4773\n",
            "[8/33] Loss: 1.4331\n",
            "[9/33] Loss: 1.3389\n",
            "[10/33] Loss: 1.4469\n",
            "[11/33] Loss: 1.4818\n",
            "[12/33] Loss: 1.9886\n",
            "[13/33] Loss: 1.4069\n",
            "[14/33] Loss: 1.4766\n",
            "[15/33] Loss: 1.3719\n",
            "[16/33] Loss: 1.3566\n",
            "[17/33] Loss: 1.3906\n",
            "[18/33] Loss: 1.5363\n",
            "[19/33] Loss: 1.4338\n",
            "[20/33] Loss: 1.4937\n",
            "[21/33] Loss: 1.3598\n",
            "[22/33] Loss: 1.3828\n",
            "[23/33] Loss: 1.4117\n",
            "[24/33] Loss: 1.4284\n",
            "[25/33] Loss: 1.3943\n",
            "[26/33] Loss: 1.5019\n",
            "[27/33] Loss: 1.3548\n",
            "[28/33] Loss: 1.3446\n",
            "[29/33] Loss: 1.4314\n",
            "[30/33] Loss: 1.3328\n",
            "[31/33] Loss: 1.3445\n",
            "[32/33] Loss: 1.3849\n",
            "[33/33] Loss: 1.3433\n",
            "Epoch: 46 | Source Accuracy: 0.9937, Target Accuracy: 0.2800, Loss: 1.4555\n",
            "Epoch 0047 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3708\n",
            "[2/33] Loss: 1.4049\n",
            "[3/33] Loss: 1.7253\n",
            "[4/33] Loss: 1.4224\n",
            "[5/33] Loss: 1.7781\n",
            "[6/33] Loss: 1.3185\n",
            "[7/33] Loss: 1.3847\n",
            "[8/33] Loss: 1.4101\n",
            "[9/33] Loss: 1.4626\n",
            "[10/33] Loss: 1.3693\n",
            "[11/33] Loss: 1.6879\n",
            "[12/33] Loss: 1.4763\n",
            "[13/33] Loss: 1.4046\n",
            "[14/33] Loss: 1.4533\n",
            "[15/33] Loss: 1.4579\n",
            "[16/33] Loss: 1.3573\n",
            "[17/33] Loss: 1.6502\n",
            "[18/33] Loss: 1.5149\n",
            "[19/33] Loss: 1.4290\n",
            "[20/33] Loss: 1.4641\n",
            "[21/33] Loss: 1.3994\n",
            "[22/33] Loss: 1.3765\n",
            "[23/33] Loss: 1.4764\n",
            "[24/33] Loss: 1.3407\n",
            "[25/33] Loss: 1.4150\n",
            "[26/33] Loss: 1.3822\n",
            "[27/33] Loss: 1.3847\n",
            "[28/33] Loss: 1.3280\n",
            "[29/33] Loss: 1.3985\n",
            "[30/33] Loss: 1.3425\n",
            "[31/33] Loss: 1.3872\n",
            "[32/33] Loss: 1.3830\n",
            "[33/33] Loss: 1.3713\n",
            "Epoch: 47 | Source Accuracy: 0.9903, Target Accuracy: 0.2868, Loss: 1.4402\n",
            "Epoch 0048 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3279\n",
            "[2/33] Loss: 1.6516\n",
            "[3/33] Loss: 1.3590\n",
            "[4/33] Loss: 1.3696\n",
            "[5/33] Loss: 1.3631\n",
            "[6/33] Loss: 1.3344\n",
            "[7/33] Loss: 1.3716\n",
            "[8/33] Loss: 1.6654\n",
            "[9/33] Loss: 1.3147\n",
            "[10/33] Loss: 1.3648\n",
            "[11/33] Loss: 1.3552\n",
            "[12/33] Loss: 1.4516\n",
            "[13/33] Loss: 1.3513\n",
            "[14/33] Loss: 1.4645\n",
            "[15/33] Loss: 1.2921\n",
            "[16/33] Loss: 1.3292\n",
            "[17/33] Loss: 1.3393\n",
            "[18/33] Loss: 1.3278\n",
            "[19/33] Loss: 1.3203\n",
            "[20/33] Loss: 1.3302\n",
            "[21/33] Loss: 1.3566\n",
            "[22/33] Loss: 1.3406\n",
            "[23/33] Loss: 1.3204\n",
            "[24/33] Loss: 1.3628\n",
            "[25/33] Loss: 1.3515\n",
            "[26/33] Loss: 1.3099\n",
            "[27/33] Loss: 1.3418\n",
            "[28/33] Loss: 1.3594\n",
            "[29/33] Loss: 1.3447\n",
            "[30/33] Loss: 1.3971\n",
            "[31/33] Loss: 1.3279\n",
            "[32/33] Loss: 1.2602\n",
            "[33/33] Loss: 1.3846\n",
            "Epoch: 48 | Source Accuracy: 0.9937, Target Accuracy: 0.2897, Loss: 1.3679\n",
            "Epoch 0049 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.2959\n",
            "[2/33] Loss: 1.4302\n",
            "[3/33] Loss: 1.2952\n",
            "[4/33] Loss: 1.3332\n",
            "[5/33] Loss: 1.3717\n",
            "[6/33] Loss: 1.3214\n",
            "[7/33] Loss: 1.2778\n",
            "[8/33] Loss: 1.2995\n",
            "[9/33] Loss: 1.3186\n",
            "[10/33] Loss: 1.2752\n",
            "[11/33] Loss: 1.3159\n",
            "[12/33] Loss: 1.3476\n",
            "[13/33] Loss: 1.2766\n",
            "[14/33] Loss: 1.3124\n",
            "[15/33] Loss: 1.2684\n",
            "[16/33] Loss: 1.2824\n",
            "[17/33] Loss: 1.3362\n",
            "[18/33] Loss: 1.3841\n",
            "[19/33] Loss: 1.4014\n",
            "[20/33] Loss: 1.2958\n",
            "[21/33] Loss: 1.2823\n",
            "[22/33] Loss: 1.3193\n",
            "[23/33] Loss: 1.3329\n",
            "[24/33] Loss: 1.3678\n",
            "[25/33] Loss: 1.2693\n",
            "[26/33] Loss: 1.4087\n",
            "[27/33] Loss: 1.3321\n",
            "[28/33] Loss: 1.3381\n",
            "[29/33] Loss: 1.3356\n",
            "[30/33] Loss: 1.3670\n",
            "[31/33] Loss: 1.3013\n",
            "[32/33] Loss: 1.4106\n",
            "[33/33] Loss: 1.2372\n",
            "Epoch: 49 | Source Accuracy: 0.9976, Target Accuracy: 0.2936, Loss: 1.3255\n",
            "Epoch 0050 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3410\n",
            "[2/33] Loss: 1.2894\n",
            "[3/33] Loss: 1.2981\n",
            "[4/33] Loss: 1.2872\n",
            "[5/33] Loss: 1.3854\n",
            "[6/33] Loss: 1.3064\n",
            "[7/33] Loss: 1.4796\n",
            "[8/33] Loss: 1.3001\n",
            "[9/33] Loss: 1.3750\n",
            "[10/33] Loss: 1.3330\n",
            "[11/33] Loss: 1.4340\n",
            "[12/33] Loss: 1.3152\n",
            "[13/33] Loss: 1.3669\n",
            "[14/33] Loss: 1.3394\n",
            "[15/33] Loss: 1.3537\n",
            "[16/33] Loss: 1.2863\n",
            "[17/33] Loss: 1.3188\n",
            "[18/33] Loss: 1.3493\n",
            "[19/33] Loss: 1.3653\n",
            "[20/33] Loss: 1.3058\n",
            "[21/33] Loss: 1.3987\n",
            "[22/33] Loss: 1.2995\n",
            "[23/33] Loss: 1.4125\n",
            "[24/33] Loss: 1.3355\n",
            "[25/33] Loss: 1.3418\n",
            "[26/33] Loss: 1.3176\n",
            "[27/33] Loss: 1.2808\n",
            "[28/33] Loss: 1.3003\n",
            "[29/33] Loss: 1.5179\n",
            "[30/33] Loss: 1.2813\n",
            "[31/33] Loss: 1.4027\n",
            "[32/33] Loss: 1.2598\n",
            "[33/33] Loss: 1.2614\n",
            "Epoch: 50 | Source Accuracy: 0.9976, Target Accuracy: 0.2902, Loss: 1.3406\n",
            "Training logs saved to LOG_DANN_dslr_to_amazon\n",
            "Source and Target are the same. Skiping...\n",
            "Epoch 0001 / 0050\n",
            "============\n",
            "[1/10] Loss: 3.4255\n",
            "[2/10] Loss: 3.4453\n",
            "[3/10] Loss: 3.4294\n",
            "[4/10] Loss: 3.4728\n",
            "[5/10] Loss: 3.4600\n",
            "[6/10] Loss: 3.4617\n",
            "[7/10] Loss: 3.4372\n",
            "[8/10] Loss: 3.4642\n",
            "[9/10] Loss: 3.4530\n",
            "[10/10] Loss: 3.5460\n",
            "Epoch: 1 | Source Accuracy: 0.0786, Target Accuracy: 0.0444, Loss: 3.4595\n",
            "Epoch 0002 / 0050\n",
            "============\n",
            "[1/10] Loss: 3.4823\n",
            "[2/10] Loss: 3.4459\n",
            "[3/10] Loss: 3.4842\n",
            "[4/10] Loss: 3.4737\n",
            "[5/10] Loss: 3.5318\n",
            "[6/10] Loss: 3.4905\n",
            "[7/10] Loss: 3.5027\n",
            "[8/10] Loss: 3.4455\n",
            "[9/10] Loss: 3.4955\n",
            "[10/10] Loss: 3.4624\n",
            "Epoch: 2 | Source Accuracy: 0.1897, Target Accuracy: 0.1179, Loss: 3.4814\n",
            "Epoch 0003 / 0050\n",
            "============\n",
            "[1/10] Loss: 3.4908\n",
            "[2/10] Loss: 3.4834\n",
            "[3/10] Loss: 3.4353\n",
            "[4/10] Loss: 3.4319\n",
            "[5/10] Loss: 3.4511\n",
            "[6/10] Loss: 3.5084\n",
            "[7/10] Loss: 3.4474\n",
            "[8/10] Loss: 3.4398\n",
            "[9/10] Loss: 3.3405\n",
            "[10/10] Loss: 3.3026\n",
            "Epoch: 3 | Source Accuracy: 0.2581, Target Accuracy: 0.1624, Loss: 3.4331\n",
            "Epoch 0004 / 0050\n",
            "============\n",
            "[1/10] Loss: 3.4057\n",
            "[2/10] Loss: 3.3374\n",
            "[3/10] Loss: 3.3399\n",
            "[4/10] Loss: 3.3143\n",
            "[5/10] Loss: 3.3535\n",
            "[6/10] Loss: 3.2427\n",
            "[7/10] Loss: 3.2524\n",
            "[8/10] Loss: 3.1665\n",
            "[9/10] Loss: 3.1419\n",
            "[10/10] Loss: 3.3858\n",
            "Epoch: 4 | Source Accuracy: 0.3316, Target Accuracy: 0.2154, Loss: 3.2940\n",
            "Epoch 0005 / 0050\n",
            "============\n",
            "[1/10] Loss: 3.0220\n",
            "[2/10] Loss: 2.9884\n",
            "[3/10] Loss: 3.1200\n",
            "[4/10] Loss: 2.9981\n",
            "[5/10] Loss: 2.9892\n",
            "[6/10] Loss: 2.7273\n",
            "[7/10] Loss: 2.6994\n",
            "[8/10] Loss: 2.6679\n",
            "[9/10] Loss: 2.7806\n",
            "[10/10] Loss: 3.0055\n",
            "Epoch: 5 | Source Accuracy: 0.5094, Target Accuracy: 0.3043, Loss: 2.8998\n",
            "Epoch 0006 / 0050\n",
            "============\n",
            "[1/10] Loss: 2.6970\n",
            "[2/10] Loss: 2.4376\n",
            "[3/10] Loss: 2.4563\n",
            "[4/10] Loss: 2.2416\n",
            "[5/10] Loss: 2.2090\n",
            "[6/10] Loss: 2.1487\n",
            "[7/10] Loss: 2.2202\n",
            "[8/10] Loss: 1.9633\n",
            "[9/10] Loss: 1.9495\n",
            "[10/10] Loss: 1.7684\n",
            "Epoch: 6 | Source Accuracy: 0.7299, Target Accuracy: 0.4479, Loss: 2.2092\n",
            "Epoch 0007 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.6895\n",
            "[2/10] Loss: 1.9576\n",
            "[3/10] Loss: 1.4172\n",
            "[4/10] Loss: 1.7065\n",
            "[5/10] Loss: 1.4762\n",
            "[6/10] Loss: 1.4985\n",
            "[7/10] Loss: 1.2676\n",
            "[8/10] Loss: 1.4595\n",
            "[9/10] Loss: 1.0571\n",
            "[10/10] Loss: 1.1178\n",
            "Epoch: 7 | Source Accuracy: 0.8889, Target Accuracy: 0.6239, Loss: 1.4647\n",
            "Epoch 0008 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.1692\n",
            "[2/10] Loss: 1.2305\n",
            "[3/10] Loss: 1.0387\n",
            "[4/10] Loss: 1.1909\n",
            "[5/10] Loss: 1.2232\n",
            "[6/10] Loss: 1.1825\n",
            "[7/10] Loss: 1.0160\n",
            "[8/10] Loss: 1.1654\n",
            "[9/10] Loss: 0.9601\n",
            "[10/10] Loss: 1.2528\n",
            "Epoch: 8 | Source Accuracy: 0.9521, Target Accuracy: 0.7333, Loss: 1.1429\n",
            "Epoch 0009 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.2035\n",
            "[2/10] Loss: 1.1582\n",
            "[3/10] Loss: 1.1992\n",
            "[4/10] Loss: 1.3003\n",
            "[5/10] Loss: 1.2290\n",
            "[6/10] Loss: 1.3701\n",
            "[7/10] Loss: 1.2748\n",
            "[8/10] Loss: 1.3550\n",
            "[9/10] Loss: 1.2580\n",
            "[10/10] Loss: 1.2730\n",
            "Epoch: 9 | Source Accuracy: 0.9812, Target Accuracy: 0.7265, Loss: 1.2621\n",
            "Epoch 0010 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3542\n",
            "[2/10] Loss: 1.2599\n",
            "[3/10] Loss: 1.2550\n",
            "[4/10] Loss: 1.1761\n",
            "[5/10] Loss: 1.3066\n",
            "[6/10] Loss: 1.1970\n",
            "[7/10] Loss: 1.3285\n",
            "[8/10] Loss: 1.2063\n",
            "[9/10] Loss: 1.2238\n",
            "[10/10] Loss: 1.1877\n",
            "Epoch: 10 | Source Accuracy: 0.9880, Target Accuracy: 0.7231, Loss: 1.2495\n",
            "Epoch 0011 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.1982\n",
            "[2/10] Loss: 1.2568\n",
            "[3/10] Loss: 1.3425\n",
            "[4/10] Loss: 1.2695\n",
            "[5/10] Loss: 1.4148\n",
            "[6/10] Loss: 1.2762\n",
            "[7/10] Loss: 1.2935\n",
            "[8/10] Loss: 1.3130\n",
            "[9/10] Loss: 1.2731\n",
            "[10/10] Loss: 1.0920\n",
            "Epoch: 11 | Source Accuracy: 0.9778, Target Accuracy: 0.7111, Loss: 1.2730\n",
            "Epoch 0012 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.1878\n",
            "[2/10] Loss: 1.3795\n",
            "[3/10] Loss: 1.4195\n",
            "[4/10] Loss: 1.5291\n",
            "[5/10] Loss: 1.4472\n",
            "[6/10] Loss: 1.4169\n",
            "[7/10] Loss: 1.2714\n",
            "[8/10] Loss: 1.2201\n",
            "[9/10] Loss: 1.2634\n",
            "[10/10] Loss: 1.2957\n",
            "Epoch: 12 | Source Accuracy: 0.9795, Target Accuracy: 0.6735, Loss: 1.3431\n",
            "Epoch 0013 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3178\n",
            "[2/10] Loss: 1.3570\n",
            "[3/10] Loss: 1.4272\n",
            "[4/10] Loss: 1.4654\n",
            "[5/10] Loss: 1.5503\n",
            "[6/10] Loss: 1.4804\n",
            "[7/10] Loss: 1.1841\n",
            "[8/10] Loss: 1.1395\n",
            "[9/10] Loss: 1.2218\n",
            "[10/10] Loss: 1.7866\n",
            "Epoch: 13 | Source Accuracy: 0.9641, Target Accuracy: 0.5556, Loss: 1.3930\n",
            "Epoch 0014 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.0767\n",
            "[2/10] Loss: 1.1119\n",
            "[3/10] Loss: 1.1480\n",
            "[4/10] Loss: 1.3297\n",
            "[5/10] Loss: 1.3900\n",
            "[6/10] Loss: 1.4487\n",
            "[7/10] Loss: 1.2384\n",
            "[8/10] Loss: 1.2966\n",
            "[9/10] Loss: 1.4613\n",
            "[10/10] Loss: 1.5664\n",
            "Epoch: 14 | Source Accuracy: 0.9607, Target Accuracy: 0.5761, Loss: 1.3068\n",
            "Epoch 0015 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.6167\n",
            "[2/10] Loss: 1.3906\n",
            "[3/10] Loss: 1.4028\n",
            "[4/10] Loss: 1.6059\n",
            "[5/10] Loss: 1.3140\n",
            "[6/10] Loss: 1.4306\n",
            "[7/10] Loss: 1.1974\n",
            "[8/10] Loss: 1.1774\n",
            "[9/10] Loss: 1.3275\n",
            "[10/10] Loss: 1.3084\n",
            "Epoch: 15 | Source Accuracy: 0.9573, Target Accuracy: 0.6051, Loss: 1.3771\n",
            "Epoch 0016 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.2322\n",
            "[2/10] Loss: 1.3526\n",
            "[3/10] Loss: 1.2628\n",
            "[4/10] Loss: 1.4025\n",
            "[5/10] Loss: 1.4239\n",
            "[6/10] Loss: 1.3816\n",
            "[7/10] Loss: 1.4844\n",
            "[8/10] Loss: 1.3573\n",
            "[9/10] Loss: 1.2679\n",
            "[10/10] Loss: 1.5465\n",
            "Epoch: 16 | Source Accuracy: 0.9829, Target Accuracy: 0.7949, Loss: 1.3712\n",
            "Epoch 0017 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3796\n",
            "[2/10] Loss: 1.3621\n",
            "[3/10] Loss: 1.3712\n",
            "[4/10] Loss: 1.3947\n",
            "[5/10] Loss: 1.4514\n",
            "[6/10] Loss: 1.4070\n",
            "[7/10] Loss: 1.3733\n",
            "[8/10] Loss: 1.3489\n",
            "[9/10] Loss: 1.3205\n",
            "[10/10] Loss: 1.3875\n",
            "Epoch: 17 | Source Accuracy: 0.9829, Target Accuracy: 0.8120, Loss: 1.3796\n",
            "Epoch 0018 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.2344\n",
            "[2/10] Loss: 1.1974\n",
            "[3/10] Loss: 1.2698\n",
            "[4/10] Loss: 1.1977\n",
            "[5/10] Loss: 1.4043\n",
            "[6/10] Loss: 1.1335\n",
            "[7/10] Loss: 1.3886\n",
            "[8/10] Loss: 1.2615\n",
            "[9/10] Loss: 1.4807\n",
            "[10/10] Loss: 1.1659\n",
            "Epoch: 18 | Source Accuracy: 0.9846, Target Accuracy: 0.8000, Loss: 1.2734\n",
            "Epoch 0019 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.5507\n",
            "[2/10] Loss: 1.6684\n",
            "[3/10] Loss: 1.5406\n",
            "[4/10] Loss: 1.6128\n",
            "[5/10] Loss: 1.7752\n",
            "[6/10] Loss: 1.8537\n",
            "[7/10] Loss: 1.9548\n",
            "[8/10] Loss: 2.0151\n",
            "[9/10] Loss: 1.9321\n",
            "[10/10] Loss: 1.8006\n",
            "Epoch: 19 | Source Accuracy: 0.9692, Target Accuracy: 0.7607, Loss: 1.7704\n",
            "Epoch 0020 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.8207\n",
            "[2/10] Loss: 1.4493\n",
            "[3/10] Loss: 1.3831\n",
            "[4/10] Loss: 1.1814\n",
            "[5/10] Loss: 0.9836\n",
            "[6/10] Loss: 1.1279\n",
            "[7/10] Loss: 1.0652\n",
            "[8/10] Loss: 1.0467\n",
            "[9/10] Loss: 1.3768\n",
            "[10/10] Loss: 1.9647\n",
            "Epoch: 20 | Source Accuracy: 0.9761, Target Accuracy: 0.6838, Loss: 1.3399\n",
            "Epoch 0021 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.5500\n",
            "[2/10] Loss: 1.6393\n",
            "[3/10] Loss: 1.5469\n",
            "[4/10] Loss: 1.8247\n",
            "[5/10] Loss: 1.6196\n",
            "[6/10] Loss: 1.6206\n",
            "[7/10] Loss: 1.6217\n",
            "[8/10] Loss: 1.5635\n",
            "[9/10] Loss: 1.6951\n",
            "[10/10] Loss: 1.8418\n",
            "Epoch: 21 | Source Accuracy: 0.9726, Target Accuracy: 0.6752, Loss: 1.6523\n",
            "Epoch 0022 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.5848\n",
            "[2/10] Loss: 1.7001\n",
            "[3/10] Loss: 1.5064\n",
            "[4/10] Loss: 1.7335\n",
            "[5/10] Loss: 1.4236\n",
            "[6/10] Loss: 1.4857\n",
            "[7/10] Loss: 1.4790\n",
            "[8/10] Loss: 1.2106\n",
            "[9/10] Loss: 1.3231\n",
            "[10/10] Loss: 1.2293\n",
            "Epoch: 22 | Source Accuracy: 0.9709, Target Accuracy: 0.6769, Loss: 1.4676\n",
            "Epoch 0023 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.5119\n",
            "[2/10] Loss: 1.2753\n",
            "[3/10] Loss: 1.4073\n",
            "[4/10] Loss: 1.2772\n",
            "[5/10] Loss: 1.3091\n",
            "[6/10] Loss: 1.3241\n",
            "[7/10] Loss: 1.4083\n",
            "[8/10] Loss: 1.2384\n",
            "[9/10] Loss: 1.3491\n",
            "[10/10] Loss: 1.2093\n",
            "Epoch: 23 | Source Accuracy: 0.9829, Target Accuracy: 0.6718, Loss: 1.3310\n",
            "Epoch 0024 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3873\n",
            "[2/10] Loss: 1.4387\n",
            "[3/10] Loss: 1.4902\n",
            "[4/10] Loss: 1.5374\n",
            "[5/10] Loss: 1.5593\n",
            "[6/10] Loss: 1.6495\n",
            "[7/10] Loss: 1.4655\n",
            "[8/10] Loss: 1.4998\n",
            "[9/10] Loss: 1.4799\n",
            "[10/10] Loss: 1.5776\n",
            "Epoch: 24 | Source Accuracy: 0.9744, Target Accuracy: 0.6598, Loss: 1.5085\n",
            "Epoch 0025 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.4912\n",
            "[2/10] Loss: 1.4590\n",
            "[3/10] Loss: 1.4374\n",
            "[4/10] Loss: 1.5218\n",
            "[5/10] Loss: 1.2928\n",
            "[6/10] Loss: 1.4030\n",
            "[7/10] Loss: 1.3104\n",
            "[8/10] Loss: 1.2934\n",
            "[9/10] Loss: 1.2743\n",
            "[10/10] Loss: 1.3041\n",
            "Epoch: 25 | Source Accuracy: 0.9829, Target Accuracy: 0.6735, Loss: 1.3787\n",
            "Epoch 0026 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.2224\n",
            "[2/10] Loss: 1.3085\n",
            "[3/10] Loss: 1.1187\n",
            "[4/10] Loss: 1.3102\n",
            "[5/10] Loss: 1.8762\n",
            "[6/10] Loss: 1.4751\n",
            "[7/10] Loss: 1.4968\n",
            "[8/10] Loss: 1.3437\n",
            "[9/10] Loss: 1.6026\n",
            "[10/10] Loss: 1.5278\n",
            "Epoch: 26 | Source Accuracy: 0.9897, Target Accuracy: 0.7214, Loss: 1.4282\n",
            "Epoch 0027 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3080\n",
            "[2/10] Loss: 1.5735\n",
            "[3/10] Loss: 1.8968\n",
            "[4/10] Loss: 1.4842\n",
            "[5/10] Loss: 1.6893\n",
            "[6/10] Loss: 1.9500\n",
            "[7/10] Loss: 1.4156\n",
            "[8/10] Loss: 1.6297\n",
            "[9/10] Loss: 1.5298\n",
            "[10/10] Loss: 1.7885\n",
            "Epoch: 27 | Source Accuracy: 0.9726, Target Accuracy: 0.7436, Loss: 1.6265\n",
            "Epoch 0028 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.4334\n",
            "[2/10] Loss: 1.4498\n",
            "[3/10] Loss: 1.4014\n",
            "[4/10] Loss: 1.6263\n",
            "[5/10] Loss: 1.4831\n",
            "[6/10] Loss: 1.5286\n",
            "[7/10] Loss: 1.4292\n",
            "[8/10] Loss: 1.3104\n",
            "[9/10] Loss: 1.3665\n",
            "[10/10] Loss: 1.4459\n",
            "Epoch: 28 | Source Accuracy: 0.9880, Target Accuracy: 0.8034, Loss: 1.4475\n",
            "Epoch 0029 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.2596\n",
            "[2/10] Loss: 1.5850\n",
            "[3/10] Loss: 1.3023\n",
            "[4/10] Loss: 1.4258\n",
            "[5/10] Loss: 1.3581\n",
            "[6/10] Loss: 1.2821\n",
            "[7/10] Loss: 1.2421\n",
            "[8/10] Loss: 1.4369\n",
            "[9/10] Loss: 1.3540\n",
            "[10/10] Loss: 1.5230\n",
            "Epoch: 29 | Source Accuracy: 0.9932, Target Accuracy: 0.8171, Loss: 1.3769\n",
            "Epoch 0030 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3421\n",
            "[2/10] Loss: 1.3166\n",
            "[3/10] Loss: 1.4598\n",
            "[4/10] Loss: 1.2690\n",
            "[5/10] Loss: 1.5442\n",
            "[6/10] Loss: 1.4209\n",
            "[7/10] Loss: 1.3541\n",
            "[8/10] Loss: 1.3286\n",
            "[9/10] Loss: 1.6931\n",
            "[10/10] Loss: 1.5853\n",
            "Epoch: 30 | Source Accuracy: 0.9863, Target Accuracy: 0.8103, Loss: 1.4314\n",
            "Epoch 0031 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3859\n",
            "[2/10] Loss: 1.3608\n",
            "[3/10] Loss: 1.3874\n",
            "[4/10] Loss: 1.4168\n",
            "[5/10] Loss: 1.3708\n",
            "[6/10] Loss: 1.3516\n",
            "[7/10] Loss: 1.3702\n",
            "[8/10] Loss: 1.5028\n",
            "[9/10] Loss: 1.3192\n",
            "[10/10] Loss: 1.3884\n",
            "Epoch: 31 | Source Accuracy: 0.9932, Target Accuracy: 0.7692, Loss: 1.3854\n",
            "Epoch 0032 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3374\n",
            "[2/10] Loss: 1.3339\n",
            "[3/10] Loss: 1.4402\n",
            "[4/10] Loss: 1.3338\n",
            "[5/10] Loss: 1.3205\n",
            "[6/10] Loss: 1.4492\n",
            "[7/10] Loss: 1.3205\n",
            "[8/10] Loss: 1.3190\n",
            "[9/10] Loss: 1.2782\n",
            "[10/10] Loss: 1.3213\n",
            "Epoch: 32 | Source Accuracy: 0.9983, Target Accuracy: 0.7504, Loss: 1.3454\n",
            "Epoch 0033 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3395\n",
            "[2/10] Loss: 1.3645\n",
            "[3/10] Loss: 1.4707\n",
            "[4/10] Loss: 1.4080\n",
            "[5/10] Loss: 1.3858\n",
            "[6/10] Loss: 1.2346\n",
            "[7/10] Loss: 1.2966\n",
            "[8/10] Loss: 1.3143\n",
            "[9/10] Loss: 1.2797\n",
            "[10/10] Loss: 1.4655\n",
            "Epoch: 33 | Source Accuracy: 0.9932, Target Accuracy: 0.7675, Loss: 1.3559\n",
            "Epoch 0034 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3178\n",
            "[2/10] Loss: 1.3537\n",
            "[3/10] Loss: 1.3712\n",
            "[4/10] Loss: 1.2721\n",
            "[5/10] Loss: 1.3725\n",
            "[6/10] Loss: 1.3007\n",
            "[7/10] Loss: 1.2944\n",
            "[8/10] Loss: 1.3315\n",
            "[9/10] Loss: 1.3547\n",
            "[10/10] Loss: 1.3448\n",
            "Epoch: 34 | Source Accuracy: 0.9983, Target Accuracy: 0.7829, Loss: 1.3313\n",
            "Epoch 0035 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3459\n",
            "[2/10] Loss: 1.3231\n",
            "[3/10] Loss: 1.3358\n",
            "[4/10] Loss: 1.3422\n",
            "[5/10] Loss: 1.4842\n",
            "[6/10] Loss: 1.3106\n",
            "[7/10] Loss: 1.3616\n",
            "[8/10] Loss: 1.2626\n",
            "[9/10] Loss: 1.2724\n",
            "[10/10] Loss: 1.3510\n",
            "Epoch: 35 | Source Accuracy: 0.9932, Target Accuracy: 0.7829, Loss: 1.3389\n",
            "Epoch 0036 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3148\n",
            "[2/10] Loss: 1.2503\n",
            "[3/10] Loss: 1.2817\n",
            "[4/10] Loss: 1.2985\n",
            "[5/10] Loss: 1.3631\n",
            "[6/10] Loss: 1.2742\n",
            "[7/10] Loss: 1.3413\n",
            "[8/10] Loss: 1.3144\n",
            "[9/10] Loss: 1.2691\n",
            "[10/10] Loss: 1.4554\n",
            "Epoch: 36 | Source Accuracy: 0.9966, Target Accuracy: 0.7795, Loss: 1.3163\n",
            "Epoch 0037 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3095\n",
            "[2/10] Loss: 1.1744\n",
            "[3/10] Loss: 1.2872\n",
            "[4/10] Loss: 1.2666\n",
            "[5/10] Loss: 1.2765\n",
            "[6/10] Loss: 1.2069\n",
            "[7/10] Loss: 1.3635\n",
            "[8/10] Loss: 1.2485\n",
            "[9/10] Loss: 1.1539\n",
            "[10/10] Loss: 1.8269\n",
            "Epoch: 37 | Source Accuracy: 0.9966, Target Accuracy: 0.7846, Loss: 1.3114\n",
            "Epoch 0038 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.0904\n",
            "[2/10] Loss: 1.1818\n",
            "[3/10] Loss: 1.3412\n",
            "[4/10] Loss: 1.3323\n",
            "[5/10] Loss: 1.3668\n",
            "[6/10] Loss: 1.2555\n",
            "[7/10] Loss: 1.0918\n",
            "[8/10] Loss: 1.3798\n",
            "[9/10] Loss: 1.3407\n",
            "[10/10] Loss: 2.0884\n",
            "Epoch: 38 | Source Accuracy: 0.9966, Target Accuracy: 0.8034, Loss: 1.3469\n",
            "Epoch 0039 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3706\n",
            "[2/10] Loss: 1.2944\n",
            "[3/10] Loss: 1.3237\n",
            "[4/10] Loss: 1.3219\n",
            "[5/10] Loss: 1.4491\n",
            "[6/10] Loss: 1.1977\n",
            "[7/10] Loss: 1.3728\n",
            "[8/10] Loss: 1.4188\n",
            "[9/10] Loss: 1.3712\n",
            "[10/10] Loss: 1.2707\n",
            "Epoch: 39 | Source Accuracy: 1.0000, Target Accuracy: 0.7812, Loss: 1.3391\n",
            "Epoch 0040 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.2747\n",
            "[2/10] Loss: 1.4959\n",
            "[3/10] Loss: 1.4651\n",
            "[4/10] Loss: 1.3120\n",
            "[5/10] Loss: 1.4879\n",
            "[6/10] Loss: 1.5516\n",
            "[7/10] Loss: 1.3317\n",
            "[8/10] Loss: 1.6011\n",
            "[9/10] Loss: 1.4310\n",
            "[10/10] Loss: 1.3371\n",
            "Epoch: 40 | Source Accuracy: 1.0000, Target Accuracy: 0.7932, Loss: 1.4288\n",
            "Epoch 0041 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.4385\n",
            "[2/10] Loss: 1.4344\n",
            "[3/10] Loss: 1.5662\n",
            "[4/10] Loss: 1.3928\n",
            "[5/10] Loss: 1.4753\n",
            "[6/10] Loss: 1.2969\n",
            "[7/10] Loss: 1.5098\n",
            "[8/10] Loss: 1.5233\n",
            "[9/10] Loss: 1.4484\n",
            "[10/10] Loss: 1.8152\n",
            "Epoch: 41 | Source Accuracy: 0.9966, Target Accuracy: 0.7932, Loss: 1.4901\n",
            "Epoch 0042 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.7448\n",
            "[2/10] Loss: 1.5499\n",
            "[3/10] Loss: 1.4649\n",
            "[4/10] Loss: 1.3015\n",
            "[5/10] Loss: 1.4737\n",
            "[6/10] Loss: 1.3246\n",
            "[7/10] Loss: 1.5009\n",
            "[8/10] Loss: 1.4777\n",
            "[9/10] Loss: 1.3547\n",
            "[10/10] Loss: 1.2558\n",
            "Epoch: 42 | Source Accuracy: 0.9983, Target Accuracy: 0.7846, Loss: 1.4448\n",
            "Epoch 0043 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.4491\n",
            "[2/10] Loss: 1.5261\n",
            "[3/10] Loss: 1.4207\n",
            "[4/10] Loss: 1.2755\n",
            "[5/10] Loss: 1.4243\n",
            "[6/10] Loss: 1.3544\n",
            "[7/10] Loss: 1.4190\n",
            "[8/10] Loss: 1.4521\n",
            "[9/10] Loss: 1.3888\n",
            "[10/10] Loss: 1.3269\n",
            "Epoch: 43 | Source Accuracy: 0.9966, Target Accuracy: 0.7709, Loss: 1.4037\n",
            "Epoch 0044 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3636\n",
            "[2/10] Loss: 1.3835\n",
            "[3/10] Loss: 1.4678\n",
            "[4/10] Loss: 1.3618\n",
            "[5/10] Loss: 1.4553\n",
            "[6/10] Loss: 1.3218\n",
            "[7/10] Loss: 1.3570\n",
            "[8/10] Loss: 1.3361\n",
            "[9/10] Loss: 1.3644\n",
            "[10/10] Loss: 1.2837\n",
            "Epoch: 44 | Source Accuracy: 0.9966, Target Accuracy: 0.7915, Loss: 1.3695\n",
            "Epoch 0045 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3466\n",
            "[2/10] Loss: 1.3258\n",
            "[3/10] Loss: 1.3289\n",
            "[4/10] Loss: 1.4098\n",
            "[5/10] Loss: 1.3730\n",
            "[6/10] Loss: 1.3593\n",
            "[7/10] Loss: 1.3335\n",
            "[8/10] Loss: 1.3020\n",
            "[9/10] Loss: 1.2942\n",
            "[10/10] Loss: 1.4482\n",
            "Epoch: 45 | Source Accuracy: 0.9949, Target Accuracy: 0.7915, Loss: 1.3521\n",
            "Epoch 0046 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3294\n",
            "[2/10] Loss: 1.2866\n",
            "[3/10] Loss: 1.3179\n",
            "[4/10] Loss: 1.3236\n",
            "[5/10] Loss: 1.3824\n",
            "[6/10] Loss: 1.2573\n",
            "[7/10] Loss: 1.3377\n",
            "[8/10] Loss: 1.2718\n",
            "[9/10] Loss: 1.4242\n",
            "[10/10] Loss: 1.3423\n",
            "Epoch: 46 | Source Accuracy: 0.9966, Target Accuracy: 0.7778, Loss: 1.3273\n",
            "Epoch 0047 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3487\n",
            "[2/10] Loss: 1.2499\n",
            "[3/10] Loss: 1.3150\n",
            "[4/10] Loss: 1.2413\n",
            "[5/10] Loss: 1.3653\n",
            "[6/10] Loss: 1.2792\n",
            "[7/10] Loss: 1.2782\n",
            "[8/10] Loss: 1.2191\n",
            "[9/10] Loss: 1.2241\n",
            "[10/10] Loss: 1.2300\n",
            "Epoch: 47 | Source Accuracy: 1.0000, Target Accuracy: 0.8017, Loss: 1.2751\n",
            "Epoch 0048 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.2013\n",
            "[2/10] Loss: 1.3145\n",
            "[3/10] Loss: 1.2692\n",
            "[4/10] Loss: 1.2331\n",
            "[5/10] Loss: 1.2878\n",
            "[6/10] Loss: 1.2996\n",
            "[7/10] Loss: 1.3272\n",
            "[8/10] Loss: 1.3172\n",
            "[9/10] Loss: 1.3180\n",
            "[10/10] Loss: 1.3077\n",
            "Epoch: 48 | Source Accuracy: 1.0000, Target Accuracy: 0.7795, Loss: 1.2876\n",
            "Epoch 0049 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.2515\n",
            "[2/10] Loss: 1.3844\n",
            "[3/10] Loss: 1.2459\n",
            "[4/10] Loss: 1.2154\n",
            "[5/10] Loss: 1.4354\n",
            "[6/10] Loss: 1.2113\n",
            "[7/10] Loss: 1.3050\n",
            "[8/10] Loss: 1.3460\n",
            "[9/10] Loss: 1.2225\n",
            "[10/10] Loss: 1.4165\n",
            "Epoch: 49 | Source Accuracy: 0.9966, Target Accuracy: 0.7949, Loss: 1.3034\n",
            "Epoch 0050 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3568\n",
            "[2/10] Loss: 1.2735\n",
            "[3/10] Loss: 1.2365\n",
            "[4/10] Loss: 1.3397\n",
            "[5/10] Loss: 1.2544\n",
            "[6/10] Loss: 1.3081\n",
            "[7/10] Loss: 1.2695\n",
            "[8/10] Loss: 1.2083\n",
            "[9/10] Loss: 1.3449\n",
            "[10/10] Loss: 1.2597\n",
            "Epoch: 50 | Source Accuracy: 0.9983, Target Accuracy: 0.7778, Loss: 1.2851\n",
            "Training logs saved to LOG_DANN_dslr_to_webcam\n",
            "Epoch 0001 / 0050\n",
            "============\n",
            "[1/33] Loss: 3.4276\n",
            "[2/33] Loss: 3.4469\n",
            "[3/33] Loss: 3.4379\n",
            "[4/33] Loss: 3.4167\n",
            "[5/33] Loss: 3.4099\n",
            "[6/33] Loss: 3.4558\n",
            "[7/33] Loss: 3.4135\n",
            "[8/33] Loss: 3.4474\n",
            "[9/33] Loss: 3.4316\n",
            "[10/33] Loss: 3.4363\n",
            "[11/33] Loss: 3.3586\n",
            "[12/33] Loss: 3.4031\n",
            "[13/33] Loss: 3.3908\n",
            "[14/33] Loss: 3.3778\n",
            "[15/33] Loss: 3.3593\n",
            "[16/33] Loss: 3.4262\n",
            "[17/33] Loss: 3.3648\n",
            "[18/33] Loss: 3.4102\n",
            "[19/33] Loss: 3.3836\n",
            "[20/33] Loss: 3.3675\n",
            "[21/33] Loss: 3.3297\n",
            "[22/33] Loss: 3.3649\n",
            "[23/33] Loss: 3.3446\n",
            "[24/33] Loss: 3.3393\n",
            "[25/33] Loss: 3.3013\n",
            "[26/33] Loss: 3.3988\n",
            "[27/33] Loss: 3.3224\n",
            "[28/33] Loss: 3.3415\n",
            "[29/33] Loss: 3.3162\n",
            "[30/33] Loss: 3.3133\n",
            "[31/33] Loss: 3.2723\n",
            "[32/33] Loss: 3.3278\n",
            "[33/33] Loss: 3.2925\n",
            "Epoch: 1 | Source Accuracy: 0.2237, Target Accuracy: 0.0919, Loss: 3.3767\n",
            "Epoch 0002 / 0050\n",
            "============\n",
            "[1/33] Loss: 3.2964\n",
            "[2/33] Loss: 3.2941\n",
            "[3/33] Loss: 3.2475\n",
            "[4/33] Loss: 3.2564\n",
            "[5/33] Loss: 3.2756\n",
            "[6/33] Loss: 3.2706\n",
            "[7/33] Loss: 3.2476\n",
            "[8/33] Loss: 3.2471\n",
            "[9/33] Loss: 3.2294\n",
            "[10/33] Loss: 3.2622\n",
            "[11/33] Loss: 3.2187\n",
            "[12/33] Loss: 3.2252\n",
            "[13/33] Loss: 3.1459\n",
            "[14/33] Loss: 3.1399\n",
            "[15/33] Loss: 3.1922\n",
            "[16/33] Loss: 3.1680\n",
            "[17/33] Loss: 3.1447\n",
            "[18/33] Loss: 3.1449\n",
            "[19/33] Loss: 3.1150\n",
            "[20/33] Loss: 3.1208\n",
            "[21/33] Loss: 3.1078\n",
            "[22/33] Loss: 3.1057\n",
            "[23/33] Loss: 2.9976\n",
            "[24/33] Loss: 2.9913\n",
            "[25/33] Loss: 3.0794\n",
            "[26/33] Loss: 3.0379\n",
            "[27/33] Loss: 2.9869\n",
            "[28/33] Loss: 2.9792\n",
            "[29/33] Loss: 2.9616\n",
            "[30/33] Loss: 2.9195\n",
            "[31/33] Loss: 2.9324\n",
            "[32/33] Loss: 2.9380\n",
            "[33/33] Loss: 2.7870\n",
            "Epoch: 2 | Source Accuracy: 0.4505, Target Accuracy: 0.1894, Loss: 3.1232\n",
            "Epoch 0003 / 0050\n",
            "============\n",
            "[1/33] Loss: 2.8093\n",
            "[2/33] Loss: 2.7841\n",
            "[3/33] Loss: 2.7387\n",
            "[4/33] Loss: 2.7525\n",
            "[5/33] Loss: 2.7945\n",
            "[6/33] Loss: 2.8984\n",
            "[7/33] Loss: 2.6568\n",
            "[8/33] Loss: 2.7891\n",
            "[9/33] Loss: 2.5985\n",
            "[10/33] Loss: 2.5836\n",
            "[11/33] Loss: 2.4932\n",
            "[12/33] Loss: 2.4856\n",
            "[13/33] Loss: 2.4125\n",
            "[14/33] Loss: 2.4497\n",
            "[15/33] Loss: 2.5012\n",
            "[16/33] Loss: 2.6391\n",
            "[17/33] Loss: 2.3709\n",
            "[18/33] Loss: 2.5280\n",
            "[19/33] Loss: 2.3096\n",
            "[20/33] Loss: 2.2424\n",
            "[21/33] Loss: 2.1990\n",
            "[22/33] Loss: 2.1807\n",
            "[23/33] Loss: 2.0869\n",
            "[24/33] Loss: 2.1060\n",
            "[25/33] Loss: 2.2106\n",
            "[26/33] Loss: 2.3973\n",
            "[27/33] Loss: 2.0598\n",
            "[28/33] Loss: 2.2617\n",
            "[29/33] Loss: 2.0127\n",
            "[30/33] Loss: 1.7998\n",
            "[31/33] Loss: 1.8607\n",
            "[32/33] Loss: 1.8495\n",
            "[33/33] Loss: 1.7893\n",
            "Epoch: 3 | Source Accuracy: 0.6212, Target Accuracy: 0.2444, Loss: 2.3834\n",
            "Epoch 0004 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.9260\n",
            "[2/33] Loss: 1.8033\n",
            "[3/33] Loss: 1.8830\n",
            "[4/33] Loss: 1.7977\n",
            "[5/33] Loss: 1.7612\n",
            "[6/33] Loss: 1.8826\n",
            "[7/33] Loss: 1.7133\n",
            "[8/33] Loss: 1.6707\n",
            "[9/33] Loss: 1.6265\n",
            "[10/33] Loss: 1.6027\n",
            "[11/33] Loss: 1.6896\n",
            "[12/33] Loss: 1.4592\n",
            "[13/33] Loss: 1.5729\n",
            "[14/33] Loss: 1.5314\n",
            "[15/33] Loss: 1.4878\n",
            "[16/33] Loss: 1.6527\n",
            "[17/33] Loss: 1.4528\n",
            "[18/33] Loss: 1.4034\n",
            "[19/33] Loss: 1.3732\n",
            "[20/33] Loss: 1.3214\n",
            "[21/33] Loss: 1.4409\n",
            "[22/33] Loss: 1.2105\n",
            "[23/33] Loss: 1.3587\n",
            "[24/33] Loss: 1.2908\n",
            "[25/33] Loss: 1.2449\n",
            "[26/33] Loss: 1.3509\n",
            "[27/33] Loss: 1.1730\n",
            "[28/33] Loss: 1.1592\n",
            "[29/33] Loss: 1.1110\n",
            "[30/33] Loss: 1.0716\n",
            "[31/33] Loss: 1.2357\n",
            "[32/33] Loss: 0.9484\n",
            "[33/33] Loss: 1.0986\n",
            "Epoch: 4 | Source Accuracy: 0.7677, Target Accuracy: 0.2717, Loss: 1.4638\n",
            "Epoch 0005 / 0050\n",
            "============\n",
            "[1/33] Loss: 0.9480\n",
            "[2/33] Loss: 0.8732\n",
            "[3/33] Loss: 0.8835\n",
            "[4/33] Loss: 1.1873\n",
            "[5/33] Loss: 0.9503\n",
            "[6/33] Loss: 0.8928\n",
            "[7/33] Loss: 0.7522\n",
            "[8/33] Loss: 1.0011\n",
            "[9/33] Loss: 1.0370\n",
            "[10/33] Loss: 1.4363\n",
            "[11/33] Loss: 0.7413\n",
            "[12/33] Loss: 0.6605\n",
            "[13/33] Loss: 0.7178\n",
            "[14/33] Loss: 0.9729\n",
            "[15/33] Loss: 0.7512\n",
            "[16/33] Loss: 0.7426\n",
            "[17/33] Loss: 0.6317\n",
            "[18/33] Loss: 0.8526\n",
            "[19/33] Loss: 0.7883\n",
            "[20/33] Loss: 0.8819\n",
            "[21/33] Loss: 0.7051\n",
            "[22/33] Loss: 0.6525\n",
            "[23/33] Loss: 0.5769\n",
            "[24/33] Loss: 0.8169\n",
            "[25/33] Loss: 0.7267\n",
            "[26/33] Loss: 0.6148\n",
            "[27/33] Loss: 0.6120\n",
            "[28/33] Loss: 0.7377\n",
            "[29/33] Loss: 0.6600\n",
            "[30/33] Loss: 0.8520\n",
            "[31/33] Loss: 0.5799\n",
            "[32/33] Loss: 0.5891\n",
            "[33/33] Loss: 0.6118\n",
            "Epoch: 5 | Source Accuracy: 0.9263, Target Accuracy: 0.2742, Loss: 0.8011\n",
            "Epoch 0006 / 0050\n",
            "============\n",
            "[1/33] Loss: 0.7319\n",
            "[2/33] Loss: 0.7043\n",
            "[3/33] Loss: 0.7267\n",
            "[4/33] Loss: 0.6830\n",
            "[5/33] Loss: 0.8057\n",
            "[6/33] Loss: 0.6914\n",
            "[7/33] Loss: 0.6917\n",
            "[8/33] Loss: 0.7794\n",
            "[9/33] Loss: 0.7947\n",
            "[10/33] Loss: 0.8586\n",
            "[11/33] Loss: 0.8565\n",
            "[12/33] Loss: 0.7935\n",
            "[13/33] Loss: 0.8240\n",
            "[14/33] Loss: 0.7536\n",
            "[15/33] Loss: 0.9109\n",
            "[16/33] Loss: 0.8026\n",
            "[17/33] Loss: 0.8449\n",
            "[18/33] Loss: 0.8489\n",
            "[19/33] Loss: 0.9314\n",
            "[20/33] Loss: 0.8939\n",
            "[21/33] Loss: 1.0196\n",
            "[22/33] Loss: 0.8818\n",
            "[23/33] Loss: 0.9261\n",
            "[24/33] Loss: 0.8904\n",
            "[25/33] Loss: 0.9262\n",
            "[26/33] Loss: 0.8205\n",
            "[27/33] Loss: 0.8910\n",
            "[28/33] Loss: 0.8729\n",
            "[29/33] Loss: 0.9140\n",
            "[30/33] Loss: 0.9631\n",
            "[31/33] Loss: 0.9148\n",
            "[32/33] Loss: 0.9257\n",
            "[33/33] Loss: 0.8280\n",
            "Epoch: 6 | Source Accuracy: 0.9884, Target Accuracy: 0.2773, Loss: 0.8394\n",
            "Epoch 0007 / 0050\n",
            "============\n",
            "[1/33] Loss: 0.9541\n",
            "[2/33] Loss: 0.9811\n",
            "[3/33] Loss: 0.9360\n",
            "[4/33] Loss: 0.9022\n",
            "[5/33] Loss: 0.9421\n",
            "[6/33] Loss: 0.8625\n",
            "[7/33] Loss: 0.8553\n",
            "[8/33] Loss: 0.8606\n",
            "[9/33] Loss: 0.8236\n",
            "[10/33] Loss: 0.7911\n",
            "[11/33] Loss: 0.9462\n",
            "[12/33] Loss: 0.9943\n",
            "[13/33] Loss: 0.9584\n",
            "[14/33] Loss: 0.9746\n",
            "[15/33] Loss: 0.9953\n",
            "[16/33] Loss: 0.9202\n",
            "[17/33] Loss: 0.8647\n",
            "[18/33] Loss: 0.8793\n",
            "[19/33] Loss: 0.9231\n",
            "[20/33] Loss: 0.8313\n",
            "[21/33] Loss: 0.9254\n",
            "[22/33] Loss: 0.9514\n",
            "[23/33] Loss: 0.9259\n",
            "[24/33] Loss: 1.0021\n",
            "[25/33] Loss: 0.9794\n",
            "[26/33] Loss: 0.8796\n",
            "[27/33] Loss: 0.8426\n",
            "[28/33] Loss: 0.9139\n",
            "[29/33] Loss: 0.8592\n",
            "[30/33] Loss: 0.8869\n",
            "[31/33] Loss: 1.0025\n",
            "[32/33] Loss: 0.9782\n",
            "[33/33] Loss: 0.9924\n",
            "Epoch: 7 | Source Accuracy: 0.9970, Target Accuracy: 0.2515, Loss: 0.9193\n",
            "Epoch 0008 / 0050\n",
            "============\n",
            "[1/33] Loss: 0.8732\n",
            "[2/33] Loss: 1.0369\n",
            "[3/33] Loss: 0.9673\n",
            "[4/33] Loss: 0.9597\n",
            "[5/33] Loss: 1.0475\n",
            "[6/33] Loss: 1.0710\n",
            "[7/33] Loss: 0.9591\n",
            "[8/33] Loss: 1.0622\n",
            "[9/33] Loss: 0.9988\n",
            "[10/33] Loss: 1.2155\n",
            "[11/33] Loss: 0.9614\n",
            "[12/33] Loss: 0.9609\n",
            "[13/33] Loss: 0.9608\n",
            "[14/33] Loss: 1.5744\n",
            "[15/33] Loss: 0.9378\n",
            "[16/33] Loss: 0.8885\n",
            "[17/33] Loss: 1.0027\n",
            "[18/33] Loss: 0.9016\n",
            "[19/33] Loss: 0.9819\n",
            "[20/33] Loss: 1.0480\n",
            "[21/33] Loss: 1.0036\n",
            "[22/33] Loss: 1.0400\n",
            "[23/33] Loss: 0.9144\n",
            "[24/33] Loss: 0.9329\n",
            "[25/33] Loss: 0.9453\n",
            "[26/33] Loss: 0.8772\n",
            "[27/33] Loss: 0.9653\n",
            "[28/33] Loss: 1.0075\n",
            "[29/33] Loss: 0.9578\n",
            "[30/33] Loss: 1.1318\n",
            "[31/33] Loss: 0.8660\n",
            "[32/33] Loss: 0.8753\n",
            "[33/33] Loss: 0.9232\n",
            "Epoch: 8 | Source Accuracy: 0.9803, Target Accuracy: 0.2470, Loss: 0.9954\n",
            "Epoch 0009 / 0050\n",
            "============\n",
            "[1/33] Loss: 0.9082\n",
            "[2/33] Loss: 0.8935\n",
            "[3/33] Loss: 0.8978\n",
            "[4/33] Loss: 0.9336\n",
            "[5/33] Loss: 0.9379\n",
            "[6/33] Loss: 0.8530\n",
            "[7/33] Loss: 0.9372\n",
            "[8/33] Loss: 0.9847\n",
            "[9/33] Loss: 0.8816\n",
            "[10/33] Loss: 1.1073\n",
            "[11/33] Loss: 0.9645\n",
            "[12/33] Loss: 0.9740\n",
            "[13/33] Loss: 0.9792\n",
            "[14/33] Loss: 0.9627\n",
            "[15/33] Loss: 0.9755\n",
            "[16/33] Loss: 1.1379\n",
            "[17/33] Loss: 1.0255\n",
            "[18/33] Loss: 1.0241\n",
            "[19/33] Loss: 1.0230\n",
            "[20/33] Loss: 1.1776\n",
            "[21/33] Loss: 0.9678\n",
            "[22/33] Loss: 1.1404\n",
            "[23/33] Loss: 1.1241\n",
            "[24/33] Loss: 0.9368\n",
            "[25/33] Loss: 0.9594\n",
            "[26/33] Loss: 0.9655\n",
            "[27/33] Loss: 0.9525\n",
            "[28/33] Loss: 1.0070\n",
            "[29/33] Loss: 0.9373\n",
            "[30/33] Loss: 0.9434\n",
            "[31/33] Loss: 0.9954\n",
            "[32/33] Loss: 1.0618\n",
            "[33/33] Loss: 0.9918\n",
            "Epoch: 9 | Source Accuracy: 0.9864, Target Accuracy: 0.2874, Loss: 0.9867\n",
            "Epoch 0010 / 0050\n",
            "============\n",
            "[1/33] Loss: 0.9291\n",
            "[2/33] Loss: 1.0318\n",
            "[3/33] Loss: 0.9989\n",
            "[4/33] Loss: 0.9707\n",
            "[5/33] Loss: 0.9082\n",
            "[6/33] Loss: 1.0245\n",
            "[7/33] Loss: 1.1481\n",
            "[8/33] Loss: 1.0463\n",
            "[9/33] Loss: 1.0427\n",
            "[10/33] Loss: 1.1268\n",
            "[11/33] Loss: 0.9328\n",
            "[12/33] Loss: 1.0084\n",
            "[13/33] Loss: 0.9990\n",
            "[14/33] Loss: 1.0813\n",
            "[15/33] Loss: 1.0962\n",
            "[16/33] Loss: 0.9863\n",
            "[17/33] Loss: 1.0276\n",
            "[18/33] Loss: 1.3245\n",
            "[19/33] Loss: 0.9959\n",
            "[20/33] Loss: 0.9773\n",
            "[21/33] Loss: 1.0883\n",
            "[22/33] Loss: 1.2170\n",
            "[23/33] Loss: 1.1032\n",
            "[24/33] Loss: 1.0751\n",
            "[25/33] Loss: 1.0888\n",
            "[26/33] Loss: 1.1416\n",
            "[27/33] Loss: 1.0508\n",
            "[28/33] Loss: 1.1555\n",
            "[29/33] Loss: 0.9940\n",
            "[30/33] Loss: 1.0722\n",
            "[31/33] Loss: 1.0271\n",
            "[32/33] Loss: 1.1048\n",
            "[33/33] Loss: 0.9982\n",
            "Epoch: 10 | Source Accuracy: 0.9727, Target Accuracy: 0.2586, Loss: 1.0537\n",
            "Epoch 0011 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.0616\n",
            "[2/33] Loss: 1.0637\n",
            "[3/33] Loss: 0.9716\n",
            "[4/33] Loss: 1.1428\n",
            "[5/33] Loss: 0.9395\n",
            "[6/33] Loss: 0.9849\n",
            "[7/33] Loss: 0.9058\n",
            "[8/33] Loss: 1.0357\n",
            "[9/33] Loss: 1.0834\n",
            "[10/33] Loss: 1.3279\n",
            "[11/33] Loss: 1.0071\n",
            "[12/33] Loss: 1.1664\n",
            "[13/33] Loss: 1.3547\n",
            "[14/33] Loss: 0.9576\n",
            "[15/33] Loss: 1.0576\n",
            "[16/33] Loss: 1.1186\n",
            "[17/33] Loss: 1.1413\n",
            "[18/33] Loss: 1.6060\n",
            "[19/33] Loss: 1.0118\n",
            "[20/33] Loss: 1.7941\n",
            "[21/33] Loss: 1.1656\n",
            "[22/33] Loss: 1.1251\n",
            "[23/33] Loss: 1.1743\n",
            "[24/33] Loss: 1.0502\n",
            "[25/33] Loss: 1.3152\n",
            "[26/33] Loss: 1.2099\n",
            "[27/33] Loss: 0.9879\n",
            "[28/33] Loss: 1.1310\n",
            "[29/33] Loss: 1.1905\n",
            "[30/33] Loss: 1.1608\n",
            "[31/33] Loss: 1.2309\n",
            "[32/33] Loss: 1.1570\n",
            "[33/33] Loss: 1.2036\n",
            "Epoch: 11 | Source Accuracy: 0.9717, Target Accuracy: 0.2833, Loss: 1.1465\n",
            "Epoch 0012 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.0860\n",
            "[2/33] Loss: 1.3064\n",
            "[3/33] Loss: 1.1874\n",
            "[4/33] Loss: 1.3939\n",
            "[5/33] Loss: 1.1908\n",
            "[6/33] Loss: 1.1747\n",
            "[7/33] Loss: 1.1520\n",
            "[8/33] Loss: 1.1971\n",
            "[9/33] Loss: 1.1587\n",
            "[10/33] Loss: 2.2323\n",
            "[11/33] Loss: 1.2494\n",
            "[12/33] Loss: 1.2557\n",
            "[13/33] Loss: 1.5793\n",
            "[14/33] Loss: 1.7597\n",
            "[15/33] Loss: 1.4669\n",
            "[16/33] Loss: 1.1564\n",
            "[17/33] Loss: 1.3225\n",
            "[18/33] Loss: 1.2857\n",
            "[19/33] Loss: 1.3639\n",
            "[20/33] Loss: 1.4809\n",
            "[21/33] Loss: 1.2637\n",
            "[22/33] Loss: 1.2005\n",
            "[23/33] Loss: 1.6913\n",
            "[24/33] Loss: 1.2403\n",
            "[25/33] Loss: 1.2412\n",
            "[26/33] Loss: 1.0191\n",
            "[27/33] Loss: 1.0021\n",
            "[28/33] Loss: 1.1409\n",
            "[29/33] Loss: 1.0936\n",
            "[30/33] Loss: 1.1569\n",
            "[31/33] Loss: 0.9653\n",
            "[32/33] Loss: 1.0450\n",
            "[33/33] Loss: 1.3098\n",
            "Epoch: 12 | Source Accuracy: 0.9545, Target Accuracy: 0.2687, Loss: 1.2839\n",
            "Epoch 0013 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.1285\n",
            "[2/33] Loss: 1.0949\n",
            "[3/33] Loss: 1.0658\n",
            "[4/33] Loss: 1.2908\n",
            "[5/33] Loss: 1.2760\n",
            "[6/33] Loss: 1.3400\n",
            "[7/33] Loss: 1.2554\n",
            "[8/33] Loss: 1.1954\n",
            "[9/33] Loss: 1.3691\n",
            "[10/33] Loss: 1.3573\n",
            "[11/33] Loss: 1.2065\n",
            "[12/33] Loss: 1.2044\n",
            "[13/33] Loss: 1.2008\n",
            "[14/33] Loss: 1.2996\n",
            "[15/33] Loss: 1.2723\n",
            "[16/33] Loss: 1.1795\n",
            "[17/33] Loss: 1.1331\n",
            "[18/33] Loss: 1.3074\n",
            "[19/33] Loss: 1.0345\n",
            "[20/33] Loss: 0.9525\n",
            "[21/33] Loss: 1.1683\n",
            "[22/33] Loss: 1.0190\n",
            "[23/33] Loss: 1.1321\n",
            "[24/33] Loss: 1.2802\n",
            "[25/33] Loss: 1.1147\n",
            "[26/33] Loss: 1.1264\n",
            "[27/33] Loss: 1.1615\n",
            "[28/33] Loss: 1.2925\n",
            "[29/33] Loss: 1.3928\n",
            "[30/33] Loss: 1.1567\n",
            "[31/33] Loss: 1.5751\n",
            "[32/33] Loss: 1.4643\n",
            "[33/33] Loss: 1.4380\n",
            "Epoch: 13 | Source Accuracy: 0.9707, Target Accuracy: 0.2990, Loss: 1.2268\n",
            "Epoch 0014 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3842\n",
            "[2/33] Loss: 1.6411\n",
            "[3/33] Loss: 1.4871\n",
            "[4/33] Loss: 1.4522\n",
            "[5/33] Loss: 1.6111\n",
            "[6/33] Loss: 1.2514\n",
            "[7/33] Loss: 1.1959\n",
            "[8/33] Loss: 1.3231\n",
            "[9/33] Loss: 1.0980\n",
            "[10/33] Loss: 1.5829\n",
            "[11/33] Loss: 1.3593\n",
            "[12/33] Loss: 1.3897\n",
            "[13/33] Loss: 1.3407\n",
            "[14/33] Loss: 1.4591\n",
            "[15/33] Loss: 1.2942\n",
            "[16/33] Loss: 1.4998\n",
            "[17/33] Loss: 1.2088\n",
            "[18/33] Loss: 1.2802\n",
            "[19/33] Loss: 1.2799\n",
            "[20/33] Loss: 1.4084\n",
            "[21/33] Loss: 1.1784\n",
            "[22/33] Loss: 1.3940\n",
            "[23/33] Loss: 1.1908\n",
            "[24/33] Loss: 1.3135\n",
            "[25/33] Loss: 1.4885\n",
            "[26/33] Loss: 2.1600\n",
            "[27/33] Loss: 1.2210\n",
            "[28/33] Loss: 1.3170\n",
            "[29/33] Loss: 1.3946\n",
            "[30/33] Loss: 1.3322\n",
            "[31/33] Loss: 1.1993\n",
            "[32/33] Loss: 1.3418\n",
            "[33/33] Loss: 1.6393\n",
            "Epoch: 14 | Source Accuracy: 0.9662, Target Accuracy: 0.2934, Loss: 1.3854\n",
            "Epoch 0015 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.5869\n",
            "[2/33] Loss: 1.3006\n",
            "[3/33] Loss: 1.6292\n",
            "[4/33] Loss: 1.7197\n",
            "[5/33] Loss: 1.3603\n",
            "[6/33] Loss: 1.3513\n",
            "[7/33] Loss: 1.3779\n",
            "[8/33] Loss: 1.3026\n",
            "[9/33] Loss: 1.4707\n",
            "[10/33] Loss: 2.1449\n",
            "[11/33] Loss: 1.2646\n",
            "[12/33] Loss: 1.2648\n",
            "[13/33] Loss: 1.6128\n",
            "[14/33] Loss: 1.8274\n",
            "[15/33] Loss: 1.6269\n",
            "[16/33] Loss: 1.2915\n",
            "[17/33] Loss: 1.3152\n",
            "[18/33] Loss: 1.1460\n",
            "[19/33] Loss: 1.1682\n",
            "[20/33] Loss: 1.3392\n",
            "[21/33] Loss: 1.4353\n",
            "[22/33] Loss: 1.3142\n",
            "[23/33] Loss: 1.4619\n",
            "[24/33] Loss: 1.4122\n",
            "[25/33] Loss: 1.4512\n",
            "[26/33] Loss: 1.3996\n",
            "[27/33] Loss: 1.2754\n",
            "[28/33] Loss: 1.3994\n",
            "[29/33] Loss: 1.2305\n",
            "[30/33] Loss: 2.4202\n",
            "[31/33] Loss: 1.3459\n",
            "[32/33] Loss: 1.5504\n",
            "[33/33] Loss: 1.2085\n",
            "Epoch: 15 | Source Accuracy: 0.9379, Target Accuracy: 0.2970, Loss: 1.4547\n",
            "Epoch 0016 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.2613\n",
            "[2/33] Loss: 1.7236\n",
            "[3/33] Loss: 1.2352\n",
            "[4/33] Loss: 1.4683\n",
            "[5/33] Loss: 1.3904\n",
            "[6/33] Loss: 1.3921\n",
            "[7/33] Loss: 1.2152\n",
            "[8/33] Loss: 1.5033\n",
            "[9/33] Loss: 1.1634\n",
            "[10/33] Loss: 2.2209\n",
            "[11/33] Loss: 1.1970\n",
            "[12/33] Loss: 1.3103\n",
            "[13/33] Loss: 1.2175\n",
            "[14/33] Loss: 1.4118\n",
            "[15/33] Loss: 1.5260\n",
            "[16/33] Loss: 1.2320\n",
            "[17/33] Loss: 1.2768\n",
            "[18/33] Loss: 1.1086\n",
            "[19/33] Loss: 1.2028\n",
            "[20/33] Loss: 1.3397\n",
            "[21/33] Loss: 1.2000\n",
            "[22/33] Loss: 1.3707\n",
            "[23/33] Loss: 1.1221\n",
            "[24/33] Loss: 1.2090\n",
            "[25/33] Loss: 1.3229\n",
            "[26/33] Loss: 1.4772\n",
            "[27/33] Loss: 1.2788\n",
            "[28/33] Loss: 1.1618\n",
            "[29/33] Loss: 1.2183\n",
            "[30/33] Loss: 1.6912\n",
            "[31/33] Loss: 1.3392\n",
            "[32/33] Loss: 1.4953\n",
            "[33/33] Loss: 1.3801\n",
            "Epoch: 16 | Source Accuracy: 0.9672, Target Accuracy: 0.2980, Loss: 1.3534\n",
            "Epoch 0017 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.1921\n",
            "[2/33] Loss: 1.3647\n",
            "[3/33] Loss: 1.4421\n",
            "[4/33] Loss: 1.2142\n",
            "[5/33] Loss: 1.3541\n",
            "[6/33] Loss: 1.0955\n",
            "[7/33] Loss: 1.0998\n",
            "[8/33] Loss: 1.1564\n",
            "[9/33] Loss: 1.1027\n",
            "[10/33] Loss: 1.4848\n",
            "[11/33] Loss: 0.9573\n",
            "[12/33] Loss: 1.0062\n",
            "[13/33] Loss: 1.4655\n",
            "[14/33] Loss: 1.0916\n",
            "[15/33] Loss: 1.2667\n",
            "[16/33] Loss: 1.1964\n",
            "[17/33] Loss: 1.0094\n",
            "[18/33] Loss: 1.5341\n",
            "[19/33] Loss: 1.2419\n",
            "[20/33] Loss: 1.4330\n",
            "[21/33] Loss: 1.0641\n",
            "[22/33] Loss: 1.1401\n",
            "[23/33] Loss: 1.4187\n",
            "[24/33] Loss: 1.3089\n",
            "[25/33] Loss: 1.2840\n",
            "[26/33] Loss: 1.4081\n",
            "[27/33] Loss: 1.4127\n",
            "[28/33] Loss: 1.3412\n",
            "[29/33] Loss: 1.3900\n",
            "[30/33] Loss: 1.1965\n",
            "[31/33] Loss: 1.4426\n",
            "[32/33] Loss: 1.5885\n",
            "[33/33] Loss: 1.8060\n",
            "Epoch: 17 | Source Accuracy: 0.9803, Target Accuracy: 0.2884, Loss: 1.2882\n",
            "Epoch 0018 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.5484\n",
            "[2/33] Loss: 1.4406\n",
            "[3/33] Loss: 1.4269\n",
            "[4/33] Loss: 1.4209\n",
            "[5/33] Loss: 1.5824\n",
            "[6/33] Loss: 1.6578\n",
            "[7/33] Loss: 1.3649\n",
            "[8/33] Loss: 1.5558\n",
            "[9/33] Loss: 1.8388\n",
            "[10/33] Loss: 1.5275\n",
            "[11/33] Loss: 1.2345\n",
            "[12/33] Loss: 1.3800\n",
            "[13/33] Loss: 1.2921\n",
            "[14/33] Loss: 1.4082\n",
            "[15/33] Loss: 1.4479\n",
            "[16/33] Loss: 1.3289\n",
            "[17/33] Loss: 1.0730\n",
            "[18/33] Loss: 1.3223\n",
            "[19/33] Loss: 1.2831\n",
            "[20/33] Loss: 1.3183\n",
            "[21/33] Loss: 1.3848\n",
            "[22/33] Loss: 1.4407\n",
            "[23/33] Loss: 1.4699\n",
            "[24/33] Loss: 1.4718\n",
            "[25/33] Loss: 1.3912\n",
            "[26/33] Loss: 1.3322\n",
            "[27/33] Loss: 1.1430\n",
            "[28/33] Loss: 1.4482\n",
            "[29/33] Loss: 1.3461\n",
            "[30/33] Loss: 1.1493\n",
            "[31/33] Loss: 1.6506\n",
            "[32/33] Loss: 1.2849\n",
            "[33/33] Loss: 1.7239\n",
            "Epoch: 18 | Source Accuracy: 0.9788, Target Accuracy: 0.2955, Loss: 1.4148\n",
            "Epoch 0019 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3770\n",
            "[2/33] Loss: 1.3032\n",
            "[3/33] Loss: 1.2891\n",
            "[4/33] Loss: 1.5329\n",
            "[5/33] Loss: 1.4577\n",
            "[6/33] Loss: 1.2343\n",
            "[7/33] Loss: 1.6504\n",
            "[8/33] Loss: 1.3569\n",
            "[9/33] Loss: 1.1826\n",
            "[10/33] Loss: 1.1123\n",
            "[11/33] Loss: 1.4831\n",
            "[12/33] Loss: 1.2531\n",
            "[13/33] Loss: 1.0004\n",
            "[14/33] Loss: 1.2804\n",
            "[15/33] Loss: 1.4901\n",
            "[16/33] Loss: 1.2473\n",
            "[17/33] Loss: 1.6498\n",
            "[18/33] Loss: 1.3890\n",
            "[19/33] Loss: 1.6269\n",
            "[20/33] Loss: 1.5429\n",
            "[21/33] Loss: 1.5155\n",
            "[22/33] Loss: 1.4662\n",
            "[23/33] Loss: 1.6633\n",
            "[24/33] Loss: 1.2631\n",
            "[25/33] Loss: 1.6332\n",
            "[26/33] Loss: 1.6517\n",
            "[27/33] Loss: 1.6569\n",
            "[28/33] Loss: 1.5374\n",
            "[29/33] Loss: 1.5821\n",
            "[30/33] Loss: 1.4745\n",
            "[31/33] Loss: 1.5133\n",
            "[32/33] Loss: 1.4355\n",
            "[33/33] Loss: 1.6689\n",
            "Epoch: 19 | Source Accuracy: 0.9828, Target Accuracy: 0.3126, Loss: 1.4400\n",
            "Epoch 0020 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.5377\n",
            "[2/33] Loss: 1.4972\n",
            "[3/33] Loss: 1.5565\n",
            "[4/33] Loss: 1.3023\n",
            "[5/33] Loss: 1.5343\n",
            "[6/33] Loss: 1.5617\n",
            "[7/33] Loss: 1.5371\n",
            "[8/33] Loss: 1.5300\n",
            "[9/33] Loss: 1.5831\n",
            "[10/33] Loss: 1.5348\n",
            "[11/33] Loss: 1.4854\n",
            "[12/33] Loss: 1.5622\n",
            "[13/33] Loss: 1.3359\n",
            "[14/33] Loss: 2.0542\n",
            "[15/33] Loss: 1.5180\n",
            "[16/33] Loss: 1.7220\n",
            "[17/33] Loss: 1.9192\n",
            "[18/33] Loss: 1.8664\n",
            "[19/33] Loss: 1.8800\n",
            "[20/33] Loss: 1.6312\n",
            "[21/33] Loss: 1.9659\n",
            "[22/33] Loss: 1.5832\n",
            "[23/33] Loss: 1.6605\n",
            "[24/33] Loss: 1.6845\n",
            "[25/33] Loss: 1.6042\n",
            "[26/33] Loss: 1.5954\n",
            "[27/33] Loss: 1.5102\n",
            "[28/33] Loss: 1.6108\n",
            "[29/33] Loss: 1.6849\n",
            "[30/33] Loss: 1.3257\n",
            "[31/33] Loss: 1.3312\n",
            "[32/33] Loss: 1.3409\n",
            "[33/33] Loss: 1.2926\n",
            "Epoch: 20 | Source Accuracy: 0.9818, Target Accuracy: 0.2965, Loss: 1.5860\n",
            "Epoch 0021 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.2949\n",
            "[2/33] Loss: 1.3449\n",
            "[3/33] Loss: 1.2481\n",
            "[4/33] Loss: 1.2904\n",
            "[5/33] Loss: 1.3545\n",
            "[6/33] Loss: 1.4527\n",
            "[7/33] Loss: 1.1766\n",
            "[8/33] Loss: 1.5556\n",
            "[9/33] Loss: 1.4742\n",
            "[10/33] Loss: 1.9253\n",
            "[11/33] Loss: 1.5229\n",
            "[12/33] Loss: 1.6225\n",
            "[13/33] Loss: 1.6964\n",
            "[14/33] Loss: 2.4240\n",
            "[15/33] Loss: 2.2757\n",
            "[16/33] Loss: 1.5716\n",
            "[17/33] Loss: 1.5909\n",
            "[18/33] Loss: 1.9617\n",
            "[19/33] Loss: 1.7906\n",
            "[20/33] Loss: 2.5011\n",
            "[21/33] Loss: 1.4775\n",
            "[22/33] Loss: 1.5867\n",
            "[23/33] Loss: 1.4408\n",
            "[24/33] Loss: 1.4704\n",
            "[25/33] Loss: 1.4506\n",
            "[26/33] Loss: 1.5397\n",
            "[27/33] Loss: 1.5744\n",
            "[28/33] Loss: 1.4591\n",
            "[29/33] Loss: 1.3684\n",
            "[30/33] Loss: 1.2626\n",
            "[31/33] Loss: 1.3129\n",
            "[32/33] Loss: 1.3584\n",
            "[33/33] Loss: 1.4137\n",
            "Epoch: 21 | Source Accuracy: 0.9854, Target Accuracy: 0.2960, Loss: 1.5694\n",
            "Epoch 0022 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3253\n",
            "[2/33] Loss: 1.1847\n",
            "[3/33] Loss: 1.3860\n",
            "[4/33] Loss: 1.4496\n",
            "[5/33] Loss: 1.2326\n",
            "[6/33] Loss: 1.1794\n",
            "[7/33] Loss: 1.3220\n",
            "[8/33] Loss: 1.2367\n",
            "[9/33] Loss: 1.2608\n",
            "[10/33] Loss: 1.5298\n",
            "[11/33] Loss: 1.4494\n",
            "[12/33] Loss: 1.4021\n",
            "[13/33] Loss: 1.6000\n",
            "[14/33] Loss: 1.2958\n",
            "[15/33] Loss: 1.3649\n",
            "[16/33] Loss: 1.6083\n",
            "[17/33] Loss: 1.3938\n",
            "[18/33] Loss: 1.4058\n",
            "[19/33] Loss: 1.5054\n",
            "[20/33] Loss: 1.6668\n",
            "[21/33] Loss: 1.4765\n",
            "[22/33] Loss: 1.4121\n",
            "[23/33] Loss: 1.3662\n",
            "[24/33] Loss: 1.2143\n",
            "[25/33] Loss: 1.4508\n",
            "[26/33] Loss: 1.7431\n",
            "[27/33] Loss: 1.3894\n",
            "[28/33] Loss: 1.5966\n",
            "[29/33] Loss: 1.6276\n",
            "[30/33] Loss: 1.4025\n",
            "[31/33] Loss: 1.4132\n",
            "[32/33] Loss: 1.2859\n",
            "[33/33] Loss: 1.4410\n",
            "Epoch: 22 | Source Accuracy: 0.9904, Target Accuracy: 0.2823, Loss: 1.4127\n",
            "Epoch 0023 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.5493\n",
            "[2/33] Loss: 1.5039\n",
            "[3/33] Loss: 1.5736\n",
            "[4/33] Loss: 1.4959\n",
            "[5/33] Loss: 1.4800\n",
            "[6/33] Loss: 1.3969\n",
            "[7/33] Loss: 1.4689\n",
            "[8/33] Loss: 1.2966\n",
            "[9/33] Loss: 1.2598\n",
            "[10/33] Loss: 1.2998\n",
            "[11/33] Loss: 1.1427\n",
            "[12/33] Loss: 1.2191\n",
            "[13/33] Loss: 1.2999\n",
            "[14/33] Loss: 1.3785\n",
            "[15/33] Loss: 0.9720\n",
            "[16/33] Loss: 1.2601\n",
            "[17/33] Loss: 1.3584\n",
            "[18/33] Loss: 1.3016\n",
            "[19/33] Loss: 1.5178\n",
            "[20/33] Loss: 1.2373\n",
            "[21/33] Loss: 1.3419\n",
            "[22/33] Loss: 1.3473\n",
            "[23/33] Loss: 1.5596\n",
            "[24/33] Loss: 1.5176\n",
            "[25/33] Loss: 1.3295\n",
            "[26/33] Loss: 1.4140\n",
            "[27/33] Loss: 1.3264\n",
            "[28/33] Loss: 1.5645\n",
            "[29/33] Loss: 1.4789\n",
            "[30/33] Loss: 1.4871\n",
            "[31/33] Loss: 1.4006\n",
            "[32/33] Loss: 1.5438\n",
            "[33/33] Loss: 1.7897\n",
            "Epoch: 23 | Source Accuracy: 0.9894, Target Accuracy: 0.2884, Loss: 1.3974\n",
            "Epoch 0024 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.6049\n",
            "[2/33] Loss: 1.4350\n",
            "[3/33] Loss: 1.3650\n",
            "[4/33] Loss: 1.4420\n",
            "[5/33] Loss: 1.3772\n",
            "[6/33] Loss: 1.3830\n",
            "[7/33] Loss: 1.4266\n",
            "[8/33] Loss: 1.3559\n",
            "[9/33] Loss: 1.3479\n",
            "[10/33] Loss: 1.7126\n",
            "[11/33] Loss: 1.4648\n",
            "[12/33] Loss: 1.3595\n",
            "[13/33] Loss: 2.3141\n",
            "[14/33] Loss: 1.5491\n",
            "[15/33] Loss: 1.3083\n",
            "[16/33] Loss: 1.4911\n",
            "[17/33] Loss: 1.3460\n",
            "[18/33] Loss: 1.4148\n",
            "[19/33] Loss: 1.4220\n",
            "[20/33] Loss: 2.2078\n",
            "[21/33] Loss: 1.4534\n",
            "[22/33] Loss: 1.4262\n",
            "[23/33] Loss: 1.4576\n",
            "[24/33] Loss: 1.2968\n",
            "[25/33] Loss: 1.3292\n",
            "[26/33] Loss: 1.6502\n",
            "[27/33] Loss: 1.9276\n",
            "[28/33] Loss: 1.3585\n",
            "[29/33] Loss: 1.3166\n",
            "[30/33] Loss: 1.4858\n",
            "[31/33] Loss: 1.3949\n",
            "[32/33] Loss: 1.4924\n",
            "[33/33] Loss: 1.6185\n",
            "Epoch: 24 | Source Accuracy: 0.9864, Target Accuracy: 0.2753, Loss: 1.5011\n",
            "Epoch 0025 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.7315\n",
            "[2/33] Loss: 1.2048\n",
            "[3/33] Loss: 1.4459\n",
            "[4/33] Loss: 1.3324\n",
            "[5/33] Loss: 1.5725\n",
            "[6/33] Loss: 1.3387\n",
            "[7/33] Loss: 1.5190\n",
            "[8/33] Loss: 1.2949\n",
            "[9/33] Loss: 1.4424\n",
            "[10/33] Loss: 1.6792\n",
            "[11/33] Loss: 1.3721\n",
            "[12/33] Loss: 1.2361\n",
            "[13/33] Loss: 1.4695\n",
            "[14/33] Loss: 1.2935\n",
            "[15/33] Loss: 1.4050\n",
            "[16/33] Loss: 1.3834\n",
            "[17/33] Loss: 1.3201\n",
            "[18/33] Loss: 1.2269\n",
            "[19/33] Loss: 1.3741\n",
            "[20/33] Loss: 1.5220\n",
            "[21/33] Loss: 1.2989\n",
            "[22/33] Loss: 1.2233\n",
            "[23/33] Loss: 1.5010\n",
            "[24/33] Loss: 1.4109\n",
            "[25/33] Loss: 1.4778\n",
            "[26/33] Loss: 1.3882\n",
            "[27/33] Loss: 1.3630\n",
            "[28/33] Loss: 1.4255\n",
            "[29/33] Loss: 1.2898\n",
            "[30/33] Loss: 1.4542\n",
            "[31/33] Loss: 1.3661\n",
            "[32/33] Loss: 1.3270\n",
            "[33/33] Loss: 1.3384\n",
            "Epoch: 25 | Source Accuracy: 0.9859, Target Accuracy: 0.2909, Loss: 1.3948\n",
            "Epoch 0026 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.4915\n",
            "[2/33] Loss: 1.5745\n",
            "[3/33] Loss: 1.4541\n",
            "[4/33] Loss: 1.3057\n",
            "[5/33] Loss: 1.3757\n",
            "[6/33] Loss: 1.4155\n",
            "[7/33] Loss: 1.4082\n",
            "[8/33] Loss: 1.2610\n",
            "[9/33] Loss: 1.3639\n",
            "[10/33] Loss: 1.6060\n",
            "[11/33] Loss: 1.3258\n",
            "[12/33] Loss: 1.4174\n",
            "[13/33] Loss: 1.2219\n",
            "[14/33] Loss: 1.3789\n",
            "[15/33] Loss: 1.3693\n",
            "[16/33] Loss: 1.3576\n",
            "[17/33] Loss: 1.4240\n",
            "[18/33] Loss: 1.3543\n",
            "[19/33] Loss: 1.2707\n",
            "[20/33] Loss: 1.4115\n",
            "[21/33] Loss: 1.6712\n",
            "[22/33] Loss: 1.3207\n",
            "[23/33] Loss: 1.3117\n",
            "[24/33] Loss: 1.3549\n",
            "[25/33] Loss: 1.2700\n",
            "[26/33] Loss: 1.3143\n",
            "[27/33] Loss: 1.4014\n",
            "[28/33] Loss: 1.3458\n",
            "[29/33] Loss: 1.3374\n",
            "[30/33] Loss: 1.4337\n",
            "[31/33] Loss: 1.3926\n",
            "[32/33] Loss: 1.3333\n",
            "[33/33] Loss: 1.2910\n",
            "Epoch: 26 | Source Accuracy: 0.9899, Target Accuracy: 0.2768, Loss: 1.3808\n",
            "Epoch 0027 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3480\n",
            "[2/33] Loss: 1.4238\n",
            "[3/33] Loss: 1.3108\n",
            "[4/33] Loss: 1.3326\n",
            "[5/33] Loss: 1.3733\n",
            "[6/33] Loss: 1.4838\n",
            "[7/33] Loss: 1.4646\n",
            "[8/33] Loss: 1.5892\n",
            "[9/33] Loss: 1.3311\n",
            "[10/33] Loss: 1.2442\n",
            "[11/33] Loss: 1.2764\n",
            "[12/33] Loss: 1.4225\n",
            "[13/33] Loss: 1.5971\n",
            "[14/33] Loss: 1.4135\n",
            "[15/33] Loss: 1.5384\n",
            "[16/33] Loss: 1.4826\n",
            "[17/33] Loss: 1.4758\n",
            "[18/33] Loss: 1.3174\n",
            "[19/33] Loss: 1.2666\n",
            "[20/33] Loss: 1.3363\n",
            "[21/33] Loss: 1.4004\n",
            "[22/33] Loss: 1.4244\n",
            "[23/33] Loss: 1.2906\n",
            "[24/33] Loss: 1.4632\n",
            "[25/33] Loss: 1.3805\n",
            "[26/33] Loss: 1.2932\n",
            "[27/33] Loss: 1.3532\n",
            "[28/33] Loss: 1.4173\n",
            "[29/33] Loss: 1.3692\n",
            "[30/33] Loss: 1.2734\n",
            "[31/33] Loss: 1.2440\n",
            "[32/33] Loss: 1.3864\n",
            "[33/33] Loss: 1.3110\n",
            "Epoch: 27 | Source Accuracy: 0.9965, Target Accuracy: 0.2788, Loss: 1.3829\n",
            "Epoch 0028 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.2997\n",
            "[2/33] Loss: 1.3710\n",
            "[3/33] Loss: 1.1846\n",
            "[4/33] Loss: 1.3016\n",
            "[5/33] Loss: 1.2602\n",
            "[6/33] Loss: 1.3141\n",
            "[7/33] Loss: 1.2657\n",
            "[8/33] Loss: 1.4921\n",
            "[9/33] Loss: 1.5398\n",
            "[10/33] Loss: 2.0243\n",
            "[11/33] Loss: 1.4715\n",
            "[12/33] Loss: 1.4531\n",
            "[13/33] Loss: 1.3750\n",
            "[14/33] Loss: 1.4642\n",
            "[15/33] Loss: 1.2342\n",
            "[16/33] Loss: 1.4786\n",
            "[17/33] Loss: 1.6303\n",
            "[18/33] Loss: 2.0398\n",
            "[19/33] Loss: 1.7017\n",
            "[20/33] Loss: 1.9888\n",
            "[21/33] Loss: 1.5144\n",
            "[22/33] Loss: 1.4592\n",
            "[23/33] Loss: 1.4390\n",
            "[24/33] Loss: 1.6343\n",
            "[25/33] Loss: 1.5370\n",
            "[26/33] Loss: 1.7790\n",
            "[27/33] Loss: 1.4384\n",
            "[28/33] Loss: 1.3473\n",
            "[29/33] Loss: 1.5551\n",
            "[30/33] Loss: 2.3865\n",
            "[31/33] Loss: 1.5550\n",
            "[32/33] Loss: 1.3416\n",
            "[33/33] Loss: 1.5621\n",
            "Epoch: 28 | Source Accuracy: 0.9889, Target Accuracy: 0.2965, Loss: 1.5285\n",
            "Epoch 0029 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.5596\n",
            "[2/33] Loss: 1.4920\n",
            "[3/33] Loss: 1.5613\n",
            "[4/33] Loss: 1.4765\n",
            "[5/33] Loss: 1.4912\n",
            "[6/33] Loss: 1.3929\n",
            "[7/33] Loss: 1.3719\n",
            "[8/33] Loss: 1.2606\n",
            "[9/33] Loss: 1.2242\n",
            "[10/33] Loss: 1.1397\n",
            "[11/33] Loss: 1.4260\n",
            "[12/33] Loss: 1.3379\n",
            "[13/33] Loss: 1.5167\n",
            "[14/33] Loss: 1.2322\n",
            "[15/33] Loss: 1.3596\n",
            "[16/33] Loss: 1.3382\n",
            "[17/33] Loss: 1.5210\n",
            "[18/33] Loss: 1.4017\n",
            "[19/33] Loss: 1.2711\n",
            "[20/33] Loss: 1.0447\n",
            "[21/33] Loss: 1.3509\n",
            "[22/33] Loss: 1.3777\n",
            "[23/33] Loss: 1.3626\n",
            "[24/33] Loss: 1.3014\n",
            "[25/33] Loss: 1.2233\n",
            "[26/33] Loss: 1.5934\n",
            "[27/33] Loss: 1.5403\n",
            "[28/33] Loss: 1.5273\n",
            "[29/33] Loss: 1.6076\n",
            "[30/33] Loss: 1.3131\n",
            "[31/33] Loss: 1.4197\n",
            "[32/33] Loss: 1.5056\n",
            "[33/33] Loss: 1.6730\n",
            "Epoch: 29 | Source Accuracy: 0.9960, Target Accuracy: 0.3010, Loss: 1.4005\n",
            "Epoch 0030 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.4343\n",
            "[2/33] Loss: 1.4955\n",
            "[3/33] Loss: 1.4394\n",
            "[4/33] Loss: 1.6162\n",
            "[5/33] Loss: 1.6655\n",
            "[6/33] Loss: 1.8295\n",
            "[7/33] Loss: 1.7864\n",
            "[8/33] Loss: 1.5374\n",
            "[9/33] Loss: 1.4844\n",
            "[10/33] Loss: 1.7854\n",
            "[11/33] Loss: 1.5167\n",
            "[12/33] Loss: 1.5550\n",
            "[13/33] Loss: 1.5855\n",
            "[14/33] Loss: 1.8263\n",
            "[15/33] Loss: 1.7268\n",
            "[16/33] Loss: 1.7002\n",
            "[17/33] Loss: 1.5444\n",
            "[18/33] Loss: 1.4878\n",
            "[19/33] Loss: 1.4263\n",
            "[20/33] Loss: 1.2602\n",
            "[21/33] Loss: 1.3587\n",
            "[22/33] Loss: 1.3736\n",
            "[23/33] Loss: 1.2849\n",
            "[24/33] Loss: 1.3033\n",
            "[25/33] Loss: 1.1248\n",
            "[26/33] Loss: 1.2315\n",
            "[27/33] Loss: 1.0668\n",
            "[28/33] Loss: 1.1235\n",
            "[29/33] Loss: 1.4001\n",
            "[30/33] Loss: 1.1385\n",
            "[31/33] Loss: 1.0833\n",
            "[32/33] Loss: 1.6472\n",
            "[33/33] Loss: 1.5236\n",
            "Epoch: 30 | Source Accuracy: 0.9949, Target Accuracy: 0.2697, Loss: 1.4655\n",
            "Epoch 0031 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.4920\n",
            "[2/33] Loss: 1.4143\n",
            "[3/33] Loss: 1.6919\n",
            "[4/33] Loss: 1.6315\n",
            "[5/33] Loss: 1.5239\n",
            "[6/33] Loss: 1.5957\n",
            "[7/33] Loss: 1.7016\n",
            "[8/33] Loss: 1.7373\n",
            "[9/33] Loss: 1.6290\n",
            "[10/33] Loss: 1.9711\n",
            "[11/33] Loss: 1.5478\n",
            "[12/33] Loss: 1.7407\n",
            "[13/33] Loss: 1.5200\n",
            "[14/33] Loss: 1.8400\n",
            "[15/33] Loss: 1.8009\n",
            "[16/33] Loss: 1.5453\n",
            "[17/33] Loss: 1.5198\n",
            "[18/33] Loss: 1.5274\n",
            "[19/33] Loss: 1.5841\n",
            "[20/33] Loss: 1.6234\n",
            "[21/33] Loss: 1.3673\n",
            "[22/33] Loss: 1.3715\n",
            "[23/33] Loss: 1.4012\n",
            "[24/33] Loss: 1.5260\n",
            "[25/33] Loss: 1.3753\n",
            "[26/33] Loss: 1.5823\n",
            "[27/33] Loss: 1.1822\n",
            "[28/33] Loss: 1.3404\n",
            "[29/33] Loss: 1.2596\n",
            "[30/33] Loss: 1.5733\n",
            "[31/33] Loss: 1.4495\n",
            "[32/33] Loss: 1.3170\n",
            "[33/33] Loss: 1.4902\n",
            "Epoch: 31 | Source Accuracy: 0.9904, Target Accuracy: 0.2889, Loss: 1.5416\n",
            "Epoch 0032 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3924\n",
            "[2/33] Loss: 1.2960\n",
            "[3/33] Loss: 1.3560\n",
            "[4/33] Loss: 1.4600\n",
            "[5/33] Loss: 1.4447\n",
            "[6/33] Loss: 1.9284\n",
            "[7/33] Loss: 1.2718\n",
            "[8/33] Loss: 1.3449\n",
            "[9/33] Loss: 1.3597\n",
            "[10/33] Loss: 1.2058\n",
            "[11/33] Loss: 1.4776\n",
            "[12/33] Loss: 1.5445\n",
            "[13/33] Loss: 1.2933\n",
            "[14/33] Loss: 1.3126\n",
            "[15/33] Loss: 1.4543\n",
            "[16/33] Loss: 1.5072\n",
            "[17/33] Loss: 1.3359\n",
            "[18/33] Loss: 1.5354\n",
            "[19/33] Loss: 1.4835\n",
            "[20/33] Loss: 1.2659\n",
            "[21/33] Loss: 1.7666\n",
            "[22/33] Loss: 1.6364\n",
            "[23/33] Loss: 1.4851\n",
            "[24/33] Loss: 1.6684\n",
            "[25/33] Loss: 1.4472\n",
            "[26/33] Loss: 1.4867\n",
            "[27/33] Loss: 1.8007\n",
            "[28/33] Loss: 1.9411\n",
            "[29/33] Loss: 1.7035\n",
            "[30/33] Loss: 1.2214\n",
            "[31/33] Loss: 1.8706\n",
            "[32/33] Loss: 1.5765\n",
            "[33/33] Loss: 1.5674\n",
            "Epoch: 32 | Source Accuracy: 0.9894, Target Accuracy: 0.3015, Loss: 1.4982\n",
            "Epoch 0033 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.4911\n",
            "[2/33] Loss: 1.4924\n",
            "[3/33] Loss: 1.4656\n",
            "[4/33] Loss: 1.4224\n",
            "[5/33] Loss: 1.3593\n",
            "[6/33] Loss: 1.4694\n",
            "[7/33] Loss: 1.3618\n",
            "[8/33] Loss: 1.2735\n",
            "[9/33] Loss: 1.3282\n",
            "[10/33] Loss: 1.3667\n",
            "[11/33] Loss: 1.2657\n",
            "[12/33] Loss: 1.4256\n",
            "[13/33] Loss: 1.1933\n",
            "[14/33] Loss: 1.1132\n",
            "[15/33] Loss: 1.2739\n",
            "[16/33] Loss: 1.3347\n",
            "[17/33] Loss: 1.3510\n",
            "[18/33] Loss: 1.4710\n",
            "[19/33] Loss: 1.4045\n",
            "[20/33] Loss: 1.2880\n",
            "[21/33] Loss: 1.3936\n",
            "[22/33] Loss: 1.4651\n",
            "[23/33] Loss: 1.4073\n",
            "[24/33] Loss: 1.4066\n",
            "[25/33] Loss: 1.4755\n",
            "[26/33] Loss: 1.4855\n",
            "[27/33] Loss: 1.4533\n",
            "[28/33] Loss: 1.5424\n",
            "[29/33] Loss: 1.4357\n",
            "[30/33] Loss: 1.3211\n",
            "[31/33] Loss: 1.3971\n",
            "[32/33] Loss: 1.4398\n",
            "[33/33] Loss: 1.3838\n",
            "Epoch: 33 | Source Accuracy: 0.9970, Target Accuracy: 0.3076, Loss: 1.3866\n",
            "Epoch 0034 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.2984\n",
            "[2/33] Loss: 1.4159\n",
            "[3/33] Loss: 1.3558\n",
            "[4/33] Loss: 1.3185\n",
            "[5/33] Loss: 1.3818\n",
            "[6/33] Loss: 1.4798\n",
            "[7/33] Loss: 1.3891\n",
            "[8/33] Loss: 1.3216\n",
            "[9/33] Loss: 1.3763\n",
            "[10/33] Loss: 1.5113\n",
            "[11/33] Loss: 1.2793\n",
            "[12/33] Loss: 1.3250\n",
            "[13/33] Loss: 1.2852\n",
            "[14/33] Loss: 1.3105\n",
            "[15/33] Loss: 1.2971\n",
            "[16/33] Loss: 1.4356\n",
            "[17/33] Loss: 1.4305\n",
            "[18/33] Loss: 1.4299\n",
            "[19/33] Loss: 1.4069\n",
            "[20/33] Loss: 1.4150\n",
            "[21/33] Loss: 1.3405\n",
            "[22/33] Loss: 1.3619\n",
            "[23/33] Loss: 1.4040\n",
            "[24/33] Loss: 1.3514\n",
            "[25/33] Loss: 1.3702\n",
            "[26/33] Loss: 1.3998\n",
            "[27/33] Loss: 1.3797\n",
            "[28/33] Loss: 1.3064\n",
            "[29/33] Loss: 1.3836\n",
            "[30/33] Loss: 1.3578\n",
            "[31/33] Loss: 1.3562\n",
            "[32/33] Loss: 1.3312\n",
            "[33/33] Loss: 1.3499\n",
            "Epoch: 34 | Source Accuracy: 0.9995, Target Accuracy: 0.3081, Loss: 1.3684\n",
            "Epoch 0035 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3256\n",
            "[2/33] Loss: 1.3348\n",
            "[3/33] Loss: 1.5151\n",
            "[4/33] Loss: 1.3778\n",
            "[5/33] Loss: 1.4078\n",
            "[6/33] Loss: 1.4220\n",
            "[7/33] Loss: 1.4542\n",
            "[8/33] Loss: 1.3722\n",
            "[9/33] Loss: 1.3415\n",
            "[10/33] Loss: 1.5398\n",
            "[11/33] Loss: 1.3933\n",
            "[12/33] Loss: 1.3276\n",
            "[13/33] Loss: 1.4364\n",
            "[14/33] Loss: 1.3991\n",
            "[15/33] Loss: 1.3547\n",
            "[16/33] Loss: 1.3896\n",
            "[17/33] Loss: 1.4394\n",
            "[18/33] Loss: 1.3806\n",
            "[19/33] Loss: 1.3553\n",
            "[20/33] Loss: 1.3455\n",
            "[21/33] Loss: 1.3564\n",
            "[22/33] Loss: 1.3478\n",
            "[23/33] Loss: 1.4033\n",
            "[24/33] Loss: 1.3851\n",
            "[25/33] Loss: 1.4312\n",
            "[26/33] Loss: 1.3483\n",
            "[27/33] Loss: 1.4321\n",
            "[28/33] Loss: 1.3219\n",
            "[29/33] Loss: 1.3732\n",
            "[30/33] Loss: 1.3203\n",
            "[31/33] Loss: 1.3706\n",
            "[32/33] Loss: 1.4114\n",
            "[33/33] Loss: 1.4472\n",
            "Epoch: 35 | Source Accuracy: 0.9970, Target Accuracy: 0.2803, Loss: 1.3897\n",
            "Epoch 0036 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3720\n",
            "[2/33] Loss: 1.5879\n",
            "[3/33] Loss: 1.4642\n",
            "[4/33] Loss: 1.3554\n",
            "[5/33] Loss: 1.3672\n",
            "[6/33] Loss: 1.4361\n",
            "[7/33] Loss: 1.3179\n",
            "[8/33] Loss: 1.3666\n",
            "[9/33] Loss: 1.3965\n",
            "[10/33] Loss: 1.4285\n",
            "[11/33] Loss: 1.3194\n",
            "[12/33] Loss: 1.4103\n",
            "[13/33] Loss: 1.3586\n",
            "[14/33] Loss: 1.3204\n",
            "[15/33] Loss: 1.3684\n",
            "[16/33] Loss: 1.3313\n",
            "[17/33] Loss: 1.3339\n",
            "[18/33] Loss: 1.3279\n",
            "[19/33] Loss: 1.3458\n",
            "[20/33] Loss: 1.2919\n",
            "[21/33] Loss: 1.3488\n",
            "[22/33] Loss: 1.3764\n",
            "[23/33] Loss: 1.3180\n",
            "[24/33] Loss: 1.3265\n",
            "[25/33] Loss: 1.3891\n",
            "[26/33] Loss: 1.3324\n",
            "[27/33] Loss: 1.2981\n",
            "[28/33] Loss: 1.3419\n",
            "[29/33] Loss: 1.4288\n",
            "[30/33] Loss: 1.3275\n",
            "[31/33] Loss: 1.3138\n",
            "[32/33] Loss: 1.3601\n",
            "[33/33] Loss: 1.3180\n",
            "Epoch: 36 | Source Accuracy: 0.9975, Target Accuracy: 0.2869, Loss: 1.3630\n",
            "Epoch 0037 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3391\n",
            "[2/33] Loss: 1.3862\n",
            "[3/33] Loss: 1.3179\n",
            "[4/33] Loss: 1.2986\n",
            "[5/33] Loss: 1.3944\n",
            "[6/33] Loss: 1.3279\n",
            "[7/33] Loss: 1.3478\n",
            "[8/33] Loss: 1.5222\n",
            "[9/33] Loss: 1.3163\n",
            "[10/33] Loss: 1.2933\n",
            "[11/33] Loss: 1.3422\n",
            "[12/33] Loss: 1.3732\n",
            "[13/33] Loss: 1.3587\n",
            "[14/33] Loss: 1.3197\n",
            "[15/33] Loss: 1.3499\n",
            "[16/33] Loss: 1.3348\n",
            "[17/33] Loss: 1.3567\n",
            "[18/33] Loss: 1.3182\n",
            "[19/33] Loss: 1.3004\n",
            "[20/33] Loss: 1.3122\n",
            "[21/33] Loss: 1.4038\n",
            "[22/33] Loss: 1.3606\n",
            "[23/33] Loss: 1.3657\n",
            "[24/33] Loss: 1.3108\n",
            "[25/33] Loss: 1.3915\n",
            "[26/33] Loss: 1.3869\n",
            "[27/33] Loss: 1.3737\n",
            "[28/33] Loss: 1.3462\n",
            "[29/33] Loss: 1.3111\n",
            "[30/33] Loss: 1.2802\n",
            "[31/33] Loss: 1.3562\n",
            "[32/33] Loss: 1.2967\n",
            "[33/33] Loss: 1.3946\n",
            "Epoch: 37 | Source Accuracy: 0.9985, Target Accuracy: 0.3141, Loss: 1.3481\n",
            "Epoch 0038 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3518\n",
            "[2/33] Loss: 1.3557\n",
            "[3/33] Loss: 1.4276\n",
            "[4/33] Loss: 1.3732\n",
            "[5/33] Loss: 1.3146\n",
            "[6/33] Loss: 1.3280\n",
            "[7/33] Loss: 1.3916\n",
            "[8/33] Loss: 1.2997\n",
            "[9/33] Loss: 1.3584\n",
            "[10/33] Loss: 1.3363\n",
            "[11/33] Loss: 1.3619\n",
            "[12/33] Loss: 1.3324\n",
            "[13/33] Loss: 1.3579\n",
            "[14/33] Loss: 1.3175\n",
            "[15/33] Loss: 1.3601\n",
            "[16/33] Loss: 1.3092\n",
            "[17/33] Loss: 1.3100\n",
            "[18/33] Loss: 1.4002\n",
            "[19/33] Loss: 1.4349\n",
            "[20/33] Loss: 1.4019\n",
            "[21/33] Loss: 1.3439\n",
            "[22/33] Loss: 1.3980\n",
            "[23/33] Loss: 1.3354\n",
            "[24/33] Loss: 1.3197\n",
            "[25/33] Loss: 1.3497\n",
            "[26/33] Loss: 1.2930\n",
            "[27/33] Loss: 1.3726\n",
            "[28/33] Loss: 1.3341\n",
            "[29/33] Loss: 1.4102\n",
            "[30/33] Loss: 1.4085\n",
            "[31/33] Loss: 1.2940\n",
            "[32/33] Loss: 1.3625\n",
            "[33/33] Loss: 1.3961\n",
            "Epoch: 38 | Source Accuracy: 0.9975, Target Accuracy: 0.3172, Loss: 1.3558\n",
            "Epoch 0039 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3242\n",
            "[2/33] Loss: 1.3434\n",
            "[3/33] Loss: 1.2546\n",
            "[4/33] Loss: 1.3430\n",
            "[5/33] Loss: 1.3978\n",
            "[6/33] Loss: 1.3662\n",
            "[7/33] Loss: 1.3831\n",
            "[8/33] Loss: 1.3609\n",
            "[9/33] Loss: 1.3761\n",
            "[10/33] Loss: 1.3829\n",
            "[11/33] Loss: 1.3294\n",
            "[12/33] Loss: 1.3556\n",
            "[13/33] Loss: 1.3484\n",
            "[14/33] Loss: 1.2978\n",
            "[15/33] Loss: 1.3838\n",
            "[16/33] Loss: 1.3738\n",
            "[17/33] Loss: 1.3728\n",
            "[18/33] Loss: 1.3573\n",
            "[19/33] Loss: 1.3516\n",
            "[20/33] Loss: 1.3019\n",
            "[21/33] Loss: 1.3490\n",
            "[22/33] Loss: 1.3517\n",
            "[23/33] Loss: 1.3627\n",
            "[24/33] Loss: 1.3179\n",
            "[25/33] Loss: 1.3561\n",
            "[26/33] Loss: 1.3436\n",
            "[27/33] Loss: 1.3550\n",
            "[28/33] Loss: 1.3640\n",
            "[29/33] Loss: 1.3590\n",
            "[30/33] Loss: 1.3444\n",
            "[31/33] Loss: 1.3156\n",
            "[32/33] Loss: 1.3807\n",
            "[33/33] Loss: 1.2874\n",
            "Epoch: 39 | Source Accuracy: 0.9995, Target Accuracy: 0.3051, Loss: 1.3482\n",
            "Epoch 0040 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3869\n",
            "[2/33] Loss: 1.4441\n",
            "[3/33] Loss: 1.2724\n",
            "[4/33] Loss: 1.1973\n",
            "[5/33] Loss: 1.3031\n",
            "[6/33] Loss: 1.3458\n",
            "[7/33] Loss: 1.4344\n",
            "[8/33] Loss: 1.3241\n",
            "[9/33] Loss: 1.3906\n",
            "[10/33] Loss: 1.3714\n",
            "[11/33] Loss: 1.3978\n",
            "[12/33] Loss: 1.4058\n",
            "[13/33] Loss: 1.3241\n",
            "[14/33] Loss: 1.2792\n",
            "[15/33] Loss: 1.3271\n",
            "[16/33] Loss: 1.4245\n",
            "[17/33] Loss: 1.3949\n",
            "[18/33] Loss: 1.3504\n",
            "[19/33] Loss: 1.3778\n",
            "[20/33] Loss: 1.3875\n",
            "[21/33] Loss: 1.3387\n",
            "[22/33] Loss: 1.4000\n",
            "[23/33] Loss: 1.3467\n",
            "[24/33] Loss: 1.3128\n",
            "[25/33] Loss: 1.3059\n",
            "[26/33] Loss: 1.3641\n",
            "[27/33] Loss: 1.4621\n",
            "[28/33] Loss: 1.3379\n",
            "[29/33] Loss: 1.3798\n",
            "[30/33] Loss: 1.3379\n",
            "[31/33] Loss: 1.3120\n",
            "[32/33] Loss: 1.3316\n",
            "[33/33] Loss: 1.3623\n",
            "Epoch: 40 | Source Accuracy: 0.9995, Target Accuracy: 0.3086, Loss: 1.3555\n",
            "Epoch 0041 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.2933\n",
            "[2/33] Loss: 1.4106\n",
            "[3/33] Loss: 1.3391\n",
            "[4/33] Loss: 1.3343\n",
            "[5/33] Loss: 1.3876\n",
            "[6/33] Loss: 1.3637\n",
            "[7/33] Loss: 1.3268\n",
            "[8/33] Loss: 1.3291\n",
            "[9/33] Loss: 1.3806\n",
            "[10/33] Loss: 1.6088\n",
            "[11/33] Loss: 1.3116\n",
            "[12/33] Loss: 1.3602\n",
            "[13/33] Loss: 1.3464\n",
            "[14/33] Loss: 1.3360\n",
            "[15/33] Loss: 1.3600\n",
            "[16/33] Loss: 1.3432\n",
            "[17/33] Loss: 1.3378\n",
            "[18/33] Loss: 1.3230\n",
            "[19/33] Loss: 1.3943\n",
            "[20/33] Loss: 1.3088\n",
            "[21/33] Loss: 1.3193\n",
            "[22/33] Loss: 1.3565\n",
            "[23/33] Loss: 1.3439\n",
            "[24/33] Loss: 1.3419\n",
            "[25/33] Loss: 1.3116\n",
            "[26/33] Loss: 1.3364\n",
            "[27/33] Loss: 1.3191\n",
            "[28/33] Loss: 1.2899\n",
            "[29/33] Loss: 1.3556\n",
            "[30/33] Loss: 1.3234\n",
            "[31/33] Loss: 1.3285\n",
            "[32/33] Loss: 1.3947\n",
            "[33/33] Loss: 1.3565\n",
            "Epoch: 41 | Source Accuracy: 0.9990, Target Accuracy: 0.3010, Loss: 1.3507\n",
            "Epoch 0042 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3062\n",
            "[2/33] Loss: 1.3372\n",
            "[3/33] Loss: 1.3171\n",
            "[4/33] Loss: 1.3644\n",
            "[5/33] Loss: 1.3290\n",
            "[6/33] Loss: 1.2938\n",
            "[7/33] Loss: 1.3515\n",
            "[8/33] Loss: 1.3239\n",
            "[9/33] Loss: 1.3140\n",
            "[10/33] Loss: 1.3589\n",
            "[11/33] Loss: 1.3328\n",
            "[12/33] Loss: 1.3321\n",
            "[13/33] Loss: 1.3630\n",
            "[14/33] Loss: 1.3770\n",
            "[15/33] Loss: 1.3068\n",
            "[16/33] Loss: 1.3364\n",
            "[17/33] Loss: 1.3258\n",
            "[18/33] Loss: 1.2862\n",
            "[19/33] Loss: 1.3241\n",
            "[20/33] Loss: 1.3154\n",
            "[21/33] Loss: 1.3812\n",
            "[22/33] Loss: 1.3438\n",
            "[23/33] Loss: 1.3591\n",
            "[24/33] Loss: 1.3480\n",
            "[25/33] Loss: 1.3124\n",
            "[26/33] Loss: 1.2476\n",
            "[27/33] Loss: 1.3827\n",
            "[28/33] Loss: 1.2929\n",
            "[29/33] Loss: 1.3226\n",
            "[30/33] Loss: 1.3242\n",
            "[31/33] Loss: 1.3613\n",
            "[32/33] Loss: 1.3080\n",
            "[33/33] Loss: 1.3232\n",
            "Epoch: 42 | Source Accuracy: 1.0000, Target Accuracy: 0.3177, Loss: 1.3304\n",
            "Epoch 0043 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3729\n",
            "[2/33] Loss: 1.2849\n",
            "[3/33] Loss: 1.3289\n",
            "[4/33] Loss: 1.3704\n",
            "[5/33] Loss: 1.3543\n",
            "[6/33] Loss: 1.3251\n",
            "[7/33] Loss: 1.3364\n",
            "[8/33] Loss: 1.3198\n",
            "[9/33] Loss: 1.3311\n",
            "[10/33] Loss: 1.3090\n",
            "[11/33] Loss: 1.3545\n",
            "[12/33] Loss: 1.3534\n",
            "[13/33] Loss: 1.3652\n",
            "[14/33] Loss: 1.3436\n",
            "[15/33] Loss: 1.3373\n",
            "[16/33] Loss: 1.2728\n",
            "[17/33] Loss: 1.3728\n",
            "[18/33] Loss: 1.3415\n",
            "[19/33] Loss: 1.3139\n",
            "[20/33] Loss: 1.2849\n",
            "[21/33] Loss: 1.2782\n",
            "[22/33] Loss: 1.3410\n",
            "[23/33] Loss: 1.2799\n",
            "[24/33] Loss: 1.3165\n",
            "[25/33] Loss: 1.3449\n",
            "[26/33] Loss: 1.3282\n",
            "[27/33] Loss: 1.3365\n",
            "[28/33] Loss: 1.3398\n",
            "[29/33] Loss: 1.3410\n",
            "[30/33] Loss: 1.3039\n",
            "[31/33] Loss: 1.3224\n",
            "[32/33] Loss: 1.2880\n",
            "[33/33] Loss: 1.3008\n",
            "Epoch: 43 | Source Accuracy: 1.0000, Target Accuracy: 0.3071, Loss: 1.3271\n",
            "Epoch 0044 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3518\n",
            "[2/33] Loss: 1.2388\n",
            "[3/33] Loss: 1.3326\n",
            "[4/33] Loss: 1.3731\n",
            "[5/33] Loss: 1.3875\n",
            "[6/33] Loss: 1.3495\n",
            "[7/33] Loss: 1.3283\n",
            "[8/33] Loss: 1.3051\n",
            "[9/33] Loss: 1.3027\n",
            "[10/33] Loss: 1.4273\n",
            "[11/33] Loss: 1.3623\n",
            "[12/33] Loss: 1.2928\n",
            "[13/33] Loss: 1.2976\n",
            "[14/33] Loss: 1.3657\n",
            "[15/33] Loss: 1.3778\n",
            "[16/33] Loss: 1.3528\n",
            "[17/33] Loss: 1.2765\n",
            "[18/33] Loss: 1.4067\n",
            "[19/33] Loss: 1.3194\n",
            "[20/33] Loss: 1.4323\n",
            "[21/33] Loss: 1.3591\n",
            "[22/33] Loss: 1.2484\n",
            "[23/33] Loss: 1.3451\n",
            "[24/33] Loss: 1.3485\n",
            "[25/33] Loss: 1.3728\n",
            "[26/33] Loss: 1.3699\n",
            "[27/33] Loss: 1.3377\n",
            "[28/33] Loss: 1.3967\n",
            "[29/33] Loss: 1.3651\n",
            "[30/33] Loss: 1.3660\n",
            "[31/33] Loss: 1.3795\n",
            "[32/33] Loss: 1.3035\n",
            "[33/33] Loss: 1.3443\n",
            "Epoch: 44 | Source Accuracy: 0.9995, Target Accuracy: 0.3182, Loss: 1.3460\n",
            "Epoch 0045 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3548\n",
            "[2/33] Loss: 1.5242\n",
            "[3/33] Loss: 1.3061\n",
            "[4/33] Loss: 1.3221\n",
            "[5/33] Loss: 1.3554\n",
            "[6/33] Loss: 1.3600\n",
            "[7/33] Loss: 1.3411\n",
            "[8/33] Loss: 1.3198\n",
            "[9/33] Loss: 1.2967\n",
            "[10/33] Loss: 1.5136\n",
            "[11/33] Loss: 1.3139\n",
            "[12/33] Loss: 1.4436\n",
            "[13/33] Loss: 1.2681\n",
            "[14/33] Loss: 1.3373\n",
            "[15/33] Loss: 1.3937\n",
            "[16/33] Loss: 1.3413\n",
            "[17/33] Loss: 1.3549\n",
            "[18/33] Loss: 1.3207\n",
            "[19/33] Loss: 1.3606\n",
            "[20/33] Loss: 1.3977\n",
            "[21/33] Loss: 1.3122\n",
            "[22/33] Loss: 1.3984\n",
            "[23/33] Loss: 1.3075\n",
            "[24/33] Loss: 1.3420\n",
            "[25/33] Loss: 1.3815\n",
            "[26/33] Loss: 1.3704\n",
            "[27/33] Loss: 1.3562\n",
            "[28/33] Loss: 1.3507\n",
            "[29/33] Loss: 1.3479\n",
            "[30/33] Loss: 1.3852\n",
            "[31/33] Loss: 1.3210\n",
            "[32/33] Loss: 1.3784\n",
            "[33/33] Loss: 1.3217\n",
            "Epoch: 45 | Source Accuracy: 0.9985, Target Accuracy: 0.3131, Loss: 1.3575\n",
            "Epoch 0046 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3498\n",
            "[2/33] Loss: 1.3552\n",
            "[3/33] Loss: 1.3353\n",
            "[4/33] Loss: 1.3629\n",
            "[5/33] Loss: 1.3865\n",
            "[6/33] Loss: 1.3448\n",
            "[7/33] Loss: 1.3613\n",
            "[8/33] Loss: 1.3347\n",
            "[9/33] Loss: 1.3632\n",
            "[10/33] Loss: 1.3139\n",
            "[11/33] Loss: 1.3865\n",
            "[12/33] Loss: 1.3949\n",
            "[13/33] Loss: 1.3711\n",
            "[14/33] Loss: 1.3248\n",
            "[15/33] Loss: 1.3535\n",
            "[16/33] Loss: 1.3370\n",
            "[17/33] Loss: 1.3864\n",
            "[18/33] Loss: 1.3324\n",
            "[19/33] Loss: 1.3914\n",
            "[20/33] Loss: 1.3087\n",
            "[21/33] Loss: 1.3482\n",
            "[22/33] Loss: 1.3921\n",
            "[23/33] Loss: 1.3440\n",
            "[24/33] Loss: 1.3002\n",
            "[25/33] Loss: 1.3663\n",
            "[26/33] Loss: 1.3554\n",
            "[27/33] Loss: 1.3595\n",
            "[28/33] Loss: 1.3071\n",
            "[29/33] Loss: 1.3630\n",
            "[30/33] Loss: 1.3617\n",
            "[31/33] Loss: 1.3666\n",
            "[32/33] Loss: 1.3851\n",
            "[33/33] Loss: 1.3490\n",
            "Epoch: 46 | Source Accuracy: 0.9995, Target Accuracy: 0.3157, Loss: 1.3543\n",
            "Epoch 0047 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.4154\n",
            "[2/33] Loss: 1.3264\n",
            "[3/33] Loss: 1.3809\n",
            "[4/33] Loss: 1.3496\n",
            "[5/33] Loss: 1.3814\n",
            "[6/33] Loss: 1.3590\n",
            "[7/33] Loss: 1.4432\n",
            "[8/33] Loss: 1.3975\n",
            "[9/33] Loss: 1.3468\n",
            "[10/33] Loss: 1.6328\n",
            "[11/33] Loss: 1.4076\n",
            "[12/33] Loss: 1.3129\n",
            "[13/33] Loss: 1.3573\n",
            "[14/33] Loss: 1.3284\n",
            "[15/33] Loss: 1.3319\n",
            "[16/33] Loss: 1.3981\n",
            "[17/33] Loss: 1.4258\n",
            "[18/33] Loss: 1.3840\n",
            "[19/33] Loss: 1.3534\n",
            "[20/33] Loss: 1.4042\n",
            "[21/33] Loss: 1.3902\n",
            "[22/33] Loss: 1.3278\n",
            "[23/33] Loss: 1.3712\n",
            "[24/33] Loss: 1.3090\n",
            "[25/33] Loss: 1.2996\n",
            "[26/33] Loss: 1.4146\n",
            "[27/33] Loss: 1.4608\n",
            "[28/33] Loss: 1.3808\n",
            "[29/33] Loss: 1.3651\n",
            "[30/33] Loss: 1.4002\n",
            "[31/33] Loss: 1.4034\n",
            "[32/33] Loss: 1.3584\n",
            "[33/33] Loss: 1.3807\n",
            "Epoch: 47 | Source Accuracy: 0.9995, Target Accuracy: 0.3167, Loss: 1.3818\n",
            "Epoch 0048 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3453\n",
            "[2/33] Loss: 1.3769\n",
            "[3/33] Loss: 1.3119\n",
            "[4/33] Loss: 1.3845\n",
            "[5/33] Loss: 1.3408\n",
            "[6/33] Loss: 1.3510\n",
            "[7/33] Loss: 1.3740\n",
            "[8/33] Loss: 1.3530\n",
            "[9/33] Loss: 1.4008\n",
            "[10/33] Loss: 1.4546\n",
            "[11/33] Loss: 1.3523\n",
            "[12/33] Loss: 1.3836\n",
            "[13/33] Loss: 1.3442\n",
            "[14/33] Loss: 1.3981\n",
            "[15/33] Loss: 1.3090\n",
            "[16/33] Loss: 1.3123\n",
            "[17/33] Loss: 1.3900\n",
            "[18/33] Loss: 1.3801\n",
            "[19/33] Loss: 1.3887\n",
            "[20/33] Loss: 1.4667\n",
            "[21/33] Loss: 1.3757\n",
            "[22/33] Loss: 1.3367\n",
            "[23/33] Loss: 1.3303\n",
            "[24/33] Loss: 1.3658\n",
            "[25/33] Loss: 1.3982\n",
            "[26/33] Loss: 1.3427\n",
            "[27/33] Loss: 1.3865\n",
            "[28/33] Loss: 1.3633\n",
            "[29/33] Loss: 1.3795\n",
            "[30/33] Loss: 1.4096\n",
            "[31/33] Loss: 1.3806\n",
            "[32/33] Loss: 1.3790\n",
            "[33/33] Loss: 1.3293\n",
            "Epoch: 48 | Source Accuracy: 0.9990, Target Accuracy: 0.3111, Loss: 1.3696\n",
            "Epoch 0049 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3415\n",
            "[2/33] Loss: 1.3386\n",
            "[3/33] Loss: 1.3560\n",
            "[4/33] Loss: 1.3589\n",
            "[5/33] Loss: 1.4084\n",
            "[6/33] Loss: 1.3642\n",
            "[7/33] Loss: 1.3761\n",
            "[8/33] Loss: 1.4083\n",
            "[9/33] Loss: 1.3328\n",
            "[10/33] Loss: 1.4385\n",
            "[11/33] Loss: 1.3127\n",
            "[12/33] Loss: 1.3474\n",
            "[13/33] Loss: 1.3838\n",
            "[14/33] Loss: 1.3785\n",
            "[15/33] Loss: 1.3623\n",
            "[16/33] Loss: 1.3499\n",
            "[17/33] Loss: 1.3387\n",
            "[18/33] Loss: 1.3736\n",
            "[19/33] Loss: 1.3417\n",
            "[20/33] Loss: 1.3597\n",
            "[21/33] Loss: 1.3498\n",
            "[22/33] Loss: 1.3358\n",
            "[23/33] Loss: 1.3802\n",
            "[24/33] Loss: 1.3682\n",
            "[25/33] Loss: 1.3808\n",
            "[26/33] Loss: 1.3669\n",
            "[27/33] Loss: 1.3686\n",
            "[28/33] Loss: 1.3742\n",
            "[29/33] Loss: 1.2996\n",
            "[30/33] Loss: 1.3548\n",
            "[31/33] Loss: 1.3298\n",
            "[32/33] Loss: 1.3164\n",
            "[33/33] Loss: 1.3499\n",
            "Epoch: 49 | Source Accuracy: 1.0000, Target Accuracy: 0.3152, Loss: 1.3590\n",
            "Epoch 0050 / 0050\n",
            "============\n",
            "[1/33] Loss: 1.3823\n",
            "[2/33] Loss: 1.3974\n",
            "[3/33] Loss: 1.3567\n",
            "[4/33] Loss: 1.3524\n",
            "[5/33] Loss: 1.3936\n",
            "[6/33] Loss: 1.3263\n",
            "[7/33] Loss: 1.3840\n",
            "[8/33] Loss: 1.3705\n",
            "[9/33] Loss: 1.4752\n",
            "[10/33] Loss: 1.2973\n",
            "[11/33] Loss: 1.3821\n",
            "[12/33] Loss: 1.3953\n",
            "[13/33] Loss: 1.3459\n",
            "[14/33] Loss: 1.3635\n",
            "[15/33] Loss: 1.3787\n",
            "[16/33] Loss: 1.3096\n",
            "[17/33] Loss: 1.3954\n",
            "[18/33] Loss: 1.3723\n",
            "[19/33] Loss: 1.4033\n",
            "[20/33] Loss: 1.2402\n",
            "[21/33] Loss: 1.3856\n",
            "[22/33] Loss: 1.3739\n",
            "[23/33] Loss: 1.3732\n",
            "[24/33] Loss: 1.3503\n",
            "[25/33] Loss: 1.3699\n",
            "[26/33] Loss: 1.3558\n",
            "[27/33] Loss: 1.3741\n",
            "[28/33] Loss: 1.3963\n",
            "[29/33] Loss: 1.4312\n",
            "[30/33] Loss: 1.3013\n",
            "[31/33] Loss: 1.3546\n",
            "[32/33] Loss: 1.3552\n",
            "[33/33] Loss: 1.3745\n",
            "Epoch: 50 | Source Accuracy: 0.9980, Target Accuracy: 0.3030, Loss: 1.3672\n",
            "Training logs saved to LOG_DANN_webcam_to_amazon\n",
            "Epoch 0001 / 0050\n",
            "============\n",
            "[1/10] Loss: 3.4508\n",
            "[2/10] Loss: 3.4658\n",
            "[3/10] Loss: 3.5126\n",
            "[4/10] Loss: 3.4867\n",
            "[5/10] Loss: 3.5091\n",
            "[6/10] Loss: 3.5183\n",
            "[7/10] Loss: 3.4859\n",
            "[8/10] Loss: 3.5110\n",
            "[9/10] Loss: 3.4945\n",
            "[10/10] Loss: 3.5203\n",
            "Epoch: 1 | Source Accuracy: 0.0239, Target Accuracy: 0.0513, Loss: 3.4955\n",
            "Epoch 0002 / 0050\n",
            "============\n",
            "[1/10] Loss: 3.5262\n",
            "[2/10] Loss: 3.4931\n",
            "[3/10] Loss: 3.5063\n",
            "[4/10] Loss: 3.5248\n",
            "[5/10] Loss: 3.5223\n",
            "[6/10] Loss: 3.5242\n",
            "[7/10] Loss: 3.5164\n",
            "[8/10] Loss: 3.5261\n",
            "[9/10] Loss: 3.5115\n",
            "[10/10] Loss: 3.5453\n",
            "Epoch: 2 | Source Accuracy: 0.1932, Target Accuracy: 0.1179, Loss: 3.5196\n",
            "Epoch 0003 / 0050\n",
            "============\n",
            "[1/10] Loss: 3.5192\n",
            "[2/10] Loss: 3.5277\n",
            "[3/10] Loss: 3.5385\n",
            "[4/10] Loss: 3.5140\n",
            "[5/10] Loss: 3.4834\n",
            "[6/10] Loss: 3.4567\n",
            "[7/10] Loss: 3.5046\n",
            "[8/10] Loss: 3.4757\n",
            "[9/10] Loss: 3.4602\n",
            "[10/10] Loss: 3.4907\n",
            "Epoch: 3 | Source Accuracy: 0.3915, Target Accuracy: 0.2940, Loss: 3.4971\n",
            "Epoch 0004 / 0050\n",
            "============\n",
            "[1/10] Loss: 3.4682\n",
            "[2/10] Loss: 3.4183\n",
            "[3/10] Loss: 3.3923\n",
            "[4/10] Loss: 3.3661\n",
            "[5/10] Loss: 3.2596\n",
            "[6/10] Loss: 3.2905\n",
            "[7/10] Loss: 3.3406\n",
            "[8/10] Loss: 3.2148\n",
            "[9/10] Loss: 3.2536\n",
            "[10/10] Loss: 3.3990\n",
            "Epoch: 4 | Source Accuracy: 0.4632, Target Accuracy: 0.3368, Loss: 3.3403\n",
            "Epoch 0005 / 0050\n",
            "============\n",
            "[1/10] Loss: 3.0873\n",
            "[2/10] Loss: 3.0330\n",
            "[3/10] Loss: 3.0686\n",
            "[4/10] Loss: 2.9414\n",
            "[5/10] Loss: 3.0496\n",
            "[6/10] Loss: 3.0319\n",
            "[7/10] Loss: 2.7574\n",
            "[8/10] Loss: 2.9201\n",
            "[9/10] Loss: 2.7341\n",
            "[10/10] Loss: 2.7806\n",
            "Epoch: 5 | Source Accuracy: 0.5077, Target Accuracy: 0.3521, Loss: 2.9404\n",
            "Epoch 0006 / 0050\n",
            "============\n",
            "[1/10] Loss: 2.5812\n",
            "[2/10] Loss: 2.5766\n",
            "[3/10] Loss: 2.3682\n",
            "[4/10] Loss: 2.4515\n",
            "[5/10] Loss: 2.3034\n",
            "[6/10] Loss: 2.3123\n",
            "[7/10] Loss: 2.1818\n",
            "[8/10] Loss: 2.0284\n",
            "[9/10] Loss: 2.0791\n",
            "[10/10] Loss: 2.3610\n",
            "Epoch: 6 | Source Accuracy: 0.7197, Target Accuracy: 0.5128, Loss: 2.3244\n",
            "Epoch 0007 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.7715\n",
            "[2/10] Loss: 1.7610\n",
            "[3/10] Loss: 1.6685\n",
            "[4/10] Loss: 1.6533\n",
            "[5/10] Loss: 1.5159\n",
            "[6/10] Loss: 1.5651\n",
            "[7/10] Loss: 1.4326\n",
            "[8/10] Loss: 1.5852\n",
            "[9/10] Loss: 1.4560\n",
            "[10/10] Loss: 1.3989\n",
            "Epoch: 7 | Source Accuracy: 0.8479, Target Accuracy: 0.6650, Loss: 1.5808\n",
            "Epoch 0008 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.2141\n",
            "[2/10] Loss: 1.3373\n",
            "[3/10] Loss: 1.1327\n",
            "[4/10] Loss: 1.1861\n",
            "[5/10] Loss: 1.2527\n",
            "[6/10] Loss: 1.2296\n",
            "[7/10] Loss: 1.0738\n",
            "[8/10] Loss: 1.0875\n",
            "[9/10] Loss: 1.1539\n",
            "[10/10] Loss: 1.0291\n",
            "Epoch: 8 | Source Accuracy: 0.9333, Target Accuracy: 0.8034, Loss: 1.1697\n",
            "Epoch 0009 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.1527\n",
            "[2/10] Loss: 1.0560\n",
            "[3/10] Loss: 1.0587\n",
            "[4/10] Loss: 1.0398\n",
            "[5/10] Loss: 1.2131\n",
            "[6/10] Loss: 1.1212\n",
            "[7/10] Loss: 1.1803\n",
            "[8/10] Loss: 1.2794\n",
            "[9/10] Loss: 1.1019\n",
            "[10/10] Loss: 1.1193\n",
            "Epoch: 9 | Source Accuracy: 0.9795, Target Accuracy: 0.8205, Loss: 1.1322\n",
            "Epoch 0010 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.1989\n",
            "[2/10] Loss: 1.2047\n",
            "[3/10] Loss: 1.3461\n",
            "[4/10] Loss: 1.2343\n",
            "[5/10] Loss: 1.2052\n",
            "[6/10] Loss: 1.3497\n",
            "[7/10] Loss: 1.2246\n",
            "[8/10] Loss: 1.3582\n",
            "[9/10] Loss: 1.3916\n",
            "[10/10] Loss: 1.2538\n",
            "Epoch: 10 | Source Accuracy: 0.9829, Target Accuracy: 0.7538, Loss: 1.2767\n",
            "Epoch 0011 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.2926\n",
            "[2/10] Loss: 1.2918\n",
            "[3/10] Loss: 1.3479\n",
            "[4/10] Loss: 1.3919\n",
            "[5/10] Loss: 1.4053\n",
            "[6/10] Loss: 1.3080\n",
            "[7/10] Loss: 1.3153\n",
            "[8/10] Loss: 1.2214\n",
            "[9/10] Loss: 1.1668\n",
            "[10/10] Loss: 1.2933\n",
            "Epoch: 11 | Source Accuracy: 0.9744, Target Accuracy: 0.6684, Loss: 1.3034\n",
            "Epoch 0012 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.1369\n",
            "[2/10] Loss: 1.2117\n",
            "[3/10] Loss: 1.2244\n",
            "[4/10] Loss: 1.1634\n",
            "[5/10] Loss: 1.1148\n",
            "[6/10] Loss: 1.1390\n",
            "[7/10] Loss: 1.2004\n",
            "[8/10] Loss: 1.2748\n",
            "[9/10] Loss: 1.1904\n",
            "[10/10] Loss: 1.3240\n",
            "Epoch: 12 | Source Accuracy: 0.9829, Target Accuracy: 0.7658, Loss: 1.1980\n",
            "Epoch 0013 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.2342\n",
            "[2/10] Loss: 1.3927\n",
            "[3/10] Loss: 1.3962\n",
            "[4/10] Loss: 1.3542\n",
            "[5/10] Loss: 1.5203\n",
            "[6/10] Loss: 1.2694\n",
            "[7/10] Loss: 1.2899\n",
            "[8/10] Loss: 1.3616\n",
            "[9/10] Loss: 1.3511\n",
            "[10/10] Loss: 1.4585\n",
            "Epoch: 13 | Source Accuracy: 0.9744, Target Accuracy: 0.8000, Loss: 1.3628\n",
            "Epoch 0014 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3957\n",
            "[2/10] Loss: 1.3391\n",
            "[3/10] Loss: 1.3810\n",
            "[4/10] Loss: 1.4524\n",
            "[5/10] Loss: 1.4944\n",
            "[6/10] Loss: 1.3249\n",
            "[7/10] Loss: 1.2550\n",
            "[8/10] Loss: 1.4210\n",
            "[9/10] Loss: 1.4444\n",
            "[10/10] Loss: 1.8303\n",
            "Epoch: 14 | Source Accuracy: 0.9556, Target Accuracy: 0.7880, Loss: 1.4338\n",
            "Epoch 0015 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.2996\n",
            "[2/10] Loss: 1.2800\n",
            "[3/10] Loss: 1.5592\n",
            "[4/10] Loss: 1.5596\n",
            "[5/10] Loss: 1.3615\n",
            "[6/10] Loss: 1.5137\n",
            "[7/10] Loss: 1.6599\n",
            "[8/10] Loss: 1.4433\n",
            "[9/10] Loss: 1.3508\n",
            "[10/10] Loss: 1.2013\n",
            "Epoch: 15 | Source Accuracy: 0.9453, Target Accuracy: 0.8274, Loss: 1.4229\n",
            "Epoch 0016 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3220\n",
            "[2/10] Loss: 1.5482\n",
            "[3/10] Loss: 1.2423\n",
            "[4/10] Loss: 1.2748\n",
            "[5/10] Loss: 1.4918\n",
            "[6/10] Loss: 1.5485\n",
            "[7/10] Loss: 1.3628\n",
            "[8/10] Loss: 1.2393\n",
            "[9/10] Loss: 1.3066\n",
            "[10/10] Loss: 1.4033\n",
            "Epoch: 16 | Source Accuracy: 0.9709, Target Accuracy: 0.7846, Loss: 1.3740\n",
            "Epoch 0017 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.6880\n",
            "[2/10] Loss: 1.2731\n",
            "[3/10] Loss: 1.2576\n",
            "[4/10] Loss: 1.3074\n",
            "[5/10] Loss: 1.3694\n",
            "[6/10] Loss: 1.4172\n",
            "[7/10] Loss: 1.3354\n",
            "[8/10] Loss: 1.4131\n",
            "[9/10] Loss: 1.3111\n",
            "[10/10] Loss: 1.5238\n",
            "Epoch: 17 | Source Accuracy: 0.9795, Target Accuracy: 0.7744, Loss: 1.3896\n",
            "Epoch 0018 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3031\n",
            "[2/10] Loss: 1.3297\n",
            "[3/10] Loss: 1.3030\n",
            "[4/10] Loss: 1.2321\n",
            "[5/10] Loss: 1.2398\n",
            "[6/10] Loss: 1.5752\n",
            "[7/10] Loss: 1.4503\n",
            "[8/10] Loss: 1.1937\n",
            "[9/10] Loss: 1.3283\n",
            "[10/10] Loss: 1.1881\n",
            "Epoch: 18 | Source Accuracy: 0.9812, Target Accuracy: 0.7573, Loss: 1.3143\n",
            "Epoch 0019 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3876\n",
            "[2/10] Loss: 1.3192\n",
            "[3/10] Loss: 1.4378\n",
            "[4/10] Loss: 1.4140\n",
            "[5/10] Loss: 1.3774\n",
            "[6/10] Loss: 1.3994\n",
            "[7/10] Loss: 1.3239\n",
            "[8/10] Loss: 1.4447\n",
            "[9/10] Loss: 1.5043\n",
            "[10/10] Loss: 1.8518\n",
            "Epoch: 19 | Source Accuracy: 0.9641, Target Accuracy: 0.7761, Loss: 1.4460\n",
            "Epoch 0020 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.5709\n",
            "[2/10] Loss: 1.6050\n",
            "[3/10] Loss: 1.4381\n",
            "[4/10] Loss: 1.5859\n",
            "[5/10] Loss: 1.6433\n",
            "[6/10] Loss: 1.6654\n",
            "[7/10] Loss: 1.4297\n",
            "[8/10] Loss: 1.6620\n",
            "[9/10] Loss: 1.3644\n",
            "[10/10] Loss: 1.4058\n",
            "Epoch: 20 | Source Accuracy: 0.9590, Target Accuracy: 0.7675, Loss: 1.5370\n",
            "Epoch 0021 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3258\n",
            "[2/10] Loss: 1.3708\n",
            "[3/10] Loss: 1.2624\n",
            "[4/10] Loss: 1.2891\n",
            "[5/10] Loss: 1.3443\n",
            "[6/10] Loss: 1.3828\n",
            "[7/10] Loss: 1.3159\n",
            "[8/10] Loss: 1.4153\n",
            "[9/10] Loss: 1.2990\n",
            "[10/10] Loss: 1.3073\n",
            "Epoch: 21 | Source Accuracy: 0.9915, Target Accuracy: 0.8085, Loss: 1.3313\n",
            "Epoch 0022 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.4479\n",
            "[2/10] Loss: 1.2742\n",
            "[3/10] Loss: 1.4641\n",
            "[4/10] Loss: 1.3462\n",
            "[5/10] Loss: 1.3918\n",
            "[6/10] Loss: 1.3673\n",
            "[7/10] Loss: 1.4334\n",
            "[8/10] Loss: 1.5912\n",
            "[9/10] Loss: 1.4837\n",
            "[10/10] Loss: 1.4105\n",
            "Epoch: 22 | Source Accuracy: 0.9915, Target Accuracy: 0.8154, Loss: 1.4210\n",
            "Epoch 0023 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.5002\n",
            "[2/10] Loss: 1.4511\n",
            "[3/10] Loss: 1.3832\n",
            "[4/10] Loss: 1.4760\n",
            "[5/10] Loss: 1.5778\n",
            "[6/10] Loss: 1.4024\n",
            "[7/10] Loss: 1.4791\n",
            "[8/10] Loss: 1.5120\n",
            "[9/10] Loss: 1.3633\n",
            "[10/10] Loss: 1.6977\n",
            "Epoch: 23 | Source Accuracy: 0.9829, Target Accuracy: 0.7590, Loss: 1.4843\n",
            "Epoch 0024 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.5062\n",
            "[2/10] Loss: 1.5430\n",
            "[3/10] Loss: 1.4728\n",
            "[4/10] Loss: 1.2604\n",
            "[5/10] Loss: 1.3814\n",
            "[6/10] Loss: 1.2801\n",
            "[7/10] Loss: 1.3662\n",
            "[8/10] Loss: 1.4421\n",
            "[9/10] Loss: 1.1840\n",
            "[10/10] Loss: 2.2279\n",
            "Epoch: 24 | Source Accuracy: 0.9573, Target Accuracy: 0.6838, Loss: 1.4664\n",
            "Epoch 0025 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.2876\n",
            "[2/10] Loss: 1.3193\n",
            "[3/10] Loss: 1.4029\n",
            "[4/10] Loss: 1.4645\n",
            "[5/10] Loss: 1.3543\n",
            "[6/10] Loss: 1.6014\n",
            "[7/10] Loss: 1.7815\n",
            "[8/10] Loss: 1.6572\n",
            "[9/10] Loss: 1.4222\n",
            "[10/10] Loss: 1.2268\n",
            "Epoch: 25 | Source Accuracy: 0.9538, Target Accuracy: 0.6222, Loss: 1.4518\n",
            "Epoch 0026 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.4387\n",
            "[2/10] Loss: 1.4793\n",
            "[3/10] Loss: 1.4162\n",
            "[4/10] Loss: 1.2669\n",
            "[5/10] Loss: 1.6525\n",
            "[6/10] Loss: 1.3470\n",
            "[7/10] Loss: 1.4477\n",
            "[8/10] Loss: 1.3455\n",
            "[9/10] Loss: 1.4466\n",
            "[10/10] Loss: 1.4976\n",
            "Epoch: 26 | Source Accuracy: 0.9590, Target Accuracy: 0.7111, Loss: 1.4338\n",
            "Epoch 0027 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.2308\n",
            "[2/10] Loss: 1.4077\n",
            "[3/10] Loss: 1.2479\n",
            "[4/10] Loss: 1.4562\n",
            "[5/10] Loss: 1.4162\n",
            "[6/10] Loss: 1.3628\n",
            "[7/10] Loss: 1.3222\n",
            "[8/10] Loss: 1.3842\n",
            "[9/10] Loss: 1.2572\n",
            "[10/10] Loss: 1.4623\n",
            "Epoch: 27 | Source Accuracy: 0.9863, Target Accuracy: 0.7846, Loss: 1.3548\n",
            "Epoch 0028 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3282\n",
            "[2/10] Loss: 1.4141\n",
            "[3/10] Loss: 1.3650\n",
            "[4/10] Loss: 1.4100\n",
            "[5/10] Loss: 1.4215\n",
            "[6/10] Loss: 1.3724\n",
            "[7/10] Loss: 1.3356\n",
            "[8/10] Loss: 1.4868\n",
            "[9/10] Loss: 1.4696\n",
            "[10/10] Loss: 1.4696\n",
            "Epoch: 28 | Source Accuracy: 0.9932, Target Accuracy: 0.7863, Loss: 1.4073\n",
            "Epoch 0029 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.4370\n",
            "[2/10] Loss: 1.3129\n",
            "[3/10] Loss: 1.4000\n",
            "[4/10] Loss: 1.4847\n",
            "[5/10] Loss: 1.4498\n",
            "[6/10] Loss: 1.3848\n",
            "[7/10] Loss: 1.3852\n",
            "[8/10] Loss: 1.3402\n",
            "[9/10] Loss: 1.3336\n",
            "[10/10] Loss: 1.4486\n",
            "Epoch: 29 | Source Accuracy: 0.9949, Target Accuracy: 0.7692, Loss: 1.3977\n",
            "Epoch 0030 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.4744\n",
            "[2/10] Loss: 1.2988\n",
            "[3/10] Loss: 1.3721\n",
            "[4/10] Loss: 1.2952\n",
            "[5/10] Loss: 1.2996\n",
            "[6/10] Loss: 1.3804\n",
            "[7/10] Loss: 1.3961\n",
            "[8/10] Loss: 1.2918\n",
            "[9/10] Loss: 1.3898\n",
            "[10/10] Loss: 1.3122\n",
            "Epoch: 30 | Source Accuracy: 0.9880, Target Accuracy: 0.7402, Loss: 1.3510\n",
            "Epoch 0031 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.2473\n",
            "[2/10] Loss: 1.3156\n",
            "[3/10] Loss: 1.3279\n",
            "[4/10] Loss: 1.4966\n",
            "[5/10] Loss: 1.2639\n",
            "[6/10] Loss: 1.3851\n",
            "[7/10] Loss: 1.3047\n",
            "[8/10] Loss: 1.4621\n",
            "[9/10] Loss: 1.3143\n",
            "[10/10] Loss: 1.2401\n",
            "Epoch: 31 | Source Accuracy: 0.9915, Target Accuracy: 0.7350, Loss: 1.3357\n",
            "Epoch 0032 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3606\n",
            "[2/10] Loss: 1.3949\n",
            "[3/10] Loss: 1.3125\n",
            "[4/10] Loss: 1.3808\n",
            "[5/10] Loss: 1.4210\n",
            "[6/10] Loss: 1.3808\n",
            "[7/10] Loss: 1.3166\n",
            "[8/10] Loss: 1.4147\n",
            "[9/10] Loss: 1.6718\n",
            "[10/10] Loss: 1.6324\n",
            "Epoch: 32 | Source Accuracy: 0.9880, Target Accuracy: 0.7333, Loss: 1.4286\n",
            "Epoch 0033 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.5086\n",
            "[2/10] Loss: 1.3722\n",
            "[3/10] Loss: 1.3754\n",
            "[4/10] Loss: 1.3501\n",
            "[5/10] Loss: 1.4734\n",
            "[6/10] Loss: 1.3926\n",
            "[7/10] Loss: 1.3823\n",
            "[8/10] Loss: 1.3823\n",
            "[9/10] Loss: 1.4426\n",
            "[10/10] Loss: 1.4552\n",
            "Epoch: 33 | Source Accuracy: 0.9897, Target Accuracy: 0.7641, Loss: 1.4135\n",
            "Epoch 0034 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3878\n",
            "[2/10] Loss: 1.3965\n",
            "[3/10] Loss: 1.3773\n",
            "[4/10] Loss: 1.3531\n",
            "[5/10] Loss: 1.3215\n",
            "[6/10] Loss: 1.2963\n",
            "[7/10] Loss: 1.3195\n",
            "[8/10] Loss: 1.4490\n",
            "[9/10] Loss: 1.4424\n",
            "[10/10] Loss: 1.4159\n",
            "Epoch: 34 | Source Accuracy: 0.9949, Target Accuracy: 0.7726, Loss: 1.3759\n",
            "Epoch 0035 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.4194\n",
            "[2/10] Loss: 1.3534\n",
            "[3/10] Loss: 1.3053\n",
            "[4/10] Loss: 1.4570\n",
            "[5/10] Loss: 1.4742\n",
            "[6/10] Loss: 1.3765\n",
            "[7/10] Loss: 1.3796\n",
            "[8/10] Loss: 1.3478\n",
            "[9/10] Loss: 1.3255\n",
            "[10/10] Loss: 1.5690\n",
            "Epoch: 35 | Source Accuracy: 0.9932, Target Accuracy: 0.7624, Loss: 1.4008\n",
            "Epoch 0036 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3644\n",
            "[2/10] Loss: 1.3543\n",
            "[3/10] Loss: 1.4254\n",
            "[4/10] Loss: 1.3853\n",
            "[5/10] Loss: 1.3253\n",
            "[6/10] Loss: 1.3021\n",
            "[7/10] Loss: 1.3382\n",
            "[8/10] Loss: 1.3863\n",
            "[9/10] Loss: 1.4488\n",
            "[10/10] Loss: 1.3748\n",
            "Epoch: 36 | Source Accuracy: 0.9983, Target Accuracy: 0.7692, Loss: 1.3705\n",
            "Epoch 0037 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.4154\n",
            "[2/10] Loss: 1.2518\n",
            "[3/10] Loss: 1.2969\n",
            "[4/10] Loss: 1.3904\n",
            "[5/10] Loss: 1.3903\n",
            "[6/10] Loss: 1.4234\n",
            "[7/10] Loss: 1.3707\n",
            "[8/10] Loss: 1.2644\n",
            "[9/10] Loss: 1.3592\n",
            "[10/10] Loss: 1.4937\n",
            "Epoch: 37 | Source Accuracy: 0.9915, Target Accuracy: 0.7675, Loss: 1.3656\n",
            "Epoch 0038 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.2896\n",
            "[2/10] Loss: 1.3274\n",
            "[3/10] Loss: 1.3383\n",
            "[4/10] Loss: 1.3481\n",
            "[5/10] Loss: 1.3404\n",
            "[6/10] Loss: 1.3933\n",
            "[7/10] Loss: 1.3206\n",
            "[8/10] Loss: 1.5373\n",
            "[9/10] Loss: 1.2896\n",
            "[10/10] Loss: 1.2562\n",
            "Epoch: 38 | Source Accuracy: 0.9966, Target Accuracy: 0.7573, Loss: 1.3441\n",
            "Epoch 0039 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3650\n",
            "[2/10] Loss: 1.3127\n",
            "[3/10] Loss: 1.2322\n",
            "[4/10] Loss: 1.5236\n",
            "[5/10] Loss: 1.3998\n",
            "[6/10] Loss: 1.4373\n",
            "[7/10] Loss: 1.4239\n",
            "[8/10] Loss: 1.4061\n",
            "[9/10] Loss: 1.3045\n",
            "[10/10] Loss: 1.5580\n",
            "Epoch: 39 | Source Accuracy: 0.9949, Target Accuracy: 0.7556, Loss: 1.3963\n",
            "Epoch 0040 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.2956\n",
            "[2/10] Loss: 1.5477\n",
            "[3/10] Loss: 1.4240\n",
            "[4/10] Loss: 1.4032\n",
            "[5/10] Loss: 1.3722\n",
            "[6/10] Loss: 1.4641\n",
            "[7/10] Loss: 1.3521\n",
            "[8/10] Loss: 1.4670\n",
            "[9/10] Loss: 1.5603\n",
            "[10/10] Loss: 1.3110\n",
            "Epoch: 40 | Source Accuracy: 0.9966, Target Accuracy: 0.7744, Loss: 1.4197\n",
            "Epoch 0041 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3267\n",
            "[2/10] Loss: 1.4995\n",
            "[3/10] Loss: 1.3876\n",
            "[4/10] Loss: 1.5456\n",
            "[5/10] Loss: 1.3090\n",
            "[6/10] Loss: 1.4583\n",
            "[7/10] Loss: 1.3221\n",
            "[8/10] Loss: 1.5572\n",
            "[9/10] Loss: 1.3942\n",
            "[10/10] Loss: 1.5653\n",
            "Epoch: 41 | Source Accuracy: 0.9966, Target Accuracy: 0.7846, Loss: 1.4366\n",
            "Epoch 0042 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3903\n",
            "[2/10] Loss: 1.4752\n",
            "[3/10] Loss: 1.3942\n",
            "[4/10] Loss: 1.3984\n",
            "[5/10] Loss: 1.3926\n",
            "[6/10] Loss: 1.4681\n",
            "[7/10] Loss: 1.4615\n",
            "[8/10] Loss: 1.4378\n",
            "[9/10] Loss: 1.3392\n",
            "[10/10] Loss: 1.4441\n",
            "Epoch: 42 | Source Accuracy: 0.9983, Target Accuracy: 0.7829, Loss: 1.4201\n",
            "Epoch 0043 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.4440\n",
            "[2/10] Loss: 1.3919\n",
            "[3/10] Loss: 1.3180\n",
            "[4/10] Loss: 1.4082\n",
            "[5/10] Loss: 1.4231\n",
            "[6/10] Loss: 1.3949\n",
            "[7/10] Loss: 1.3680\n",
            "[8/10] Loss: 1.4299\n",
            "[9/10] Loss: 1.2945\n",
            "[10/10] Loss: 1.2737\n",
            "Epoch: 43 | Source Accuracy: 1.0000, Target Accuracy: 0.8034, Loss: 1.3746\n",
            "Epoch 0044 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.4288\n",
            "[2/10] Loss: 1.3870\n",
            "[3/10] Loss: 1.3577\n",
            "[4/10] Loss: 1.3835\n",
            "[5/10] Loss: 1.4031\n",
            "[6/10] Loss: 1.4077\n",
            "[7/10] Loss: 1.3480\n",
            "[8/10] Loss: 1.3242\n",
            "[9/10] Loss: 1.3870\n",
            "[10/10] Loss: 1.2580\n",
            "Epoch: 44 | Source Accuracy: 0.9983, Target Accuracy: 0.7692, Loss: 1.3685\n",
            "Epoch 0045 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3789\n",
            "[2/10] Loss: 1.3615\n",
            "[3/10] Loss: 1.3927\n",
            "[4/10] Loss: 1.3254\n",
            "[5/10] Loss: 1.3997\n",
            "[6/10] Loss: 1.3401\n",
            "[7/10] Loss: 1.4226\n",
            "[8/10] Loss: 1.3458\n",
            "[9/10] Loss: 1.4089\n",
            "[10/10] Loss: 1.2277\n",
            "Epoch: 45 | Source Accuracy: 0.9983, Target Accuracy: 0.7949, Loss: 1.3603\n",
            "Epoch 0046 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3811\n",
            "[2/10] Loss: 1.2966\n",
            "[3/10] Loss: 1.4319\n",
            "[4/10] Loss: 1.3424\n",
            "[5/10] Loss: 1.3825\n",
            "[6/10] Loss: 1.3471\n",
            "[7/10] Loss: 1.3707\n",
            "[8/10] Loss: 1.3344\n",
            "[9/10] Loss: 1.5200\n",
            "[10/10] Loss: 1.2391\n",
            "Epoch: 46 | Source Accuracy: 0.9983, Target Accuracy: 0.7983, Loss: 1.3646\n",
            "Epoch 0047 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3244\n",
            "[2/10] Loss: 1.3154\n",
            "[3/10] Loss: 1.3837\n",
            "[4/10] Loss: 1.4230\n",
            "[5/10] Loss: 1.5108\n",
            "[6/10] Loss: 1.3629\n",
            "[7/10] Loss: 1.3117\n",
            "[8/10] Loss: 1.3671\n",
            "[9/10] Loss: 1.4139\n",
            "[10/10] Loss: 1.3993\n",
            "Epoch: 47 | Source Accuracy: 0.9983, Target Accuracy: 0.8051, Loss: 1.3812\n",
            "Epoch 0048 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3434\n",
            "[2/10] Loss: 1.3795\n",
            "[3/10] Loss: 1.3319\n",
            "[4/10] Loss: 1.3550\n",
            "[5/10] Loss: 1.3746\n",
            "[6/10] Loss: 1.4123\n",
            "[7/10] Loss: 1.3636\n",
            "[8/10] Loss: 1.4212\n",
            "[9/10] Loss: 1.3320\n",
            "[10/10] Loss: 1.3163\n",
            "Epoch: 48 | Source Accuracy: 1.0000, Target Accuracy: 0.8171, Loss: 1.3630\n",
            "Epoch 0049 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.2909\n",
            "[2/10] Loss: 1.3237\n",
            "[3/10] Loss: 1.4659\n",
            "[4/10] Loss: 1.4184\n",
            "[5/10] Loss: 1.3639\n",
            "[6/10] Loss: 1.4185\n",
            "[7/10] Loss: 1.2826\n",
            "[8/10] Loss: 1.3677\n",
            "[9/10] Loss: 1.3750\n",
            "[10/10] Loss: 1.6591\n",
            "Epoch: 49 | Source Accuracy: 0.9949, Target Accuracy: 0.8120, Loss: 1.3966\n",
            "Epoch 0050 / 0050\n",
            "============\n",
            "[1/10] Loss: 1.3024\n",
            "[2/10] Loss: 1.4245\n",
            "[3/10] Loss: 1.3900\n",
            "[4/10] Loss: 1.5087\n",
            "[5/10] Loss: 1.3138\n",
            "[6/10] Loss: 1.2732\n",
            "[7/10] Loss: 1.3453\n",
            "[8/10] Loss: 1.3805\n",
            "[9/10] Loss: 1.3852\n",
            "[10/10] Loss: 1.4201\n",
            "Epoch: 50 | Source Accuracy: 0.9983, Target Accuracy: 0.8034, Loss: 1.3744\n",
            "Training logs saved to LOG_DANN_webcam_to_dslr\n",
            "Source and Target are the same. Skiping...\n"
          ]
        }
      ],
      "source": [
        "summary(DANNMBNv3s().to(device), input_size=(channel_size, image_size, image_size))\n",
        "num_epochs = 50\n",
        "lr = 1e-3\n",
        "loss_fn_class = torch.nn.NLLLoss()\n",
        "loss_fn_domain = torch.nn.NLLLoss()\n",
        "srcs = [\"amazon\", \"dslr\", \"webcam\"]\n",
        "tars = [\"amazon\", \"dslr\", \"webcam\"]\n",
        "for src in srcs:\n",
        "    for tar in tars:\n",
        "        if src != tar:\n",
        "            model = DANNMBNv3s().to(device)\n",
        "            src_dataloader, tar_dataloader = get_loader_DA(src, tar)\n",
        "            optimizer = optim.Adam(model.parameters(), lr)\n",
        "            steps_per_epoch = max(len(src_dataloader), len(tar_dataloader))\n",
        "            scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "                optimizer,\n",
        "                max_lr=lr,  # Max learning rate during the cycle\n",
        "                steps_per_epoch=steps_per_epoch,\n",
        "                epochs=num_epochs,\n",
        "                anneal_strategy=\"cos\"  # Can also try 'cos' for cosine annealing\n",
        "            )\n",
        "            plot_graphDA(train_model_with_balanced_batches_DANN(model, optimizer, scheduler, loss_fn_class, loss_fn_domain, src, tar, device, num_epochs=num_epochs, batch_size=64), f\"DANN {src} -> {tar}\")\n",
        "        else: print(\"Source and Target are the same. Skiping...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5RiQqoobDJy"
      },
      "outputs": [],
      "source": [
        "zip_directory('./cp', 'DANN_Model.zip')\n",
        "zip_log_files(\"DANN_LOG.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyDpxFuDKV43"
      },
      "source": [
        "## Coral"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf7Kxfh8pGfy"
      },
      "source": [
        "### Training Loop Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUZhLZuibtmz",
        "outputId": "9caabc55-5840-4524-d952-7aa40eb6827c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
            ")\n",
            "Mod Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=31, bias=True)\n",
            ")\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 16, 112, 112]             432\n",
            "       BatchNorm2d-2         [-1, 16, 112, 112]              32\n",
            "         Hardswish-3         [-1, 16, 112, 112]               0\n",
            "            Conv2d-4           [-1, 16, 56, 56]             144\n",
            "       BatchNorm2d-5           [-1, 16, 56, 56]              32\n",
            "              ReLU-6           [-1, 16, 56, 56]               0\n",
            " AdaptiveAvgPool2d-7             [-1, 16, 1, 1]               0\n",
            "            Conv2d-8              [-1, 8, 1, 1]             136\n",
            "              ReLU-9              [-1, 8, 1, 1]               0\n",
            "           Conv2d-10             [-1, 16, 1, 1]             144\n",
            "      Hardsigmoid-11             [-1, 16, 1, 1]               0\n",
            "SqueezeExcitation-12           [-1, 16, 56, 56]               0\n",
            "           Conv2d-13           [-1, 16, 56, 56]             256\n",
            "      BatchNorm2d-14           [-1, 16, 56, 56]              32\n",
            " InvertedResidual-15           [-1, 16, 56, 56]               0\n",
            "           Conv2d-16           [-1, 72, 56, 56]           1,152\n",
            "      BatchNorm2d-17           [-1, 72, 56, 56]             144\n",
            "             ReLU-18           [-1, 72, 56, 56]               0\n",
            "           Conv2d-19           [-1, 72, 28, 28]             648\n",
            "      BatchNorm2d-20           [-1, 72, 28, 28]             144\n",
            "             ReLU-21           [-1, 72, 28, 28]               0\n",
            "           Conv2d-22           [-1, 24, 28, 28]           1,728\n",
            "      BatchNorm2d-23           [-1, 24, 28, 28]              48\n",
            " InvertedResidual-24           [-1, 24, 28, 28]               0\n",
            "           Conv2d-25           [-1, 88, 28, 28]           2,112\n",
            "      BatchNorm2d-26           [-1, 88, 28, 28]             176\n",
            "             ReLU-27           [-1, 88, 28, 28]               0\n",
            "           Conv2d-28           [-1, 88, 28, 28]             792\n",
            "      BatchNorm2d-29           [-1, 88, 28, 28]             176\n",
            "             ReLU-30           [-1, 88, 28, 28]               0\n",
            "           Conv2d-31           [-1, 24, 28, 28]           2,112\n",
            "      BatchNorm2d-32           [-1, 24, 28, 28]              48\n",
            " InvertedResidual-33           [-1, 24, 28, 28]               0\n",
            "           Conv2d-34           [-1, 96, 28, 28]           2,304\n",
            "      BatchNorm2d-35           [-1, 96, 28, 28]             192\n",
            "        Hardswish-36           [-1, 96, 28, 28]               0\n",
            "           Conv2d-37           [-1, 96, 14, 14]           2,400\n",
            "      BatchNorm2d-38           [-1, 96, 14, 14]             192\n",
            "        Hardswish-39           [-1, 96, 14, 14]               0\n",
            "AdaptiveAvgPool2d-40             [-1, 96, 1, 1]               0\n",
            "           Conv2d-41             [-1, 24, 1, 1]           2,328\n",
            "             ReLU-42             [-1, 24, 1, 1]               0\n",
            "           Conv2d-43             [-1, 96, 1, 1]           2,400\n",
            "      Hardsigmoid-44             [-1, 96, 1, 1]               0\n",
            "SqueezeExcitation-45           [-1, 96, 14, 14]               0\n",
            "           Conv2d-46           [-1, 40, 14, 14]           3,840\n",
            "      BatchNorm2d-47           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-48           [-1, 40, 14, 14]               0\n",
            "           Conv2d-49          [-1, 240, 14, 14]           9,600\n",
            "      BatchNorm2d-50          [-1, 240, 14, 14]             480\n",
            "        Hardswish-51          [-1, 240, 14, 14]               0\n",
            "           Conv2d-52          [-1, 240, 14, 14]           6,000\n",
            "      BatchNorm2d-53          [-1, 240, 14, 14]             480\n",
            "        Hardswish-54          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-55            [-1, 240, 1, 1]               0\n",
            "           Conv2d-56             [-1, 64, 1, 1]          15,424\n",
            "             ReLU-57             [-1, 64, 1, 1]               0\n",
            "           Conv2d-58            [-1, 240, 1, 1]          15,600\n",
            "      Hardsigmoid-59            [-1, 240, 1, 1]               0\n",
            "SqueezeExcitation-60          [-1, 240, 14, 14]               0\n",
            "           Conv2d-61           [-1, 40, 14, 14]           9,600\n",
            "      BatchNorm2d-62           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-63           [-1, 40, 14, 14]               0\n",
            "           Conv2d-64          [-1, 240, 14, 14]           9,600\n",
            "      BatchNorm2d-65          [-1, 240, 14, 14]             480\n",
            "        Hardswish-66          [-1, 240, 14, 14]               0\n",
            "           Conv2d-67          [-1, 240, 14, 14]           6,000\n",
            "      BatchNorm2d-68          [-1, 240, 14, 14]             480\n",
            "        Hardswish-69          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-70            [-1, 240, 1, 1]               0\n",
            "           Conv2d-71             [-1, 64, 1, 1]          15,424\n",
            "             ReLU-72             [-1, 64, 1, 1]               0\n",
            "           Conv2d-73            [-1, 240, 1, 1]          15,600\n",
            "      Hardsigmoid-74            [-1, 240, 1, 1]               0\n",
            "SqueezeExcitation-75          [-1, 240, 14, 14]               0\n",
            "           Conv2d-76           [-1, 40, 14, 14]           9,600\n",
            "      BatchNorm2d-77           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-78           [-1, 40, 14, 14]               0\n",
            "           Conv2d-79          [-1, 120, 14, 14]           4,800\n",
            "      BatchNorm2d-80          [-1, 120, 14, 14]             240\n",
            "        Hardswish-81          [-1, 120, 14, 14]               0\n",
            "           Conv2d-82          [-1, 120, 14, 14]           3,000\n",
            "      BatchNorm2d-83          [-1, 120, 14, 14]             240\n",
            "        Hardswish-84          [-1, 120, 14, 14]               0\n",
            "AdaptiveAvgPool2d-85            [-1, 120, 1, 1]               0\n",
            "           Conv2d-86             [-1, 32, 1, 1]           3,872\n",
            "             ReLU-87             [-1, 32, 1, 1]               0\n",
            "           Conv2d-88            [-1, 120, 1, 1]           3,960\n",
            "      Hardsigmoid-89            [-1, 120, 1, 1]               0\n",
            "SqueezeExcitation-90          [-1, 120, 14, 14]               0\n",
            "           Conv2d-91           [-1, 48, 14, 14]           5,760\n",
            "      BatchNorm2d-92           [-1, 48, 14, 14]              96\n",
            " InvertedResidual-93           [-1, 48, 14, 14]               0\n",
            "           Conv2d-94          [-1, 144, 14, 14]           6,912\n",
            "      BatchNorm2d-95          [-1, 144, 14, 14]             288\n",
            "        Hardswish-96          [-1, 144, 14, 14]               0\n",
            "           Conv2d-97          [-1, 144, 14, 14]           3,600\n",
            "      BatchNorm2d-98          [-1, 144, 14, 14]             288\n",
            "        Hardswish-99          [-1, 144, 14, 14]               0\n",
            "AdaptiveAvgPool2d-100            [-1, 144, 1, 1]               0\n",
            "          Conv2d-101             [-1, 40, 1, 1]           5,800\n",
            "            ReLU-102             [-1, 40, 1, 1]               0\n",
            "          Conv2d-103            [-1, 144, 1, 1]           5,904\n",
            "     Hardsigmoid-104            [-1, 144, 1, 1]               0\n",
            "SqueezeExcitation-105          [-1, 144, 14, 14]               0\n",
            "          Conv2d-106           [-1, 48, 14, 14]           6,912\n",
            "     BatchNorm2d-107           [-1, 48, 14, 14]              96\n",
            "InvertedResidual-108           [-1, 48, 14, 14]               0\n",
            "          Conv2d-109          [-1, 288, 14, 14]          13,824\n",
            "     BatchNorm2d-110          [-1, 288, 14, 14]             576\n",
            "       Hardswish-111          [-1, 288, 14, 14]               0\n",
            "          Conv2d-112            [-1, 288, 7, 7]           7,200\n",
            "     BatchNorm2d-113            [-1, 288, 7, 7]             576\n",
            "       Hardswish-114            [-1, 288, 7, 7]               0\n",
            "AdaptiveAvgPool2d-115            [-1, 288, 1, 1]               0\n",
            "          Conv2d-116             [-1, 72, 1, 1]          20,808\n",
            "            ReLU-117             [-1, 72, 1, 1]               0\n",
            "          Conv2d-118            [-1, 288, 1, 1]          21,024\n",
            "     Hardsigmoid-119            [-1, 288, 1, 1]               0\n",
            "SqueezeExcitation-120            [-1, 288, 7, 7]               0\n",
            "          Conv2d-121             [-1, 96, 7, 7]          27,648\n",
            "     BatchNorm2d-122             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-123             [-1, 96, 7, 7]               0\n",
            "          Conv2d-124            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-125            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-126            [-1, 576, 7, 7]               0\n",
            "          Conv2d-127            [-1, 576, 7, 7]          14,400\n",
            "     BatchNorm2d-128            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-129            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-130            [-1, 576, 1, 1]               0\n",
            "          Conv2d-131            [-1, 144, 1, 1]          83,088\n",
            "            ReLU-132            [-1, 144, 1, 1]               0\n",
            "          Conv2d-133            [-1, 576, 1, 1]          83,520\n",
            "     Hardsigmoid-134            [-1, 576, 1, 1]               0\n",
            "SqueezeExcitation-135            [-1, 576, 7, 7]               0\n",
            "          Conv2d-136             [-1, 96, 7, 7]          55,296\n",
            "     BatchNorm2d-137             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-138             [-1, 96, 7, 7]               0\n",
            "          Conv2d-139            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-140            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-141            [-1, 576, 7, 7]               0\n",
            "          Conv2d-142            [-1, 576, 7, 7]          14,400\n",
            "     BatchNorm2d-143            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-144            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-145            [-1, 576, 1, 1]               0\n",
            "          Conv2d-146            [-1, 144, 1, 1]          83,088\n",
            "            ReLU-147            [-1, 144, 1, 1]               0\n",
            "          Conv2d-148            [-1, 576, 1, 1]          83,520\n",
            "     Hardsigmoid-149            [-1, 576, 1, 1]               0\n",
            "SqueezeExcitation-150            [-1, 576, 7, 7]               0\n",
            "          Conv2d-151             [-1, 96, 7, 7]          55,296\n",
            "     BatchNorm2d-152             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-153             [-1, 96, 7, 7]               0\n",
            "          Conv2d-154            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-155            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-156            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-157            [-1, 576, 1, 1]               0\n",
            "          Linear-158                 [-1, 1024]         590,848\n",
            "       Hardswish-159                 [-1, 1024]               0\n",
            "         Dropout-160                 [-1, 1024]               0\n",
            "          Linear-161                   [-1, 31]          31,775\n",
            "================================================================\n",
            "Total params: 1,549,631\n",
            "Trainable params: 1,549,631\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 34.60\n",
            "Params size (MB): 5.91\n",
            "Estimated Total Size (MB): 41.09\n",
            "----------------------------------------------------------------\n",
            "Epoch 0001 / 0030\n",
            "============\n",
            "[1/33] class loss: 0.0228 CORAL_loss: 0.0000\n",
            "[2/33] class loss: -0.0371 CORAL_loss: 0.0000\n",
            "[3/33] class loss: -0.0547 CORAL_loss: 0.0000\n",
            "[4/33] class loss: -0.0922 CORAL_loss: 0.0000\n",
            "[5/33] class loss: -0.2192 CORAL_loss: 0.0000\n",
            "[6/33] class loss: -0.2095 CORAL_loss: 0.0000\n",
            "[7/33] class loss: -0.2314 CORAL_loss: 0.0000\n",
            "[8/33] class loss: -0.3427 CORAL_loss: 0.0000\n",
            "[9/33] class loss: -0.3192 CORAL_loss: 0.0000\n",
            "[10/33] class loss: -0.3656 CORAL_loss: 0.0000\n",
            "[11/33] class loss: -0.5886 CORAL_loss: 0.0000\n",
            "[12/33] class loss: -0.6287 CORAL_loss: 0.0000\n",
            "[13/33] class loss: -0.5871 CORAL_loss: 0.0000\n",
            "[14/33] class loss: -0.6304 CORAL_loss: 0.0000\n",
            "[15/33] class loss: -0.7429 CORAL_loss: 0.0000\n",
            "[16/33] class loss: -0.7495 CORAL_loss: 0.0000\n",
            "[17/33] class loss: -0.7848 CORAL_loss: 0.0000\n",
            "[18/33] class loss: -0.9215 CORAL_loss: 0.0000\n",
            "[19/33] class loss: -0.8460 CORAL_loss: 0.0000\n",
            "[20/33] class loss: -1.0308 CORAL_loss: 0.0000\n",
            "[21/33] class loss: -1.1141 CORAL_loss: 0.0000\n",
            "[22/33] class loss: -1.2044 CORAL_loss: 0.0000\n",
            "[23/33] class loss: -1.0954 CORAL_loss: 0.0000\n",
            "[24/33] class loss: -1.1818 CORAL_loss: 0.0000\n",
            "[25/33] class loss: -1.3514 CORAL_loss: 0.0000\n",
            "[26/33] class loss: -1.3302 CORAL_loss: 0.0000\n",
            "[27/33] class loss: -1.3702 CORAL_loss: 0.0000\n",
            "[28/33] class loss: -1.5022 CORAL_loss: 0.0000\n",
            "[29/33] class loss: -1.5096 CORAL_loss: 0.0000\n",
            "[30/33] class loss: -1.9148 CORAL_loss: 0.0000\n",
            "[31/33] class loss: -1.7117 CORAL_loss: 0.0000\n",
            "[32/33] class loss: -1.8702 CORAL_loss: 0.0000\n",
            "[33/33] class loss: -1.6993 CORAL_loss: 0.0000\n",
            "Epoch: 1 || Train_src_acc: 0.4136, Train_tar_acc: 0.1061, Train_loss: -0.8550\n",
            "Epoch 0002 / 0030\n",
            "============\n",
            "[1/33] class loss: -1.9460 CORAL_loss: 0.0000\n",
            "[2/33] class loss: -2.0649 CORAL_loss: 0.0000\n",
            "[3/33] class loss: -2.0772 CORAL_loss: 0.0000\n",
            "[4/33] class loss: -2.1156 CORAL_loss: 0.0000\n",
            "[5/33] class loss: -2.4043 CORAL_loss: 0.0000\n",
            "[6/33] class loss: -2.3185 CORAL_loss: 0.0000\n",
            "[7/33] class loss: -2.5326 CORAL_loss: 0.0000\n",
            "[8/33] class loss: -2.4128 CORAL_loss: 0.0000\n",
            "[9/33] class loss: -2.4118 CORAL_loss: 0.0000\n",
            "[10/33] class loss: -2.5050 CORAL_loss: 0.0000\n",
            "[11/33] class loss: -2.9349 CORAL_loss: 0.0000\n",
            "[12/33] class loss: -3.0346 CORAL_loss: 0.0000\n",
            "[13/33] class loss: -3.1251 CORAL_loss: 0.0000\n",
            "[14/33] class loss: -3.0967 CORAL_loss: 0.0000\n",
            "[15/33] class loss: -3.5866 CORAL_loss: 0.0000\n",
            "[16/33] class loss: -3.4095 CORAL_loss: 0.0000\n",
            "[17/33] class loss: -3.7945 CORAL_loss: 0.0000\n",
            "[18/33] class loss: -3.4493 CORAL_loss: 0.0000\n",
            "[19/33] class loss: -3.5280 CORAL_loss: 0.0000\n",
            "[20/33] class loss: -3.9304 CORAL_loss: 0.0001\n",
            "[21/33] class loss: -4.1841 CORAL_loss: 0.0000\n",
            "[22/33] class loss: -4.4363 CORAL_loss: 0.0001\n",
            "[23/33] class loss: -4.4788 CORAL_loss: 0.0001\n",
            "[24/33] class loss: -4.5302 CORAL_loss: 0.0001\n",
            "[25/33] class loss: -5.1471 CORAL_loss: 0.0001\n",
            "[26/33] class loss: -4.8587 CORAL_loss: 0.0001\n",
            "[27/33] class loss: -5.4717 CORAL_loss: 0.0001\n",
            "[28/33] class loss: -4.9050 CORAL_loss: 0.0001\n",
            "[29/33] class loss: -4.9805 CORAL_loss: 0.0001\n",
            "[30/33] class loss: -5.8114 CORAL_loss: 0.0004\n",
            "[31/33] class loss: -5.9126 CORAL_loss: 0.0003\n",
            "[32/33] class loss: -6.2377 CORAL_loss: 0.0002\n",
            "[33/33] class loss: -6.4313 CORAL_loss: 0.0004\n",
            "Epoch: 2 || Train_src_acc: 0.7096, Train_tar_acc: 0.2197, Train_loss: -3.7594\n",
            "Epoch 0003 / 0030\n",
            "============\n",
            "[1/33] class loss: -6.7591 CORAL_loss: 0.0004\n",
            "[2/33] class loss: -6.3691 CORAL_loss: 0.0003\n",
            "[3/33] class loss: -6.8322 CORAL_loss: 0.0007\n",
            "[4/33] class loss: -7.2111 CORAL_loss: 0.0005\n",
            "[5/33] class loss: -7.4440 CORAL_loss: 0.0012\n",
            "[6/33] class loss: -7.8293 CORAL_loss: 0.0010\n",
            "[7/33] class loss: -8.6763 CORAL_loss: 0.0020\n",
            "[8/33] class loss: -8.0891 CORAL_loss: 0.0017\n",
            "[9/33] class loss: -8.4044 CORAL_loss: 0.0018\n",
            "[10/33] class loss: -6.8838 CORAL_loss: 0.0010\n",
            "[11/33] class loss: -9.5223 CORAL_loss: 0.0022\n",
            "[12/33] class loss: -8.8248 CORAL_loss: 0.0013\n",
            "[13/33] class loss: -9.5585 CORAL_loss: 0.0023\n",
            "[14/33] class loss: -10.0490 CORAL_loss: 0.0031\n",
            "[15/33] class loss: -10.3586 CORAL_loss: 0.0063\n",
            "[16/33] class loss: -10.7963 CORAL_loss: 0.0067\n",
            "[17/33] class loss: -11.8601 CORAL_loss: 0.0142\n",
            "[18/33] class loss: -10.9164 CORAL_loss: 0.0079\n",
            "[19/33] class loss: -11.5165 CORAL_loss: 0.0147\n",
            "[20/33] class loss: -9.8106 CORAL_loss: 0.0081\n",
            "[21/33] class loss: -13.0123 CORAL_loss: 0.0113\n",
            "[22/33] class loss: -12.0071 CORAL_loss: 0.0074\n",
            "[23/33] class loss: -12.6126 CORAL_loss: 0.0208\n",
            "[24/33] class loss: -13.5325 CORAL_loss: 0.0184\n",
            "[25/33] class loss: -14.0955 CORAL_loss: 0.0307\n",
            "[26/33] class loss: -14.8667 CORAL_loss: 0.0567\n",
            "[27/33] class loss: -15.9772 CORAL_loss: 0.0715\n",
            "[28/33] class loss: -14.6782 CORAL_loss: 0.0247\n",
            "[29/33] class loss: -15.6957 CORAL_loss: 0.0883\n",
            "[30/33] class loss: -13.8433 CORAL_loss: 0.0727\n",
            "[31/33] class loss: -17.6437 CORAL_loss: 0.0830\n",
            "[32/33] class loss: -15.7316 CORAL_loss: 0.0332\n",
            "[33/33] class loss: -16.8329 CORAL_loss: 0.1265\n",
            "Epoch: 3 || Train_src_acc: 0.6758, Train_tar_acc: 0.2222, Train_loss: -11.1369\n",
            "Epoch 0004 / 0030\n",
            "============\n",
            "[1/33] class loss: -18.6201 CORAL_loss: 0.0708\n",
            "[2/33] class loss: -17.7188 CORAL_loss: 0.1371\n",
            "[3/33] class loss: -19.7925 CORAL_loss: 0.2369\n",
            "[4/33] class loss: -19.3275 CORAL_loss: 0.1603\n",
            "[5/33] class loss: -19.3610 CORAL_loss: 0.2215\n",
            "[6/33] class loss: -22.5908 CORAL_loss: 0.1487\n",
            "[7/33] class loss: -19.3986 CORAL_loss: 0.2217\n",
            "[8/33] class loss: -21.3996 CORAL_loss: 0.4617\n",
            "[9/33] class loss: -22.0290 CORAL_loss: 0.4954\n",
            "[10/33] class loss: -23.7338 CORAL_loss: 0.0691\n",
            "[11/33] class loss: -24.5642 CORAL_loss: 0.1242\n",
            "[12/33] class loss: -23.1853 CORAL_loss: 0.3577\n",
            "[13/33] class loss: -25.8360 CORAL_loss: 0.4921\n",
            "[14/33] class loss: -25.2431 CORAL_loss: 0.3803\n",
            "[15/33] class loss: -25.3974 CORAL_loss: 0.5134\n",
            "[16/33] class loss: -29.4493 CORAL_loss: 0.1567\n",
            "[17/33] class loss: -25.4193 CORAL_loss: 0.3178\n",
            "[18/33] class loss: -28.1037 CORAL_loss: 0.6888\n",
            "[19/33] class loss: -28.8183 CORAL_loss: 0.2336\n",
            "[20/33] class loss: -31.3184 CORAL_loss: 0.2158\n",
            "[21/33] class loss: -31.8006 CORAL_loss: 0.3110\n",
            "[22/33] class loss: -29.6826 CORAL_loss: 0.2481\n",
            "[23/33] class loss: -33.8920 CORAL_loss: 1.5091\n",
            "[24/33] class loss: -33.0026 CORAL_loss: 0.7576\n",
            "[25/33] class loss: -32.2669 CORAL_loss: 1.2047\n",
            "[26/33] class loss: -37.9322 CORAL_loss: 0.8364\n",
            "[27/33] class loss: -32.7972 CORAL_loss: 0.7287\n",
            "[28/33] class loss: -35.9763 CORAL_loss: 0.7763\n",
            "[29/33] class loss: -36.6174 CORAL_loss: 0.6503\n",
            "[30/33] class loss: -40.4863 CORAL_loss: 0.4304\n",
            "[31/33] class loss: -40.0919 CORAL_loss: 0.6247\n",
            "[32/33] class loss: -37.7939 CORAL_loss: 0.3267\n",
            "[33/33] class loss: -42.6200 CORAL_loss: 1.5988\n",
            "Epoch: 4 || Train_src_acc: 0.5823, Train_tar_acc: 0.1803, Train_loss: -27.8958\n",
            "Epoch 0005 / 0030\n",
            "============\n",
            "[1/33] class loss: -42.3303 CORAL_loss: 1.1321\n",
            "[2/33] class loss: -39.9000 CORAL_loss: 1.1247\n",
            "[3/33] class loss: -45.1230 CORAL_loss: 0.5935\n",
            "[4/33] class loss: -46.0417 CORAL_loss: 0.5169\n",
            "[5/33] class loss: -47.0230 CORAL_loss: 6.0764\n",
            "[6/33] class loss: -45.1733 CORAL_loss: 3.3308\n",
            "[7/33] class loss: -46.8785 CORAL_loss: 1.0204\n",
            "[8/33] class loss: -47.9780 CORAL_loss: 2.6781\n",
            "[9/33] class loss: -41.8651 CORAL_loss: 4.8177\n",
            "[10/33] class loss: -46.1046 CORAL_loss: 12.4310\n",
            "[11/33] class loss: -51.7932 CORAL_loss: 3.8981\n",
            "[12/33] class loss: -48.3830 CORAL_loss: 2.7724\n",
            "[13/33] class loss: -54.0958 CORAL_loss: 3.3843\n",
            "[14/33] class loss: -54.0720 CORAL_loss: 2.5425\n",
            "[15/33] class loss: -55.2798 CORAL_loss: 2.8751\n",
            "[16/33] class loss: -51.6813 CORAL_loss: 1.4535\n",
            "[17/33] class loss: -54.9885 CORAL_loss: 2.7625\n",
            "[18/33] class loss: -54.6684 CORAL_loss: 3.5172\n",
            "[19/33] class loss: -46.1864 CORAL_loss: 3.2475\n",
            "[20/33] class loss: -54.9852 CORAL_loss: 34.9088\n",
            "[21/33] class loss: -58.9052 CORAL_loss: 2.0108\n",
            "[22/33] class loss: -53.9867 CORAL_loss: 4.5059\n",
            "[23/33] class loss: -60.0248 CORAL_loss: 0.9668\n",
            "[24/33] class loss: -59.9763 CORAL_loss: 1.0665\n",
            "[25/33] class loss: -58.5808 CORAL_loss: 0.9104\n",
            "[26/33] class loss: -57.0119 CORAL_loss: 2.7371\n",
            "[27/33] class loss: -59.9404 CORAL_loss: 3.0774\n",
            "[28/33] class loss: -59.7533 CORAL_loss: 2.9828\n",
            "[29/33] class loss: -47.3675 CORAL_loss: 3.3097\n",
            "[30/33] class loss: -54.2372 CORAL_loss: 16.3287\n",
            "[31/33] class loss: -63.4994 CORAL_loss: 12.2220\n",
            "[32/33] class loss: -57.6511 CORAL_loss: 0.7479\n",
            "[33/33] class loss: -64.8674 CORAL_loss: 1.9921\n",
            "Epoch: 5 || Train_src_acc: 0.4828, Train_tar_acc: 0.1379, Train_loss: -47.9519\n",
            "Epoch 0006 / 0030\n",
            "============\n",
            "[1/33] class loss: -63.9068 CORAL_loss: 1.1266\n",
            "[2/33] class loss: -65.8646 CORAL_loss: 2.9628\n",
            "[3/33] class loss: -63.9115 CORAL_loss: 6.9009\n",
            "[4/33] class loss: -63.9509 CORAL_loss: 5.1826\n",
            "[5/33] class loss: -50.4067 CORAL_loss: 4.3120\n",
            "[6/33] class loss: -63.5245 CORAL_loss: 1.2844\n",
            "[7/33] class loss: -58.1747 CORAL_loss: 5.0093\n",
            "[8/33] class loss: -72.1446 CORAL_loss: 12.7713\n",
            "[9/33] class loss: -64.9456 CORAL_loss: 2.6447\n",
            "[10/33] class loss: -65.1551 CORAL_loss: 2.8486\n",
            "[11/33] class loss: -70.9149 CORAL_loss: 1.7081\n",
            "[12/33] class loss: -73.3476 CORAL_loss: 3.8836\n",
            "[13/33] class loss: -68.4153 CORAL_loss: 12.9877\n",
            "[14/33] class loss: -69.9093 CORAL_loss: 5.3972\n",
            "[15/33] class loss: -56.2680 CORAL_loss: 3.4701\n",
            "[16/33] class loss: -70.1600 CORAL_loss: 2.6786\n",
            "[17/33] class loss: -63.4400 CORAL_loss: 9.0016\n",
            "[18/33] class loss: -79.4903 CORAL_loss: 2.3955\n",
            "[19/33] class loss: -70.5904 CORAL_loss: 8.3790\n",
            "[20/33] class loss: -70.4853 CORAL_loss: 6.3755\n",
            "[21/33] class loss: -77.8368 CORAL_loss: 6.1490\n",
            "[22/33] class loss: -78.7973 CORAL_loss: 3.3102\n",
            "[23/33] class loss: -74.4316 CORAL_loss: 4.8364\n",
            "[24/33] class loss: -75.8661 CORAL_loss: 2.8311\n",
            "[25/33] class loss: -59.5017 CORAL_loss: 12.0957\n",
            "[26/33] class loss: -75.6051 CORAL_loss: 4.0967\n",
            "[27/33] class loss: -66.6674 CORAL_loss: 1.1550\n",
            "[28/33] class loss: -85.7364 CORAL_loss: 3.1982\n",
            "[29/33] class loss: -75.4841 CORAL_loss: 12.0137\n",
            "[30/33] class loss: -79.1571 CORAL_loss: 44.1051\n",
            "[31/33] class loss: -83.5254 CORAL_loss: 2.5786\n",
            "[32/33] class loss: -83.3126 CORAL_loss: 6.8131\n",
            "[33/33] class loss: -79.8621 CORAL_loss: 5.5815\n",
            "Epoch: 6 || Train_src_acc: 0.4652, Train_tar_acc: 0.1217, Train_loss: -63.9608\n",
            "Epoch 0007 / 0030\n",
            "============\n",
            "[1/33] class loss: -80.3741 CORAL_loss: 8.0235\n",
            "[2/33] class loss: -73.9852 CORAL_loss: 2.9081\n",
            "[3/33] class loss: -84.1874 CORAL_loss: 4.3173\n",
            "[4/33] class loss: -68.2638 CORAL_loss: 5.4869\n",
            "[5/33] class loss: -87.0004 CORAL_loss: 4.2332\n",
            "[6/33] class loss: -73.2063 CORAL_loss: 8.2124\n",
            "[7/33] class loss: -77.6815 CORAL_loss: 14.2893\n",
            "[8/33] class loss: -90.4587 CORAL_loss: 20.5048\n",
            "[9/33] class loss: -89.9290 CORAL_loss: 23.7113\n",
            "[10/33] class loss: -89.4113 CORAL_loss: 16.0083\n",
            "[11/33] class loss: -87.1793 CORAL_loss: 2.0594\n",
            "[12/33] class loss: -78.2407 CORAL_loss: 3.4051\n",
            "[13/33] class loss: -88.3879 CORAL_loss: 4.7532\n",
            "[14/33] class loss: -71.3077 CORAL_loss: 14.8279\n",
            "[15/33] class loss: -92.0666 CORAL_loss: 11.8052\n",
            "[16/33] class loss: -75.6156 CORAL_loss: 5.4049\n",
            "[17/33] class loss: -81.7799 CORAL_loss: 3.4456\n",
            "[18/33] class loss: -95.5950 CORAL_loss: 6.0796\n",
            "[19/33] class loss: -94.1056 CORAL_loss: 6.7952\n",
            "[20/33] class loss: -101.3342 CORAL_loss: 17.7432\n",
            "[21/33] class loss: -90.3519 CORAL_loss: 9.8936\n",
            "[22/33] class loss: -80.5546 CORAL_loss: 15.4969\n",
            "[23/33] class loss: -93.5209 CORAL_loss: 4.4902\n",
            "[24/33] class loss: -71.6520 CORAL_loss: 2.8343\n",
            "[25/33] class loss: -94.4820 CORAL_loss: 3.1202\n",
            "[26/33] class loss: -77.4601 CORAL_loss: 3.9862\n",
            "[27/33] class loss: -86.2040 CORAL_loss: 5.6023\n",
            "[28/33] class loss: -99.4448 CORAL_loss: 3.8204\n",
            "[29/33] class loss: -98.7338 CORAL_loss: 3.1640\n",
            "[30/33] class loss: -100.9537 CORAL_loss: 33.0152\n",
            "[31/33] class loss: -94.3540 CORAL_loss: 25.1503\n",
            "[32/33] class loss: -84.5153 CORAL_loss: 3.4595\n",
            "[33/33] class loss: -97.3493 CORAL_loss: 9.1007\n",
            "Epoch: 7 || Train_src_acc: 0.4540, Train_tar_acc: 0.1096, Train_loss: -77.0466\n",
            "Epoch 0008 / 0030\n",
            "============\n",
            "[1/33] class loss: -94.8749 CORAL_loss: 34.8259\n",
            "[2/33] class loss: -86.5273 CORAL_loss: 3.0995\n",
            "[3/33] class loss: -85.3876 CORAL_loss: 23.7537\n",
            "[4/33] class loss: -97.0204 CORAL_loss: 4.9026\n",
            "[5/33] class loss: -97.4484 CORAL_loss: 3.9929\n",
            "[6/33] class loss: -96.9363 CORAL_loss: 6.3350\n",
            "[7/33] class loss: -107.3369 CORAL_loss: 8.0097\n",
            "[8/33] class loss: -85.7193 CORAL_loss: 2.2459\n",
            "[9/33] class loss: -97.0730 CORAL_loss: 2.9069\n",
            "[10/33] class loss: -85.8269 CORAL_loss: 15.1392\n",
            "[11/33] class loss: -98.6080 CORAL_loss: 22.9919\n",
            "[12/33] class loss: -91.2859 CORAL_loss: 7.4632\n",
            "[13/33] class loss: -90.7859 CORAL_loss: 3.4067\n",
            "[14/33] class loss: -102.3314 CORAL_loss: 9.4413\n",
            "[15/33] class loss: -102.7313 CORAL_loss: 10.5563\n",
            "[16/33] class loss: -101.7905 CORAL_loss: 6.1255\n",
            "[17/33] class loss: -112.4246 CORAL_loss: 4.9778\n",
            "[18/33] class loss: -90.5581 CORAL_loss: 22.1265\n",
            "[19/33] class loss: -102.4059 CORAL_loss: 15.7980\n",
            "[20/33] class loss: -90.5327 CORAL_loss: 36.4873\n",
            "[21/33] class loss: -100.5182 CORAL_loss: 7.7173\n",
            "[22/33] class loss: -95.7343 CORAL_loss: 6.3732\n",
            "[23/33] class loss: -95.7352 CORAL_loss: 4.0620\n",
            "[24/33] class loss: -104.3706 CORAL_loss: 2.7754\n",
            "[25/33] class loss: -103.6997 CORAL_loss: 2.9870\n",
            "[26/33] class loss: -105.3111 CORAL_loss: 4.1144\n",
            "[27/33] class loss: -115.1892 CORAL_loss: 12.7551\n",
            "[28/33] class loss: -91.4288 CORAL_loss: 24.7200\n",
            "[29/33] class loss: -103.9975 CORAL_loss: 5.4916\n",
            "[30/33] class loss: -97.1847 CORAL_loss: 73.8806\n",
            "[31/33] class loss: -102.4688 CORAL_loss: 6.2607\n",
            "[32/33] class loss: -97.6514 CORAL_loss: 26.5359\n",
            "[33/33] class loss: -97.0239 CORAL_loss: 4.0675\n",
            "Epoch: 8 || Train_src_acc: 0.4500, Train_tar_acc: 0.1187, Train_loss: -84.8967\n",
            "Epoch 0009 / 0030\n",
            "============\n",
            "[1/33] class loss: -96.6648 CORAL_loss: 32.7354\n",
            "[2/33] class loss: -112.4219 CORAL_loss: 3.6827\n",
            "[3/33] class loss: -94.3555 CORAL_loss: 16.9272\n",
            "[4/33] class loss: -108.1275 CORAL_loss: 3.3479\n",
            "[5/33] class loss: -102.1222 CORAL_loss: 24.1976\n",
            "[6/33] class loss: -101.5199 CORAL_loss: 10.0384\n",
            "[7/33] class loss: -104.2696 CORAL_loss: 28.1594\n",
            "[8/33] class loss: -109.5655 CORAL_loss: 11.4737\n",
            "[9/33] class loss: -106.9876 CORAL_loss: 3.6728\n",
            "[10/33] class loss: -111.1145 CORAL_loss: 10.7841\n",
            "[11/33] class loss: -100.9461 CORAL_loss: 15.4422\n",
            "[12/33] class loss: -114.2196 CORAL_loss: 18.3469\n",
            "[13/33] class loss: -93.0134 CORAL_loss: 7.2080\n",
            "[14/33] class loss: -110.1905 CORAL_loss: 5.4515\n",
            "[15/33] class loss: -106.3204 CORAL_loss: 6.1708\n",
            "[16/33] class loss: -105.8582 CORAL_loss: 4.2835\n",
            "[17/33] class loss: -104.2291 CORAL_loss: 3.8646\n",
            "[18/33] class loss: -110.4200 CORAL_loss: 4.3594\n",
            "[19/33] class loss: -109.9894 CORAL_loss: 10.1359\n",
            "[20/33] class loss: -114.8509 CORAL_loss: 65.9147\n",
            "[21/33] class loss: -101.4872 CORAL_loss: 5.2809\n",
            "[22/33] class loss: -119.6449 CORAL_loss: 5.1712\n",
            "[23/33] class loss: -92.6080 CORAL_loss: 7.1481\n",
            "[24/33] class loss: -113.6964 CORAL_loss: 9.4589\n",
            "[25/33] class loss: -109.5612 CORAL_loss: 3.4041\n",
            "[26/33] class loss: -109.6184 CORAL_loss: 8.8488\n",
            "[27/33] class loss: -105.8869 CORAL_loss: 7.2297\n",
            "[28/33] class loss: -113.1606 CORAL_loss: 7.7514\n",
            "[29/33] class loss: -113.9359 CORAL_loss: 7.8435\n",
            "[30/33] class loss: -121.0164 CORAL_loss: 71.7373\n",
            "[31/33] class loss: -104.2128 CORAL_loss: 14.1809\n",
            "[32/33] class loss: -123.2210 CORAL_loss: 9.3951\n",
            "[33/33] class loss: -97.3375 CORAL_loss: 22.7176\n",
            "Epoch: 9 || Train_src_acc: 0.4515, Train_tar_acc: 0.1111, Train_loss: -93.2185\n",
            "Epoch 0010 / 0030\n",
            "============\n",
            "[1/33] class loss: -112.2339 CORAL_loss: 5.4197\n",
            "[2/33] class loss: -110.6222 CORAL_loss: 5.6353\n",
            "[3/33] class loss: -110.6863 CORAL_loss: 7.0484\n",
            "[4/33] class loss: -114.2368 CORAL_loss: 3.7999\n",
            "[5/33] class loss: -112.3576 CORAL_loss: 10.7827\n",
            "[6/33] class loss: -116.2446 CORAL_loss: 3.5897\n",
            "[7/33] class loss: -107.8935 CORAL_loss: 17.7939\n",
            "[8/33] class loss: -120.2147 CORAL_loss: 6.3956\n",
            "[9/33] class loss: -125.5650 CORAL_loss: 8.2378\n",
            "[10/33] class loss: -119.5394 CORAL_loss: 21.2364\n",
            "[11/33] class loss: -118.0725 CORAL_loss: 13.4445\n",
            "[12/33] class loss: -115.6060 CORAL_loss: 7.2248\n",
            "[13/33] class loss: -116.3790 CORAL_loss: 9.3846\n",
            "[14/33] class loss: -121.4927 CORAL_loss: 23.3449\n",
            "[15/33] class loss: -117.1980 CORAL_loss: 7.9096\n",
            "[16/33] class loss: -121.9720 CORAL_loss: 33.3914\n",
            "[17/33] class loss: -115.8778 CORAL_loss: 5.2822\n",
            "[18/33] class loss: -127.4940 CORAL_loss: 3.1355\n",
            "[19/33] class loss: -130.2094 CORAL_loss: 21.7936\n",
            "[20/33] class loss: -131.5618 CORAL_loss: 89.5088\n",
            "[21/33] class loss: -124.0501 CORAL_loss: 18.0622\n",
            "[22/33] class loss: -120.7664 CORAL_loss: 21.2477\n",
            "[23/33] class loss: -118.0042 CORAL_loss: 20.6647\n",
            "[24/33] class loss: -124.5158 CORAL_loss: 6.3375\n",
            "[25/33] class loss: -119.8828 CORAL_loss: 9.3475\n",
            "[26/33] class loss: -125.3766 CORAL_loss: 6.8628\n",
            "[27/33] class loss: -117.7704 CORAL_loss: 9.0773\n",
            "[28/33] class loss: -129.8497 CORAL_loss: 6.1529\n",
            "[29/33] class loss: -133.7955 CORAL_loss: 11.8964\n",
            "[30/33] class loss: -136.0684 CORAL_loss: 16.8598\n",
            "[31/33] class loss: -126.5662 CORAL_loss: 5.5675\n",
            "[32/33] class loss: -124.1383 CORAL_loss: 5.9681\n",
            "[33/33] class loss: -121.5673 CORAL_loss: 13.9191\n",
            "Epoch: 10 || Train_src_acc: 0.4702, Train_tar_acc: 0.1197, Train_loss: -107.0147\n",
            "Epoch 0011 / 0030\n",
            "============\n",
            "[1/33] class loss: -134.8142 CORAL_loss: 11.1957\n",
            "[2/33] class loss: -136.5393 CORAL_loss: 24.4595\n",
            "[3/33] class loss: -118.7403 CORAL_loss: 13.9955\n",
            "[4/33] class loss: -123.8392 CORAL_loss: 20.8253\n",
            "[5/33] class loss: -136.9723 CORAL_loss: 6.2307\n",
            "[6/33] class loss: -144.2279 CORAL_loss: 7.1602\n",
            "[7/33] class loss: -130.2358 CORAL_loss: 39.2661\n",
            "[8/33] class loss: -119.8535 CORAL_loss: 22.2374\n",
            "[9/33] class loss: -116.7176 CORAL_loss: 8.5232\n",
            "[10/33] class loss: -126.6004 CORAL_loss: 17.1056\n",
            "[11/33] class loss: -141.3730 CORAL_loss: 13.3651\n",
            "[12/33] class loss: -142.2123 CORAL_loss: 10.4492\n",
            "[13/33] class loss: -122.2248 CORAL_loss: 14.4148\n",
            "[14/33] class loss: -127.1505 CORAL_loss: 15.5103\n",
            "[15/33] class loss: -142.5326 CORAL_loss: 22.9711\n",
            "[16/33] class loss: -149.1084 CORAL_loss: 13.7269\n",
            "[17/33] class loss: -137.0252 CORAL_loss: 32.9296\n",
            "[18/33] class loss: -124.8009 CORAL_loss: 12.2592\n",
            "[19/33] class loss: -121.6019 CORAL_loss: 87.2474\n",
            "[20/33] class loss: -132.0553 CORAL_loss: 59.5552\n",
            "[21/33] class loss: -143.9286 CORAL_loss: 39.6260\n",
            "[22/33] class loss: -144.2907 CORAL_loss: 18.1837\n",
            "[23/33] class loss: -121.2400 CORAL_loss: 15.4123\n",
            "[24/33] class loss: -127.7334 CORAL_loss: 7.4637\n",
            "[25/33] class loss: -141.1885 CORAL_loss: 75.3112\n",
            "[26/33] class loss: -152.6764 CORAL_loss: 29.4560\n",
            "[27/33] class loss: -134.8320 CORAL_loss: 8.9694\n",
            "[28/33] class loss: -122.9195 CORAL_loss: 25.5824\n",
            "[29/33] class loss: -114.8712 CORAL_loss: 10.0483\n",
            "[30/33] class loss: -127.1046 CORAL_loss: 118.0113\n",
            "[31/33] class loss: -140.2431 CORAL_loss: 19.4688\n",
            "[32/33] class loss: -142.0241 CORAL_loss: 6.4042\n",
            "[33/33] class loss: -119.7220 CORAL_loss: 14.0011\n",
            "Epoch: 11 || Train_src_acc: 0.4990, Train_tar_acc: 0.1131, Train_loss: -106.6677\n",
            "Epoch 0012 / 0030\n",
            "============\n",
            "[1/33] class loss: -106.7702 CORAL_loss: 21.9617\n",
            "[2/33] class loss: -135.1271 CORAL_loss: 21.5609\n",
            "[3/33] class loss: -111.4528 CORAL_loss: 8.0100\n",
            "[4/33] class loss: -117.9405 CORAL_loss: 7.2781\n",
            "[5/33] class loss: -122.3721 CORAL_loss: 7.6659\n",
            "[6/33] class loss: -130.2457 CORAL_loss: 18.8701\n",
            "[7/33] class loss: -124.7168 CORAL_loss: 21.6456\n",
            "[8/33] class loss: -145.8826 CORAL_loss: 7.0410\n",
            "[9/33] class loss: -155.2233 CORAL_loss: 15.3263\n",
            "[10/33] class loss: -149.8867 CORAL_loss: 25.6557\n",
            "[11/33] class loss: -106.2127 CORAL_loss: 3.8331\n",
            "[12/33] class loss: -133.5294 CORAL_loss: 7.7640\n",
            "[13/33] class loss: -112.3800 CORAL_loss: 9.9496\n",
            "[14/33] class loss: -117.0578 CORAL_loss: 11.4305\n",
            "[15/33] class loss: -122.4867 CORAL_loss: 20.5919\n",
            "[16/33] class loss: -130.7781 CORAL_loss: 6.2528\n",
            "[17/33] class loss: -126.0639 CORAL_loss: 10.9538\n",
            "[18/33] class loss: -145.6046 CORAL_loss: 5.1408\n",
            "[19/33] class loss: -158.4747 CORAL_loss: 10.9334\n",
            "[20/33] class loss: -151.8219 CORAL_loss: 41.8915\n",
            "[21/33] class loss: -109.1529 CORAL_loss: 6.1287\n",
            "[22/33] class loss: -135.1464 CORAL_loss: 15.2796\n",
            "[23/33] class loss: -114.3248 CORAL_loss: 4.5988\n",
            "[24/33] class loss: -122.7813 CORAL_loss: 13.4695\n",
            "[25/33] class loss: -126.4533 CORAL_loss: 15.4142\n",
            "[26/33] class loss: -135.0309 CORAL_loss: 8.5340\n",
            "[27/33] class loss: -129.5213 CORAL_loss: 9.6805\n",
            "[28/33] class loss: -150.4854 CORAL_loss: 10.8390\n",
            "[29/33] class loss: -161.7321 CORAL_loss: 14.4132\n",
            "[30/33] class loss: -154.9728 CORAL_loss: 36.8101\n",
            "[31/33] class loss: -112.6699 CORAL_loss: 8.0744\n",
            "[32/33] class loss: -141.3875 CORAL_loss: 13.3110\n",
            "[33/33] class loss: -116.9734 CORAL_loss: 5.7167\n",
            "Epoch: 12 || Train_src_acc: 0.4480, Train_tar_acc: 0.1293, Train_loss: -117.2313\n",
            "Epoch 0013 / 0030\n",
            "============\n",
            "[1/33] class loss: -125.0043 CORAL_loss: 6.6489\n",
            "[2/33] class loss: -147.6314 CORAL_loss: 12.7092\n",
            "[3/33] class loss: -135.7684 CORAL_loss: 36.9675\n",
            "[4/33] class loss: -144.3743 CORAL_loss: 6.4870\n",
            "[5/33] class loss: -142.8497 CORAL_loss: 8.0304\n",
            "[6/33] class loss: -106.3911 CORAL_loss: 44.3909\n",
            "[7/33] class loss: -138.3082 CORAL_loss: 11.8057\n",
            "[8/33] class loss: -145.8848 CORAL_loss: 16.6770\n",
            "[9/33] class loss: -148.9999 CORAL_loss: 13.5083\n",
            "[10/33] class loss: -157.6138 CORAL_loss: 15.7700\n",
            "[11/33] class loss: -129.0686 CORAL_loss: 36.8497\n",
            "[12/33] class loss: -153.0031 CORAL_loss: 20.9007\n",
            "[13/33] class loss: -137.0824 CORAL_loss: 22.1134\n",
            "[14/33] class loss: -146.8429 CORAL_loss: 7.5838\n",
            "[15/33] class loss: -147.9745 CORAL_loss: 20.9084\n",
            "[16/33] class loss: -109.8922 CORAL_loss: 9.4857\n",
            "[17/33] class loss: -138.0219 CORAL_loss: 30.0130\n",
            "[18/33] class loss: -151.7456 CORAL_loss: 8.1136\n",
            "[19/33] class loss: -153.1300 CORAL_loss: 24.0449\n",
            "[20/33] class loss: -161.2969 CORAL_loss: 61.9243\n",
            "[21/33] class loss: -130.6159 CORAL_loss: 9.8079\n",
            "[22/33] class loss: -153.0526 CORAL_loss: 10.1753\n",
            "[23/33] class loss: -136.0779 CORAL_loss: 21.5743\n",
            "[24/33] class loss: -148.9630 CORAL_loss: 15.5306\n",
            "[25/33] class loss: -146.4625 CORAL_loss: 19.7145\n",
            "[26/33] class loss: -111.9768 CORAL_loss: 12.5368\n",
            "[27/33] class loss: -142.7905 CORAL_loss: 10.4447\n",
            "[28/33] class loss: -153.3637 CORAL_loss: 12.5968\n",
            "[29/33] class loss: -154.5623 CORAL_loss: 11.6847\n",
            "[30/33] class loss: -162.4306 CORAL_loss: 21.8370\n",
            "[31/33] class loss: -136.2657 CORAL_loss: 7.0364\n",
            "[32/33] class loss: -160.4567 CORAL_loss: 35.3793\n",
            "[33/33] class loss: -143.4496 CORAL_loss: 43.4995\n",
            "Epoch: 13 || Train_src_acc: 0.4551, Train_tar_acc: 0.1333, Train_loss: -122.8667\n",
            "Epoch 0014 / 0030\n",
            "============\n",
            "[1/33] class loss: -152.2532 CORAL_loss: 7.1839\n",
            "[2/33] class loss: -126.9572 CORAL_loss: 10.6882\n",
            "[3/33] class loss: -152.6157 CORAL_loss: 11.9142\n",
            "[4/33] class loss: -151.2065 CORAL_loss: 11.6167\n",
            "[5/33] class loss: -156.8425 CORAL_loss: 13.2794\n",
            "[6/33] class loss: -159.4848 CORAL_loss: 20.4584\n",
            "[7/33] class loss: -163.9045 CORAL_loss: 11.6126\n",
            "[8/33] class loss: -140.3064 CORAL_loss: 9.9862\n",
            "[9/33] class loss: -152.2388 CORAL_loss: 7.4804\n",
            "[10/33] class loss: -144.2199 CORAL_loss: 35.1702\n",
            "[11/33] class loss: -156.1916 CORAL_loss: 49.4093\n",
            "[12/33] class loss: -129.5190 CORAL_loss: 11.6019\n",
            "[13/33] class loss: -158.7367 CORAL_loss: 21.2963\n",
            "[14/33] class loss: -155.0952 CORAL_loss: 44.8082\n",
            "[15/33] class loss: -162.3381 CORAL_loss: 10.7368\n",
            "[16/33] class loss: -160.9282 CORAL_loss: 10.0924\n",
            "[17/33] class loss: -167.2734 CORAL_loss: 31.9785\n",
            "[18/33] class loss: -143.7701 CORAL_loss: 13.6385\n",
            "[19/33] class loss: -157.9018 CORAL_loss: 22.0973\n",
            "[20/33] class loss: -145.7228 CORAL_loss: 51.8753\n",
            "[21/33] class loss: -157.2243 CORAL_loss: 13.2704\n",
            "[22/33] class loss: -130.9068 CORAL_loss: 10.1478\n",
            "[23/33] class loss: -161.6437 CORAL_loss: 14.1984\n",
            "[24/33] class loss: -157.6547 CORAL_loss: 7.7007\n",
            "[25/33] class loss: -166.5096 CORAL_loss: 14.1323\n",
            "[26/33] class loss: -166.0730 CORAL_loss: 14.4474\n",
            "[27/33] class loss: -173.1099 CORAL_loss: 16.4080\n",
            "[28/33] class loss: -147.2421 CORAL_loss: 27.8645\n",
            "[29/33] class loss: -165.1454 CORAL_loss: 36.5059\n",
            "[30/33] class loss: -150.0641 CORAL_loss: 141.9090\n",
            "[31/33] class loss: -159.6997 CORAL_loss: 34.7261\n",
            "[32/33] class loss: -133.9845 CORAL_loss: 12.8888\n",
            "[33/33] class loss: -164.1049 CORAL_loss: 20.3625\n",
            "Epoch: 14 || Train_src_acc: 0.4596, Train_tar_acc: 0.1283, Train_loss: -130.2843\n",
            "Epoch 0015 / 0030\n",
            "============\n",
            "[1/33] class loss: -186.0652 CORAL_loss: 16.6378\n",
            "[2/33] class loss: -170.9583 CORAL_loss: 15.4318\n",
            "[3/33] class loss: -152.0668 CORAL_loss: 22.5096\n",
            "[4/33] class loss: -146.2966 CORAL_loss: 33.1697\n",
            "[5/33] class loss: -165.9499 CORAL_loss: 17.9346\n",
            "[6/33] class loss: -174.5812 CORAL_loss: 21.0077\n",
            "[7/33] class loss: -159.1521 CORAL_loss: 12.4492\n",
            "[8/33] class loss: -151.7682 CORAL_loss: 20.7289\n",
            "[9/33] class loss: -144.2046 CORAL_loss: 31.8274\n",
            "[10/33] class loss: -161.0539 CORAL_loss: 33.8023\n",
            "[11/33] class loss: -189.6949 CORAL_loss: 13.9287\n",
            "[12/33] class loss: -173.0561 CORAL_loss: 9.3389\n",
            "[13/33] class loss: -153.3867 CORAL_loss: 20.0636\n",
            "[14/33] class loss: -148.3364 CORAL_loss: 29.8233\n",
            "[15/33] class loss: -168.6528 CORAL_loss: 63.8274\n",
            "[16/33] class loss: -176.1848 CORAL_loss: 16.4740\n",
            "[17/33] class loss: -163.0461 CORAL_loss: 49.8761\n",
            "[18/33] class loss: -152.6013 CORAL_loss: 9.6176\n",
            "[19/33] class loss: -144.1279 CORAL_loss: 39.7975\n",
            "[20/33] class loss: -162.4736 CORAL_loss: 175.3225\n",
            "[21/33] class loss: -189.7908 CORAL_loss: 32.2216\n",
            "[22/33] class loss: -175.9232 CORAL_loss: 13.4072\n",
            "[23/33] class loss: -154.8700 CORAL_loss: 13.2136\n",
            "[24/33] class loss: -146.3116 CORAL_loss: 19.9122\n",
            "[25/33] class loss: -167.8215 CORAL_loss: 50.9725\n",
            "[26/33] class loss: -176.4818 CORAL_loss: 34.1390\n",
            "[27/33] class loss: -165.5232 CORAL_loss: 28.0475\n",
            "[28/33] class loss: -152.2587 CORAL_loss: 7.0776\n",
            "[29/33] class loss: -145.0648 CORAL_loss: 21.2859\n",
            "[30/33] class loss: -157.2521 CORAL_loss: 60.7759\n",
            "[31/33] class loss: -190.7518 CORAL_loss: 11.0001\n",
            "[32/33] class loss: -175.0012 CORAL_loss: 16.6820\n",
            "[33/33] class loss: -156.5191 CORAL_loss: 14.5792\n",
            "Epoch: 15 || Train_src_acc: 0.4646, Train_tar_acc: 0.1207, Train_loss: -133.9498\n",
            "Epoch 0016 / 0030\n",
            "============\n",
            "[1/33] class loss: -121.3824 CORAL_loss: 63.6757\n",
            "[2/33] class loss: -179.6521 CORAL_loss: 11.8928\n",
            "[3/33] class loss: -160.7037 CORAL_loss: 9.1029\n",
            "[4/33] class loss: -155.7813 CORAL_loss: 21.1520\n",
            "[5/33] class loss: -174.6074 CORAL_loss: 21.2762\n",
            "[6/33] class loss: -154.9179 CORAL_loss: 39.5271\n",
            "[7/33] class loss: -181.4174 CORAL_loss: 26.6072\n",
            "[8/33] class loss: -164.9420 CORAL_loss: 26.8306\n",
            "[9/33] class loss: -171.8320 CORAL_loss: 17.0121\n",
            "[10/33] class loss: -180.4995 CORAL_loss: 114.9285\n",
            "[11/33] class loss: -125.2016 CORAL_loss: 14.0765\n",
            "[12/33] class loss: -180.8152 CORAL_loss: 18.6052\n",
            "[13/33] class loss: -163.2500 CORAL_loss: 18.0467\n",
            "[14/33] class loss: -156.4321 CORAL_loss: 14.4866\n",
            "[15/33] class loss: -174.1389 CORAL_loss: 18.9453\n",
            "[16/33] class loss: -156.4391 CORAL_loss: 15.2025\n",
            "[17/33] class loss: -185.5253 CORAL_loss: 26.1977\n",
            "[18/33] class loss: -166.8970 CORAL_loss: 19.0102\n",
            "[19/33] class loss: -173.3976 CORAL_loss: 17.5525\n",
            "[20/33] class loss: -176.1570 CORAL_loss: 162.2623\n",
            "[21/33] class loss: -126.8530 CORAL_loss: 19.7589\n",
            "[22/33] class loss: -182.7332 CORAL_loss: 9.2871\n",
            "[23/33] class loss: -164.2993 CORAL_loss: 7.1308\n",
            "[24/33] class loss: -157.8910 CORAL_loss: 14.3705\n",
            "[25/33] class loss: -174.4142 CORAL_loss: 13.2573\n",
            "[26/33] class loss: -158.6767 CORAL_loss: 20.2478\n",
            "[27/33] class loss: -186.2893 CORAL_loss: 16.0135\n",
            "[28/33] class loss: -164.3259 CORAL_loss: 17.0937\n",
            "[29/33] class loss: -176.4694 CORAL_loss: 13.3638\n",
            "[30/33] class loss: -170.7366 CORAL_loss: 52.4492\n",
            "[31/33] class loss: -129.2144 CORAL_loss: 27.6407\n",
            "[32/33] class loss: -183.8336 CORAL_loss: 26.0216\n",
            "[33/33] class loss: -164.5843 CORAL_loss: 20.8443\n",
            "Epoch: 16 || Train_src_acc: 0.4576, Train_tar_acc: 0.1283, Train_loss: -136.6800\n",
            "Epoch 0017 / 0030\n",
            "============\n",
            "[1/33] class loss: -157.0723 CORAL_loss: 12.1537\n",
            "[2/33] class loss: -177.7099 CORAL_loss: 55.5958\n",
            "[3/33] class loss: -174.6301 CORAL_loss: 8.2378\n",
            "[4/33] class loss: -174.2125 CORAL_loss: 49.5262\n",
            "[5/33] class loss: -163.9648 CORAL_loss: 17.6134\n",
            "[6/33] class loss: -162.2661 CORAL_loss: 16.6739\n",
            "[7/33] class loss: -176.5930 CORAL_loss: 10.8033\n",
            "[8/33] class loss: -160.2918 CORAL_loss: 37.4037\n",
            "[9/33] class loss: -187.9970 CORAL_loss: 19.9440\n",
            "[10/33] class loss: -170.2045 CORAL_loss: 53.7819\n",
            "[11/33] class loss: -159.1324 CORAL_loss: 12.9388\n",
            "[12/33] class loss: -181.5406 CORAL_loss: 11.9994\n",
            "[13/33] class loss: -176.6602 CORAL_loss: 34.1442\n",
            "[14/33] class loss: -177.7495 CORAL_loss: 12.5578\n",
            "[15/33] class loss: -167.4142 CORAL_loss: 14.2539\n",
            "[16/33] class loss: -168.5646 CORAL_loss: 15.3227\n",
            "[17/33] class loss: -179.0889 CORAL_loss: 12.9504\n",
            "[18/33] class loss: -161.8649 CORAL_loss: 17.2391\n",
            "[19/33] class loss: -189.3128 CORAL_loss: 22.5017\n",
            "[20/33] class loss: -173.9555 CORAL_loss: 64.3637\n",
            "[21/33] class loss: -159.6652 CORAL_loss: 24.0701\n",
            "[22/33] class loss: -183.4051 CORAL_loss: 15.6509\n",
            "[23/33] class loss: -180.9887 CORAL_loss: 17.6881\n",
            "[24/33] class loss: -181.8088 CORAL_loss: 8.6159\n",
            "[25/33] class loss: -170.2631 CORAL_loss: 21.0951\n",
            "[26/33] class loss: -170.1698 CORAL_loss: 10.8366\n",
            "[27/33] class loss: -182.8826 CORAL_loss: 12.8055\n",
            "[28/33] class loss: -165.5406 CORAL_loss: 21.8338\n",
            "[29/33] class loss: -193.9438 CORAL_loss: 22.9251\n",
            "[30/33] class loss: -178.7749 CORAL_loss: 79.4263\n",
            "[31/33] class loss: -162.4890 CORAL_loss: 29.4932\n",
            "[32/33] class loss: -186.0846 CORAL_loss: 13.7442\n",
            "[33/33] class loss: -185.4350 CORAL_loss: 21.1897\n",
            "Epoch: 17 || Train_src_acc: 0.4904, Train_tar_acc: 0.1258, Train_loss: -149.7666\n",
            "Epoch 0018 / 0030\n",
            "============\n",
            "[1/33] class loss: -184.3375 CORAL_loss: 9.9294\n",
            "[2/33] class loss: -175.5358 CORAL_loss: 16.5234\n",
            "[3/33] class loss: -179.7478 CORAL_loss: 21.2631\n",
            "[4/33] class loss: -207.6513 CORAL_loss: 29.2913\n",
            "[5/33] class loss: -162.4981 CORAL_loss: 17.1044\n",
            "[6/33] class loss: -201.7473 CORAL_loss: 19.5476\n",
            "[7/33] class loss: -192.5479 CORAL_loss: 60.6124\n",
            "[8/33] class loss: -179.7484 CORAL_loss: 17.8549\n",
            "[9/33] class loss: -176.0186 CORAL_loss: 23.5024\n",
            "[10/33] class loss: -140.4813 CORAL_loss: 73.4266\n",
            "[11/33] class loss: -188.4668 CORAL_loss: 12.4781\n",
            "[12/33] class loss: -179.3983 CORAL_loss: 59.1977\n",
            "[13/33] class loss: -184.0075 CORAL_loss: 45.1967\n",
            "[14/33] class loss: -210.0828 CORAL_loss: 15.5491\n",
            "[15/33] class loss: -162.5105 CORAL_loss: 22.8779\n",
            "[16/33] class loss: -205.3765 CORAL_loss: 21.6744\n",
            "[17/33] class loss: -191.7660 CORAL_loss: 17.4719\n",
            "[18/33] class loss: -182.0918 CORAL_loss: 78.3434\n",
            "[19/33] class loss: -175.6385 CORAL_loss: 15.3174\n",
            "[20/33] class loss: -137.5851 CORAL_loss: 113.9042\n",
            "[21/33] class loss: -192.4005 CORAL_loss: 17.0024\n",
            "[22/33] class loss: -178.4303 CORAL_loss: 81.3996\n",
            "[23/33] class loss: -181.8705 CORAL_loss: 24.6091\n",
            "[24/33] class loss: -210.7798 CORAL_loss: 16.1096\n",
            "[25/33] class loss: -160.3787 CORAL_loss: 17.7368\n",
            "[26/33] class loss: -202.3266 CORAL_loss: 14.3837\n",
            "[27/33] class loss: -192.7234 CORAL_loss: 18.7467\n",
            "[28/33] class loss: -176.9102 CORAL_loss: 16.0768\n",
            "[29/33] class loss: -173.1220 CORAL_loss: 22.1037\n",
            "[30/33] class loss: -138.5080 CORAL_loss: 45.5312\n",
            "[31/33] class loss: -191.0740 CORAL_loss: 18.0957\n",
            "[32/33] class loss: -174.7039 CORAL_loss: 17.9504\n",
            "[33/33] class loss: -181.0963 CORAL_loss: 37.4331\n",
            "Epoch: 18 || Train_src_acc: 0.4874, Train_tar_acc: 0.1263, Train_loss: -149.4944\n",
            "Epoch 0019 / 0030\n",
            "============\n",
            "[1/33] class loss: -190.4025 CORAL_loss: 10.4899\n",
            "[2/33] class loss: -184.5412 CORAL_loss: 28.5543\n",
            "[3/33] class loss: -163.9280 CORAL_loss: 17.7551\n",
            "[4/33] class loss: -178.9927 CORAL_loss: 42.0612\n",
            "[5/33] class loss: -178.2410 CORAL_loss: 28.8589\n",
            "[6/33] class loss: -154.9435 CORAL_loss: 24.7392\n",
            "[7/33] class loss: -158.5428 CORAL_loss: 31.1967\n",
            "[8/33] class loss: -211.6926 CORAL_loss: 18.4884\n",
            "[9/33] class loss: -212.4669 CORAL_loss: 31.7374\n",
            "[10/33] class loss: -194.6228 CORAL_loss: 64.0370\n",
            "[11/33] class loss: -192.1627 CORAL_loss: 14.1197\n",
            "[12/33] class loss: -186.2772 CORAL_loss: 17.9576\n",
            "[13/33] class loss: -169.1527 CORAL_loss: 43.7929\n",
            "[14/33] class loss: -182.8278 CORAL_loss: 113.0478\n",
            "[15/33] class loss: -179.2059 CORAL_loss: 9.8061\n",
            "[16/33] class loss: -159.8941 CORAL_loss: 18.8940\n",
            "[17/33] class loss: -163.1597 CORAL_loss: 41.9611\n",
            "[18/33] class loss: -213.9914 CORAL_loss: 11.1703\n",
            "[19/33] class loss: -211.2554 CORAL_loss: 44.7830\n",
            "[20/33] class loss: -199.7227 CORAL_loss: 59.1586\n",
            "[21/33] class loss: -193.6118 CORAL_loss: 30.0027\n",
            "[22/33] class loss: -189.8615 CORAL_loss: 28.4737\n",
            "[23/33] class loss: -167.5013 CORAL_loss: 17.0228\n",
            "[24/33] class loss: -179.5961 CORAL_loss: 20.7379\n",
            "[25/33] class loss: -179.6634 CORAL_loss: 19.7368\n",
            "[26/33] class loss: -159.8884 CORAL_loss: 28.3938\n",
            "[27/33] class loss: -160.3443 CORAL_loss: 20.3888\n",
            "[28/33] class loss: -215.1369 CORAL_loss: 20.3248\n",
            "[29/33] class loss: -212.1282 CORAL_loss: 15.3946\n",
            "[30/33] class loss: -199.0031 CORAL_loss: 45.8681\n",
            "[31/33] class loss: -195.1765 CORAL_loss: 21.9649\n",
            "[32/33] class loss: -192.0635 CORAL_loss: 20.6752\n",
            "[33/33] class loss: -173.0467 CORAL_loss: 14.3616\n",
            "Epoch: 19 || Train_src_acc: 0.4692, Train_tar_acc: 0.1338, Train_loss: -155.3664\n",
            "Epoch 0020 / 0030\n",
            "============\n",
            "[1/33] class loss: -186.6564 CORAL_loss: 18.6455\n",
            "[2/33] class loss: -180.9012 CORAL_loss: 26.2941\n",
            "[3/33] class loss: -189.0767 CORAL_loss: 16.1537\n",
            "[4/33] class loss: -184.5992 CORAL_loss: 15.4174\n",
            "[5/33] class loss: -189.3258 CORAL_loss: 14.3276\n",
            "[6/33] class loss: -194.2773 CORAL_loss: 20.3739\n",
            "[7/33] class loss: -188.6863 CORAL_loss: 19.4694\n",
            "[8/33] class loss: -196.4015 CORAL_loss: 27.1218\n",
            "[9/33] class loss: -205.9939 CORAL_loss: 22.4605\n",
            "[10/33] class loss: -190.7870 CORAL_loss: 112.0136\n",
            "[11/33] class loss: -188.0610 CORAL_loss: 17.2654\n",
            "[12/33] class loss: -184.5125 CORAL_loss: 24.9225\n",
            "[13/33] class loss: -192.6306 CORAL_loss: 32.2854\n",
            "[14/33] class loss: -190.2538 CORAL_loss: 29.0780\n",
            "[15/33] class loss: -193.0795 CORAL_loss: 29.7316\n",
            "[16/33] class loss: -198.9398 CORAL_loss: 16.6153\n",
            "[17/33] class loss: -192.6679 CORAL_loss: 19.5111\n",
            "[18/33] class loss: -198.5546 CORAL_loss: 22.8179\n",
            "[19/33] class loss: -206.5156 CORAL_loss: 29.4406\n",
            "[20/33] class loss: -189.0525 CORAL_loss: 33.3030\n",
            "[21/33] class loss: -189.8585 CORAL_loss: 22.2539\n",
            "[22/33] class loss: -188.1301 CORAL_loss: 17.8006\n",
            "[23/33] class loss: -195.8146 CORAL_loss: 16.8713\n",
            "[24/33] class loss: -192.2427 CORAL_loss: 15.2958\n",
            "[25/33] class loss: -194.6393 CORAL_loss: 19.9883\n",
            "[26/33] class loss: -201.1752 CORAL_loss: 26.8375\n",
            "[27/33] class loss: -195.8984 CORAL_loss: 36.8111\n",
            "[28/33] class loss: -203.7805 CORAL_loss: 24.7418\n",
            "[29/33] class loss: -209.3759 CORAL_loss: 60.6637\n",
            "[30/33] class loss: -193.8683 CORAL_loss: 146.0614\n",
            "[31/33] class loss: -195.1287 CORAL_loss: 19.0295\n",
            "[32/33] class loss: -190.9254 CORAL_loss: 23.5579\n",
            "[33/33] class loss: -199.4191 CORAL_loss: 21.1120\n",
            "Epoch: 20 || Train_src_acc: 0.4879, Train_tar_acc: 0.1333, Train_loss: -163.4229\n",
            "Epoch 0021 / 0030\n",
            "============\n",
            "[1/33] class loss: -202.9871 CORAL_loss: 28.6574\n",
            "[2/33] class loss: -188.1751 CORAL_loss: 32.3559\n",
            "[3/33] class loss: -196.3642 CORAL_loss: 16.3748\n",
            "[4/33] class loss: -190.7146 CORAL_loss: 56.6099\n",
            "[5/33] class loss: -207.8282 CORAL_loss: 46.2898\n",
            "[6/33] class loss: -205.5328 CORAL_loss: 28.2987\n",
            "[7/33] class loss: -216.5789 CORAL_loss: 58.6810\n",
            "[8/33] class loss: -186.1751 CORAL_loss: 11.3414\n",
            "[9/33] class loss: -198.0589 CORAL_loss: 24.7855\n",
            "[10/33] class loss: -186.2212 CORAL_loss: 58.4382\n",
            "[11/33] class loss: -201.7104 CORAL_loss: 28.2644\n",
            "[12/33] class loss: -188.7137 CORAL_loss: 22.9275\n",
            "[13/33] class loss: -193.5964 CORAL_loss: 48.4299\n",
            "[14/33] class loss: -186.1285 CORAL_loss: 13.4705\n",
            "[15/33] class loss: -204.5605 CORAL_loss: 52.3256\n",
            "[16/33] class loss: -204.4202 CORAL_loss: 20.1136\n",
            "[17/33] class loss: -213.8626 CORAL_loss: 12.3778\n",
            "[18/33] class loss: -181.2392 CORAL_loss: 9.7648\n",
            "[19/33] class loss: -197.3691 CORAL_loss: 26.4277\n",
            "[20/33] class loss: -185.7922 CORAL_loss: 34.4864\n",
            "[21/33] class loss: -199.8420 CORAL_loss: 46.3528\n",
            "[22/33] class loss: -189.2313 CORAL_loss: 14.3574\n",
            "[23/33] class loss: -194.8495 CORAL_loss: 38.9954\n",
            "[24/33] class loss: -186.7064 CORAL_loss: 16.4220\n",
            "[25/33] class loss: -209.1122 CORAL_loss: 16.0733\n",
            "[26/33] class loss: -205.5094 CORAL_loss: 17.5757\n",
            "[27/33] class loss: -214.3509 CORAL_loss: 29.8410\n",
            "[28/33] class loss: -186.4002 CORAL_loss: 27.9253\n",
            "[29/33] class loss: -201.9701 CORAL_loss: 35.2454\n",
            "[30/33] class loss: -191.2317 CORAL_loss: 62.2048\n",
            "[31/33] class loss: -200.2272 CORAL_loss: 25.6375\n",
            "[32/33] class loss: -192.5925 CORAL_loss: 26.3811\n",
            "[33/33] class loss: -195.2848 CORAL_loss: 29.3380\n",
            "Epoch: 21 || Train_src_acc: 0.4530, Train_tar_acc: 0.1288, Train_loss: -166.2596\n",
            "Epoch 0022 / 0030\n",
            "============\n",
            "[1/33] class loss: -213.1398 CORAL_loss: 26.3793\n",
            "[2/33] class loss: -182.9464 CORAL_loss: 37.3521\n",
            "[3/33] class loss: -192.8249 CORAL_loss: 22.2440\n",
            "[4/33] class loss: -208.3727 CORAL_loss: 82.3646\n",
            "[5/33] class loss: -165.0557 CORAL_loss: 69.1211\n",
            "[6/33] class loss: -197.1086 CORAL_loss: 29.1820\n",
            "[7/33] class loss: -193.2470 CORAL_loss: 22.8809\n",
            "[8/33] class loss: -217.9226 CORAL_loss: 36.2702\n",
            "[9/33] class loss: -219.4940 CORAL_loss: 15.3016\n",
            "[10/33] class loss: -194.8765 CORAL_loss: 83.6542\n",
            "[11/33] class loss: -218.6024 CORAL_loss: 20.9847\n",
            "[12/33] class loss: -184.2292 CORAL_loss: 20.5250\n",
            "[13/33] class loss: -194.6692 CORAL_loss: 50.0342\n",
            "[14/33] class loss: -216.5733 CORAL_loss: 28.3147\n",
            "[15/33] class loss: -166.9664 CORAL_loss: 17.2282\n",
            "[16/33] class loss: -199.4928 CORAL_loss: 12.5818\n",
            "[17/33] class loss: -194.9578 CORAL_loss: 18.9921\n",
            "[18/33] class loss: -221.0238 CORAL_loss: 28.3539\n",
            "[19/33] class loss: -221.9767 CORAL_loss: 16.2779\n",
            "[20/33] class loss: -192.8288 CORAL_loss: 43.6325\n",
            "[21/33] class loss: -218.1641 CORAL_loss: 28.7772\n",
            "[22/33] class loss: -188.2790 CORAL_loss: 19.7215\n",
            "[23/33] class loss: -194.5895 CORAL_loss: 29.4781\n",
            "[24/33] class loss: -218.6111 CORAL_loss: 16.3879\n",
            "[25/33] class loss: -169.7161 CORAL_loss: 33.5651\n",
            "[26/33] class loss: -204.4789 CORAL_loss: 45.2240\n",
            "[27/33] class loss: -198.4871 CORAL_loss: 29.5837\n",
            "[28/33] class loss: -225.6595 CORAL_loss: 28.5752\n",
            "[29/33] class loss: -224.7275 CORAL_loss: 35.8586\n",
            "[30/33] class loss: -199.4926 CORAL_loss: 65.9124\n",
            "[31/33] class loss: -222.0473 CORAL_loss: 17.7414\n",
            "[32/33] class loss: -191.3821 CORAL_loss: 20.5855\n",
            "[33/33] class loss: -199.2926 CORAL_loss: 26.5026\n",
            "Epoch: 22 || Train_src_acc: 0.4682, Train_tar_acc: 0.1364, Train_loss: -168.8378\n",
            "Epoch 0023 / 0030\n",
            "============\n",
            "[1/33] class loss: -214.8850 CORAL_loss: 49.2074\n",
            "[2/33] class loss: -232.7017 CORAL_loss: 38.4108\n",
            "[3/33] class loss: -228.3883 CORAL_loss: 101.6990\n",
            "[4/33] class loss: -181.2609 CORAL_loss: 14.3768\n",
            "[5/33] class loss: -206.6207 CORAL_loss: 21.7278\n",
            "[6/33] class loss: -178.6515 CORAL_loss: 78.4550\n",
            "[7/33] class loss: -230.3115 CORAL_loss: 42.9906\n",
            "[8/33] class loss: -205.8255 CORAL_loss: 22.9246\n",
            "[9/33] class loss: -208.2513 CORAL_loss: 56.4628\n",
            "[10/33] class loss: -170.9421 CORAL_loss: 177.1065\n",
            "[11/33] class loss: -216.6011 CORAL_loss: 42.1043\n",
            "[12/33] class loss: -235.8324 CORAL_loss: 31.1370\n",
            "[13/33] class loss: -230.2486 CORAL_loss: 36.5122\n",
            "[14/33] class loss: -182.0495 CORAL_loss: 22.4218\n",
            "[15/33] class loss: -206.5558 CORAL_loss: 17.7095\n",
            "[16/33] class loss: -178.5683 CORAL_loss: 33.2753\n",
            "[17/33] class loss: -226.1898 CORAL_loss: 19.3863\n",
            "[18/33] class loss: -202.7335 CORAL_loss: 26.1618\n",
            "[19/33] class loss: -206.2045 CORAL_loss: 35.0323\n",
            "[20/33] class loss: -171.9229 CORAL_loss: 58.7515\n",
            "[21/33] class loss: -213.7605 CORAL_loss: 31.1911\n",
            "[22/33] class loss: -232.8068 CORAL_loss: 15.0601\n",
            "[23/33] class loss: -228.9109 CORAL_loss: 27.6866\n",
            "[24/33] class loss: -182.0325 CORAL_loss: 64.4608\n",
            "[25/33] class loss: -206.7897 CORAL_loss: 18.6113\n",
            "[26/33] class loss: -180.8509 CORAL_loss: 36.7264\n",
            "[27/33] class loss: -228.7482 CORAL_loss: 19.4095\n",
            "[28/33] class loss: -205.6524 CORAL_loss: 20.1394\n",
            "[29/33] class loss: -204.6151 CORAL_loss: 29.4174\n",
            "[30/33] class loss: -170.0828 CORAL_loss: 70.5092\n",
            "[31/33] class loss: -216.5048 CORAL_loss: 21.9732\n",
            "[32/33] class loss: -235.5370 CORAL_loss: 69.1650\n",
            "[33/33] class loss: -230.3340 CORAL_loss: 35.6504\n",
            "Epoch: 23 || Train_src_acc: 0.4763, Train_tar_acc: 0.1354, Train_loss: -165.6217\n",
            "Epoch 0024 / 0030\n",
            "============\n",
            "[1/33] class loss: -198.4456 CORAL_loss: 78.3956\n",
            "[2/33] class loss: -216.7936 CORAL_loss: 27.4791\n",
            "[3/33] class loss: -210.8516 CORAL_loss: 19.8031\n",
            "[4/33] class loss: -227.5168 CORAL_loss: 17.0655\n",
            "[5/33] class loss: -232.5369 CORAL_loss: 22.7476\n",
            "[6/33] class loss: -217.2083 CORAL_loss: 22.2989\n",
            "[7/33] class loss: -219.7022 CORAL_loss: 40.3410\n",
            "[8/33] class loss: -208.6445 CORAL_loss: 49.0524\n",
            "[9/33] class loss: -199.1322 CORAL_loss: 57.0391\n",
            "[10/33] class loss: -141.5280 CORAL_loss: 117.0435\n",
            "[11/33] class loss: -196.3862 CORAL_loss: 56.2183\n",
            "[12/33] class loss: -214.0445 CORAL_loss: 17.5835\n",
            "[13/33] class loss: -211.2761 CORAL_loss: 25.5968\n",
            "[14/33] class loss: -227.1969 CORAL_loss: 24.4809\n",
            "[15/33] class loss: -234.5482 CORAL_loss: 44.1923\n",
            "[16/33] class loss: -219.3849 CORAL_loss: 33.1561\n",
            "[17/33] class loss: -220.8905 CORAL_loss: 35.4626\n",
            "[18/33] class loss: -208.7719 CORAL_loss: 27.2834\n",
            "[19/33] class loss: -203.7904 CORAL_loss: 59.9959\n",
            "[20/33] class loss: -141.4684 CORAL_loss: 147.7374\n",
            "[21/33] class loss: -195.5281 CORAL_loss: 18.6120\n",
            "[22/33] class loss: -211.9610 CORAL_loss: 24.8599\n",
            "[23/33] class loss: -212.5065 CORAL_loss: 46.2203\n",
            "[24/33] class loss: -227.0287 CORAL_loss: 24.7263\n",
            "[25/33] class loss: -233.1942 CORAL_loss: 18.6162\n",
            "[26/33] class loss: -219.8477 CORAL_loss: 25.2474\n",
            "[27/33] class loss: -218.9969 CORAL_loss: 17.2662\n",
            "[28/33] class loss: -209.2967 CORAL_loss: 22.1858\n",
            "[29/33] class loss: -205.1864 CORAL_loss: 24.4657\n",
            "[30/33] class loss: -144.9091 CORAL_loss: 53.4313\n",
            "[31/33] class loss: -197.1196 CORAL_loss: 22.1027\n",
            "[32/33] class loss: -214.4640 CORAL_loss: 19.8835\n",
            "[33/33] class loss: -211.8992 CORAL_loss: 19.4123\n",
            "Epoch: 24 || Train_src_acc: 0.4611, Train_tar_acc: 0.1232, Train_loss: -169.4562\n",
            "Epoch 0025 / 0030\n",
            "============\n",
            "[1/33] class loss: -179.2441 CORAL_loss: 25.7938\n",
            "[2/33] class loss: -198.2085 CORAL_loss: 29.0970\n",
            "[3/33] class loss: -220.5542 CORAL_loss: 20.4316\n",
            "[4/33] class loss: -212.2827 CORAL_loss: 25.8234\n",
            "[5/33] class loss: -191.1365 CORAL_loss: 52.8385\n",
            "[6/33] class loss: -250.0149 CORAL_loss: 28.3762\n",
            "[7/33] class loss: -210.0933 CORAL_loss: 23.8719\n",
            "[8/33] class loss: -239.3429 CORAL_loss: 17.1242\n",
            "[9/33] class loss: -223.9342 CORAL_loss: 22.0538\n",
            "[10/33] class loss: -205.2586 CORAL_loss: 110.4769\n",
            "[11/33] class loss: -182.0256 CORAL_loss: 44.3977\n",
            "[12/33] class loss: -199.6656 CORAL_loss: 26.3910\n",
            "[13/33] class loss: -223.2513 CORAL_loss: 44.9044\n",
            "[14/33] class loss: -215.4497 CORAL_loss: 28.1945\n",
            "[15/33] class loss: -193.7285 CORAL_loss: 20.5460\n",
            "[16/33] class loss: -251.5931 CORAL_loss: 23.1569\n",
            "[17/33] class loss: -211.2522 CORAL_loss: 24.7245\n",
            "[18/33] class loss: -240.4521 CORAL_loss: 18.6812\n",
            "[19/33] class loss: -223.4732 CORAL_loss: 18.9827\n",
            "[20/33] class loss: -215.6100 CORAL_loss: 64.8784\n",
            "[21/33] class loss: -185.8375 CORAL_loss: 37.6018\n",
            "[22/33] class loss: -199.7460 CORAL_loss: 17.9864\n",
            "[23/33] class loss: -224.1132 CORAL_loss: 19.3568\n",
            "[24/33] class loss: -218.0272 CORAL_loss: 18.1044\n",
            "[25/33] class loss: -195.5067 CORAL_loss: 29.7168\n",
            "[26/33] class loss: -257.9176 CORAL_loss: 37.9951\n",
            "[27/33] class loss: -213.4224 CORAL_loss: 32.6093\n",
            "[28/33] class loss: -243.0809 CORAL_loss: 21.9476\n",
            "[29/33] class loss: -229.6840 CORAL_loss: 13.8490\n",
            "[30/33] class loss: -218.7308 CORAL_loss: 63.7177\n",
            "[31/33] class loss: -187.1149 CORAL_loss: 34.9415\n",
            "[32/33] class loss: -203.7552 CORAL_loss: 25.8314\n",
            "[33/33] class loss: -227.0097 CORAL_loss: 40.9590\n",
            "Epoch: 25 || Train_src_acc: 0.4586, Train_tar_acc: 0.1242, Train_loss: -182.5805\n",
            "Epoch 0026 / 0030\n",
            "============\n",
            "[1/33] class loss: -219.0553 CORAL_loss: 25.2096\n",
            "[2/33] class loss: -205.2373 CORAL_loss: 31.4547\n",
            "[3/33] class loss: -249.7204 CORAL_loss: 24.4416\n",
            "[4/33] class loss: -219.5101 CORAL_loss: 21.7508\n",
            "[5/33] class loss: -237.8522 CORAL_loss: 70.9445\n",
            "[6/33] class loss: -235.1502 CORAL_loss: 33.3592\n",
            "[7/33] class loss: -211.6911 CORAL_loss: 18.1798\n",
            "[8/33] class loss: -221.3140 CORAL_loss: 47.6266\n",
            "[9/33] class loss: -218.8640 CORAL_loss: 23.0993\n",
            "[10/33] class loss: -148.9720 CORAL_loss: 99.8975\n",
            "[11/33] class loss: -224.9225 CORAL_loss: 42.0324\n",
            "[12/33] class loss: -208.0619 CORAL_loss: 30.0854\n",
            "[13/33] class loss: -252.3220 CORAL_loss: 51.5423\n",
            "[14/33] class loss: -220.9933 CORAL_loss: 27.1088\n",
            "[15/33] class loss: -238.5079 CORAL_loss: 51.9885\n",
            "[16/33] class loss: -236.8620 CORAL_loss: 46.9875\n",
            "[17/33] class loss: -213.6109 CORAL_loss: 37.2852\n",
            "[18/33] class loss: -221.1090 CORAL_loss: 97.2296\n",
            "[19/33] class loss: -220.0868 CORAL_loss: 23.3398\n",
            "[20/33] class loss: -150.9044 CORAL_loss: 101.5164\n",
            "[21/33] class loss: -221.4548 CORAL_loss: 20.0710\n",
            "[22/33] class loss: -206.8463 CORAL_loss: 31.3753\n",
            "[23/33] class loss: -251.2364 CORAL_loss: 18.4012\n",
            "[24/33] class loss: -223.0262 CORAL_loss: 16.2779\n",
            "[25/33] class loss: -237.2421 CORAL_loss: 20.0499\n",
            "[26/33] class loss: -237.8585 CORAL_loss: 21.8899\n",
            "[27/33] class loss: -214.0773 CORAL_loss: 34.8115\n",
            "[28/33] class loss: -221.2109 CORAL_loss: 17.4495\n",
            "[29/33] class loss: -221.0305 CORAL_loss: 22.4865\n",
            "[30/33] class loss: -147.3141 CORAL_loss: 84.2289\n",
            "[31/33] class loss: -221.2259 CORAL_loss: 21.5606\n",
            "[32/33] class loss: -208.2670 CORAL_loss: 18.0304\n",
            "[33/33] class loss: -255.5530 CORAL_loss: 23.2089\n",
            "Epoch: 26 || Train_src_acc: 0.4495, Train_tar_acc: 0.1182, Train_loss: -180.7930\n",
            "Epoch 0027 / 0030\n",
            "============\n",
            "[1/33] class loss: -230.8791 CORAL_loss: 24.5490\n",
            "[2/33] class loss: -248.8545 CORAL_loss: 30.5240\n",
            "[3/33] class loss: -229.8195 CORAL_loss: 36.7557\n",
            "[4/33] class loss: -200.1974 CORAL_loss: 28.8264\n",
            "[5/33] class loss: -212.3659 CORAL_loss: 58.7086\n",
            "[6/33] class loss: -243.9325 CORAL_loss: 34.2222\n",
            "[7/33] class loss: -245.9394 CORAL_loss: 16.2830\n",
            "[8/33] class loss: -212.3276 CORAL_loss: 29.2312\n",
            "[9/33] class loss: -232.6118 CORAL_loss: 12.1891\n",
            "[10/33] class loss: -205.3902 CORAL_loss: 274.4181\n",
            "[11/33] class loss: -233.5088 CORAL_loss: 29.9709\n",
            "[12/33] class loss: -252.7943 CORAL_loss: 21.3532\n",
            "[13/33] class loss: -232.5193 CORAL_loss: 41.9524\n",
            "[14/33] class loss: -204.1668 CORAL_loss: 60.4006\n",
            "[15/33] class loss: -212.2725 CORAL_loss: 41.2119\n",
            "[16/33] class loss: -244.9393 CORAL_loss: 24.7291\n",
            "[17/33] class loss: -246.9779 CORAL_loss: 37.4484\n",
            "[18/33] class loss: -210.9477 CORAL_loss: 43.4149\n",
            "[19/33] class loss: -232.8476 CORAL_loss: 20.9334\n",
            "[20/33] class loss: -205.0500 CORAL_loss: 197.8368\n",
            "[21/33] class loss: -235.4036 CORAL_loss: 32.6545\n",
            "[22/33] class loss: -247.0078 CORAL_loss: 48.1943\n",
            "[23/33] class loss: -231.5840 CORAL_loss: 40.4823\n",
            "[24/33] class loss: -199.6894 CORAL_loss: 30.7531\n",
            "[25/33] class loss: -211.0754 CORAL_loss: 30.5341\n",
            "[26/33] class loss: -243.4297 CORAL_loss: 21.6799\n",
            "[27/33] class loss: -245.6683 CORAL_loss: 35.1585\n",
            "[28/33] class loss: -209.8228 CORAL_loss: 18.2971\n",
            "[29/33] class loss: -230.8466 CORAL_loss: 25.2316\n",
            "[30/33] class loss: -201.7393 CORAL_loss: 136.7328\n",
            "[31/33] class loss: -233.0589 CORAL_loss: 17.5121\n",
            "[32/33] class loss: -250.4348 CORAL_loss: 16.3835\n",
            "[33/33] class loss: -229.7164 CORAL_loss: 21.7854\n",
            "Epoch: 27 || Train_src_acc: 0.4475, Train_tar_acc: 0.1192, Train_loss: -180.8321\n",
            "Epoch 0028 / 0030\n",
            "============\n",
            "[1/33] class loss: -251.5311 CORAL_loss: 50.0937\n",
            "[2/33] class loss: -212.0479 CORAL_loss: 15.5056\n",
            "[3/33] class loss: -213.3414 CORAL_loss: 33.7711\n",
            "[4/33] class loss: -229.0841 CORAL_loss: 50.1016\n",
            "[5/33] class loss: -210.6050 CORAL_loss: 24.9446\n",
            "[6/33] class loss: -268.8530 CORAL_loss: 23.2645\n",
            "[7/33] class loss: -233.3930 CORAL_loss: 59.0086\n",
            "[8/33] class loss: -239.1028 CORAL_loss: 24.9247\n",
            "[9/33] class loss: -194.6872 CORAL_loss: 30.6853\n",
            "[10/33] class loss: -177.2969 CORAL_loss: 251.2844\n",
            "[11/33] class loss: -247.9921 CORAL_loss: 23.6509\n",
            "[12/33] class loss: -215.2150 CORAL_loss: 55.9598\n",
            "[13/33] class loss: -215.5932 CORAL_loss: 29.0275\n",
            "[14/33] class loss: -228.9395 CORAL_loss: 32.3464\n",
            "[15/33] class loss: -209.0100 CORAL_loss: 14.2842\n",
            "[16/33] class loss: -269.8330 CORAL_loss: 16.6740\n",
            "[17/33] class loss: -232.4328 CORAL_loss: 17.7303\n",
            "[18/33] class loss: -238.7372 CORAL_loss: 33.4880\n",
            "[19/33] class loss: -195.5597 CORAL_loss: 34.9858\n",
            "[20/33] class loss: -177.1730 CORAL_loss: 56.5491\n",
            "[21/33] class loss: -248.2153 CORAL_loss: 46.3248\n",
            "[22/33] class loss: -216.0651 CORAL_loss: 74.3933\n",
            "[23/33] class loss: -215.6862 CORAL_loss: 36.2112\n",
            "[24/33] class loss: -231.9113 CORAL_loss: 18.2390\n",
            "[25/33] class loss: -213.5473 CORAL_loss: 22.6125\n",
            "[26/33] class loss: -272.0992 CORAL_loss: 32.7238\n",
            "[27/33] class loss: -235.0863 CORAL_loss: 18.9286\n",
            "[28/33] class loss: -241.9944 CORAL_loss: 82.2533\n",
            "[29/33] class loss: -199.1341 CORAL_loss: 52.8064\n",
            "[30/33] class loss: -185.2968 CORAL_loss: 217.4577\n",
            "[31/33] class loss: -251.0332 CORAL_loss: 19.6781\n",
            "[32/33] class loss: -214.8222 CORAL_loss: 18.4205\n",
            "[33/33] class loss: -217.2135 CORAL_loss: 26.8035\n",
            "Epoch: 28 || Train_src_acc: 0.4318, Train_tar_acc: 0.1152, Train_loss: -177.4970\n",
            "Epoch 0029 / 0030\n",
            "============\n",
            "[1/33] class loss: -209.1414 CORAL_loss: 31.2792\n",
            "[2/33] class loss: -259.6117 CORAL_loss: 46.1764\n",
            "[3/33] class loss: -207.8181 CORAL_loss: 38.7547\n",
            "[4/33] class loss: -198.3431 CORAL_loss: 17.7430\n",
            "[5/33] class loss: -231.6538 CORAL_loss: 48.8671\n",
            "[6/33] class loss: -209.4604 CORAL_loss: 33.5736\n",
            "[7/33] class loss: -247.0215 CORAL_loss: 33.5652\n",
            "[8/33] class loss: -248.1092 CORAL_loss: 23.4463\n",
            "[9/33] class loss: -238.6620 CORAL_loss: 44.5944\n",
            "[10/33] class loss: -251.7850 CORAL_loss: 112.5217\n",
            "[11/33] class loss: -207.5232 CORAL_loss: 38.7644\n",
            "[12/33] class loss: -261.7211 CORAL_loss: 11.7492\n",
            "[13/33] class loss: -208.3068 CORAL_loss: 23.8025\n",
            "[14/33] class loss: -200.0274 CORAL_loss: 20.4788\n",
            "[15/33] class loss: -226.3029 CORAL_loss: 21.9896\n",
            "[16/33] class loss: -208.8678 CORAL_loss: 23.3890\n",
            "[17/33] class loss: -248.5587 CORAL_loss: 18.9966\n",
            "[18/33] class loss: -249.4499 CORAL_loss: 21.4645\n",
            "[19/33] class loss: -241.1433 CORAL_loss: 25.7632\n",
            "[20/33] class loss: -264.4360 CORAL_loss: 69.8293\n",
            "[21/33] class loss: -210.6817 CORAL_loss: 29.6625\n",
            "[22/33] class loss: -264.3371 CORAL_loss: 28.5511\n",
            "[23/33] class loss: -210.7548 CORAL_loss: 37.1573\n",
            "[24/33] class loss: -201.9019 CORAL_loss: 29.4357\n",
            "[25/33] class loss: -232.2053 CORAL_loss: 46.0846\n",
            "[26/33] class loss: -210.6703 CORAL_loss: 33.5485\n",
            "[27/33] class loss: -251.1649 CORAL_loss: 34.0184\n",
            "[28/33] class loss: -253.2318 CORAL_loss: 32.4871\n",
            "[29/33] class loss: -243.8074 CORAL_loss: 24.6556\n",
            "[30/33] class loss: -269.8480 CORAL_loss: 63.4175\n",
            "[31/33] class loss: -212.7870 CORAL_loss: 46.9016\n",
            "[32/33] class loss: -268.2184 CORAL_loss: 24.0981\n",
            "[33/33] class loss: -213.9339 CORAL_loss: 16.8392\n",
            "Epoch: 29 || Train_src_acc: 0.4232, Train_tar_acc: 0.1172, Train_loss: -197.2085\n",
            "Epoch 0030 / 0030\n",
            "============\n",
            "[1/33] class loss: -238.9473 CORAL_loss: 22.1057\n",
            "[2/33] class loss: -240.9493 CORAL_loss: 24.4966\n",
            "[3/33] class loss: -247.0068 CORAL_loss: 17.9444\n",
            "[4/33] class loss: -242.2100 CORAL_loss: 42.8868\n",
            "[5/33] class loss: -187.6800 CORAL_loss: 43.8480\n",
            "[6/33] class loss: -254.5513 CORAL_loss: 23.1772\n",
            "[7/33] class loss: -226.3988 CORAL_loss: 18.8454\n",
            "[8/33] class loss: -234.8172 CORAL_loss: 48.3421\n",
            "[9/33] class loss: -243.3843 CORAL_loss: 60.1031\n",
            "[10/33] class loss: -270.0496 CORAL_loss: 39.0501\n",
            "[11/33] class loss: -243.1404 CORAL_loss: 26.6604\n",
            "[12/33] class loss: -246.9784 CORAL_loss: 22.9673\n",
            "[13/33] class loss: -249.0624 CORAL_loss: 25.5111\n",
            "[14/33] class loss: -246.0619 CORAL_loss: 19.8569\n",
            "[15/33] class loss: -193.6556 CORAL_loss: 24.7024\n",
            "[16/33] class loss: -261.3380 CORAL_loss: 22.1008\n",
            "[17/33] class loss: -232.5994 CORAL_loss: 38.1805\n",
            "[18/33] class loss: -236.9139 CORAL_loss: 30.6928\n",
            "[19/33] class loss: -243.8421 CORAL_loss: 27.6228\n",
            "[20/33] class loss: -278.9512 CORAL_loss: 70.6049\n",
            "[21/33] class loss: -251.0497 CORAL_loss: 106.8755\n",
            "[22/33] class loss: -247.6583 CORAL_loss: 22.2816\n",
            "[23/33] class loss: -256.5163 CORAL_loss: 41.1629\n",
            "[24/33] class loss: -251.8459 CORAL_loss: 34.3039\n",
            "[25/33] class loss: -197.8922 CORAL_loss: 28.2354\n",
            "[26/33] class loss: -264.5556 CORAL_loss: 12.0472\n",
            "[27/33] class loss: -233.0439 CORAL_loss: 33.8737\n",
            "[28/33] class loss: -243.2990 CORAL_loss: 45.4144\n",
            "[29/33] class loss: -247.1393 CORAL_loss: 33.2268\n",
            "[30/33] class loss: -270.0415 CORAL_loss: 138.2861\n",
            "[31/33] class loss: -246.7869 CORAL_loss: 27.1496\n",
            "[32/33] class loss: -249.5661 CORAL_loss: 34.0768\n",
            "[33/33] class loss: -254.7966 CORAL_loss: 33.6989\n",
            "Epoch: 30 || Train_src_acc: 0.4369, Train_tar_acc: 0.1217, Train_loss: -205.8302\n"
          ]
        }
      ],
      "source": [
        "# Function to compute the CORAL loss\n",
        "class CorrelationAlignmentLoss(nn.Module):\n",
        "    def forward(self, source, target):\n",
        "        d = source.size(1)  # Number of features\n",
        "\n",
        "        # Compute covariance matrices\n",
        "        source_cov = (source.t() @ source) / (source.size(0) - 1)\n",
        "        target_cov = (target.t() @ target) / (target.size(0) - 1)\n",
        "\n",
        "        # Compute CORAL loss as the Frobenius norm between covariance matrices\n",
        "        coral_loss = torch.mean((source_cov - target_cov) ** 2) / (4 * d * d)\n",
        "\n",
        "        return coral_loss\n",
        "\n",
        "# Balancing dataloaders\n",
        "def create_balanced_dataloaders(src_dataloader, tar_dataloader, batch_size, blur_sigma=2):\n",
        "    src_size = len(src_dataloader.dataset)\n",
        "    tar_size = len(tar_dataloader.dataset)\n",
        "\n",
        "    # Define a blur transformation\n",
        "    blur_transform = transforms.GaussianBlur(kernel_size=(5, 5), sigma=blur_sigma)\n",
        "\n",
        "    def blur_images(batch):\n",
        "        images, labels = batch\n",
        "        # Apply blur to images\n",
        "        blurred_images = blur_transform(images)\n",
        "        return blurred_images, labels\n",
        "\n",
        "    # Ensure iterators are reset appropriately for smaller dataset\n",
        "    if src_size > tar_size:\n",
        "        Dl_source_iter = iter(src_dataloader)\n",
        "        Dl_target_iter = itertools.cycle(tar_dataloader)  # Cycle the smaller dataset\n",
        "        max_batches = len(src_dataloader)\n",
        "\n",
        "        # Apply blur to the cycled target images\n",
        "        Dl_target_iter = (blur_images(batch) for batch in Dl_target_iter)\n",
        "    else:\n",
        "        Dl_source_iter = itertools.cycle(src_dataloader)  # Cycle the smaller dataset\n",
        "        Dl_target_iter = iter(tar_dataloader)\n",
        "        max_batches = len(tar_dataloader)\n",
        "\n",
        "        # Apply blur to the cycled source images\n",
        "        Dl_source_iter = (blur_images(batch) for batch in Dl_source_iter)\n",
        "\n",
        "    return Dl_source_iter, Dl_target_iter, max_batches\n",
        "\n",
        "# CORAL training loop\n",
        "def train_model_with_balanced_batches_CORAL(model, optimizer, loss_fn_class, src, tar, device, num_epochs=30, batch_size=64, lambda_coral=0.01):\n",
        "    training_logs_coral = {\"train_loss\": [],  \"train_src_acc\": [], \"train_tar_acc\": []}\n",
        "\n",
        "\n",
        "    src_dataloader, tar_dataloader = get_loader_DA(src, tar)\n",
        "    model_save_path = f\"SO_{src}_to_{tar}.pth\"\n",
        "    best_vloss = 1_000_000.\n",
        "    for epoch_idx in range(num_epochs):\n",
        "        train_loss, train_src_correct, train_tar_correct = 0, 0, 0\n",
        "        actual_src_samples, actual_tar_samples = 0, 0\n",
        "        print(f'Epoch {epoch_idx+1:04d} / {num_epochs:04d}', end='\\n============\\n')\n",
        "\n",
        "        # Create balanced iterators\n",
        "        Dl_source_iter, Dl_target_iter, max_batches = create_balanced_dataloaders(src_dataloader, tar_dataloader, batch_size)\n",
        "\n",
        "        for batch_idx in range(max_batches):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get next batch from source and target\n",
        "            X_s, y_s = next(Dl_source_iter)\n",
        "            X_t, y_t = next(Dl_target_iter)\n",
        "\n",
        "            # Ensure the batches have the same size by taking the minimum batch size\n",
        "            if X_s.shape[0] != X_t.shape[0]:\n",
        "                min_bs = min(X_s.shape[0], X_t.shape[0])\n",
        "                X_s, y_s = X_s[:min_bs], y_s[:min_bs]\n",
        "                X_t, y_t = X_t[:min_bs], y_t[:min_bs]\n",
        "\n",
        "            # Track the number of samples processed\n",
        "            actual_src_samples += y_s.size(0)\n",
        "            actual_tar_samples += y_t.size(0)\n",
        "\n",
        "            # Send data to device (GPU or CPU)\n",
        "            X_s, y_s = X_s.to(device), y_s.to(device)\n",
        "            X_t, y_t = X_t.to(device), y_t.to(device)\n",
        "\n",
        "            # Forward pass for classification on the source domain\n",
        "            class_pred_s = model(X_s)\n",
        "\n",
        "            # Forward pass for target domain (used for CORAL loss)\n",
        "            class_pred_t = model(X_t)\n",
        "\n",
        "            # Compute source classification loss\n",
        "            loss_src = loss_fn_class(class_pred_s, y_s)\n",
        "\n",
        "            # Compute CORAL loss between source and target features\n",
        "            loss_coral = CorrelationAlignmentLoss()(class_pred_s, class_pred_t)\n",
        "\n",
        "            # Combine losses: classification + CORAL\n",
        "            loss = loss_src + lambda_coral * loss_coral\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute accuracy for source and target domains\n",
        "            with torch.no_grad():\n",
        "                class_prediction_s = model(X_s)\n",
        "                class_prediction_t = model(X_t)\n",
        "\n",
        "            train_src_correct += (class_prediction_s.argmax(1) == y_s).float().sum().item()\n",
        "            train_tar_correct += (class_prediction_t.argmax(1) == y_t).float().sum().item()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Print batch info\n",
        "            print(f'[{batch_idx+1}/{max_batches}] class loss: {loss_src.item():.4f} CORAL_loss: {loss_coral.item():.4f}')\n",
        "\n",
        "        # Calculate and store metrics for the epoch\n",
        "        training_logs_coral[\"train_loss\"].append(train_loss / max_batches)\n",
        "        training_logs_coral[\"train_src_acc\"].append(train_src_correct / actual_src_samples)\n",
        "        training_logs_coral[\"train_tar_acc\"].append(train_tar_correct / actual_tar_samples)\n",
        "\n",
        "        print(f'Epoch: {epoch_idx+1} || '\n",
        "              f'Train_src_acc: {train_src_correct / actual_src_samples:.4f}, '\n",
        "              f'Train_tar_acc: {train_tar_correct / actual_tar_samples:.4f}, '\n",
        "              f'Train_loss: {train_loss / max_batches:.4f}')\n",
        "        if train_loss < best_vloss:\n",
        "            best_vloss = train_loss\n",
        "            path_save_cp = './cp/'\n",
        "            if not os.path.exists(path_save_cp): os.mkdir(path_save_cp)\n",
        "            torch.save(model.state_dict(), path_save_cp+model_save_path)\n",
        "\n",
        "\n",
        "    save_training_logs(training_logs_coral, f\"LOG_CORAL_{src}_to_{tar}\")\n",
        "\n",
        "    return training_logs_coral\n",
        "\n",
        "model = models.mobilenet_v3_small(pretrained=True)\n",
        "print(\"Original Classifier:\", model.classifier)\n",
        "model.classifier[-1] = nn.Linear(in_features=1024, out_features=31, bias=True)\n",
        "print(\"Mod Classifier:\", model.classifier)\n",
        "model.to(device)\n",
        "summary(model.to('cpu'), input_size=(3, 224, 224), device='cpu')\n",
        "\n",
        "\n",
        "model.to('cuda')\n",
        "# shape and tensor test\n",
        "# x0_s, y0_s = next(iter(Dl_source))\n",
        "# x0_t, y0_t = next(iter(Dl_target))\n",
        "#\n",
        "# print('Source domain input: ', x0_s.shape, y0_s.shape)\n",
        "# print('Target domain input: ', x0_t.shape, y0_t.shape)\n",
        "#\n",
        "# # Test forward pass: get class prediction and domain prediction\n",
        "# y0_pred_s_class, y0_pred_s_domain = model(x0_s)\n",
        "# y0_pred_t_class, y0_pred_t_domain = model(x0_t)\n",
        "#\n",
        "# print('label prediction from classification: ', y0_pred_t_class, y0_pred_t_class.shape)\n",
        "# print('domain prediction from classification: ', y0_pred_t_domain, y0_pred_t_domain.shape)\n",
        "\n",
        "# Optimizer and cost function\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "# loss_fn_class = torch.nn.NLLLoss()\n",
        "# loss_fn_domain = torch.nn.NLLLoss()\n",
        "loss_fn_class = torch.nn.NLLLoss()\n",
        "\n",
        "plot_graph(train_model_with_balanced_batches_CORAL(model, optimizer, loss_fn_class, webcam_tl, amazon_tl, device, num_epochs=30, batch_size=32, lambda_coral=1.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TME271QwpVSn"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ulO_G3jqOQnP",
        "outputId": "c1a1eebe-634b-4958-88f8-4e67ee3eeb72"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-317e9b64d2d8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Optimizer and initial learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m  \u001b[0;31m# You can start with this low learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define a scheduler to reduce learning rate when there's no improvement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# Optimizer and initial learning rate\n",
        "lr = 1e-4  # You can start with this low learning rate\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Define a scheduler to reduce learning rate when there's no improvement\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "class CorrelationAlignmentLoss(nn.Module):\n",
        "    def forward(self, source, target):\n",
        "        d = source.size(1)  # Number of features\n",
        "\n",
        "        # Compute covariance matrices\n",
        "        source_cov = (source.t() @ source) / (source.size(0) - 1)\n",
        "        target_cov = (target.t() @ target) / (target.size(0) - 1)\n",
        "\n",
        "        # Compute CORAL loss as the Frobenius norm between covariance matrices\n",
        "        coral_loss = torch.mean((source_cov - target_cov) ** 2) / (4 * d * d)\n",
        "\n",
        "        return coral_loss\n",
        "\n",
        "\n",
        "# Function to adjust learning rate for warm-up\n",
        "def adjust_learning_rate(optimizer, epoch, lr, warmup_epochs=5):\n",
        "    \"\"\"Adjusts the learning rate during the warm-up phase.\"\"\"\n",
        "    if epoch < warmup_epochs:\n",
        "        new_lr = lr * (epoch + 1) / warmup_epochs  # Gradually increase LR\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = new_lr\n",
        "\n",
        "# CORAL training loop with LR fine-tuning\n",
        "def train_model_with_balanced_batches_CORAL(model, optimizer, loss_fn_class, src_dataloader, tar_dataloader, device, num_epochs=30, batch_size=32, lambda_coral=0.01, lr=1e-4):\n",
        "    training_logs_coral = {\"train_loss\": [],  \"train_src_acc\": [], \"train_tar_acc\": []}\n",
        "\n",
        "    for epoch_idx in range(num_epochs):\n",
        "        # Adjust the learning rate (warm-up phase for the first few epochs)\n",
        "        adjust_learning_rate(optimizer, epoch_idx, lr)\n",
        "\n",
        "        train_loss, train_src_correct, train_tar_correct = 0, 0, 0\n",
        "        actual_src_samples, actual_tar_samples = 0, 0\n",
        "        print(f'Epoch {epoch_idx+1:04d} / {num_epochs:04d}', end='\\n============\\n')\n",
        "\n",
        "        # Create balanced iterators\n",
        "        Dl_source_iter, Dl_target_iter, max_batches = create_balanced_dataloaders(src_dataloader, tar_dataloader, batch_size)\n",
        "\n",
        "        for batch_idx in range(max_batches):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get next batch from source and target\n",
        "            X_s, y_s = next(Dl_source_iter)\n",
        "            X_t, y_t = next(Dl_target_iter)\n",
        "\n",
        "            # Ensure the batches have the same size by taking the minimum batch size\n",
        "            if X_s.shape[0] != X_t.shape[0]:\n",
        "                min_bs = min(X_s.shape[0], X_t.shape[0])\n",
        "                X_s, y_s = X_s[:min_bs], y_s[:min_bs]\n",
        "                X_t, y_t = X_t[:min_bs], y_t[:min_bs]\n",
        "\n",
        "            # Track the number of samples processed\n",
        "            actual_src_samples += y_s.size(0)\n",
        "            actual_tar_samples += y_t.size(0)\n",
        "\n",
        "            # Send data to device (GPU or CPU)\n",
        "            X_s, y_s = X_s.to(device), y_s.to(device)\n",
        "            X_t, y_t = X_t.to(device), y_t.to(device)\n",
        "\n",
        "            features_s = model.features(X_s)\n",
        "            features_t = model.features(X_t)\n",
        "\n",
        "            # Forward pass for classification on the source domain\n",
        "            class_pred_s = model(X_s)\n",
        "\n",
        "            # Forward pass for target domain (used for CORAL loss)\n",
        "            class_pred_t = model(X_t)\n",
        "\n",
        "            # Compute source classification loss\n",
        "            loss_src = loss_fn_class(class_pred_s, y_s)\n",
        "\n",
        "            # Compute CORAL loss between source and target features\n",
        "            loss_coral = CorrelationAlignmentLoss()(features_s, features_t)\n",
        "\n",
        "            # Combine losses: classification + CORAL\n",
        "            loss = loss_src + lambda_coral * loss_coral\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute accuracy for source and target domains\n",
        "            with torch.no_grad():\n",
        "                class_prediction_s = model(X_s)\n",
        "                class_prediction_t = model(X_t)\n",
        "\n",
        "            train_src_correct += (class_prediction_s.argmax(1) == y_s).float().sum().item()\n",
        "            train_tar_correct += (class_prediction_t.argmax(1) == y_t).float().sum().item()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Print batch info\n",
        "            print(f'[{batch_idx+1}/{max_batches}] class loss: {loss_src.item():.4f} CORAL_loss: {loss_coral.item():.4f}')\n",
        "\n",
        "        # Calculate and store metrics for the epoch\n",
        "        training_logs_coral[\"train_loss\"].append(train_loss / max_batches)\n",
        "        training_logs_coral[\"train_src_acc\"].append(train_src_correct / actual_src_samples)\n",
        "        training_logs_coral[\"train_tar_acc\"].append(train_tar_correct / actual_tar_samples)\n",
        "\n",
        "        print(f'Epoch: {epoch_idx+1} || '\n",
        "              f'Train_src_acc: {train_src_correct / actual_src_samples:.4f}, '\n",
        "              f'Train_tar_acc: {train_tar_correct / actual_tar_samples:.4f}, '\n",
        "              f'Train_loss: {train_loss / max_batches:.4f}')\n",
        "\n",
        "        # Step the learning rate scheduler based on the current loss\n",
        "        scheduler.step(train_loss)\n",
        "\n",
        "    return training_logs_coral\n",
        "\n",
        "# Fine-tuned learning rate\n",
        "lr = 1e-4  # Lower base learning rate for fine-tuning\n",
        "model = models.mobilenet_v3_small(pretrained=True)\n",
        "model.classifier[-1] = nn.Linear(in_features=1024, out_features=31, bias=True)\n",
        "model.to(device)\n",
        "\n",
        "# Run training with fine-tuned learning rate\n",
        "train_model_with_balanced_batches_CORAL(model, optimizer, loss_fn_class, webcam_tl, amazon_tl, device, num_epochs=30, batch_size=32, lambda_coral=1.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsc6skLU277K"
      },
      "source": [
        "## Baseline Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGxyk5d_n6tf"
      },
      "source": [
        "### Training Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkTNhxP22_5M"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_baseline_model(model, optimizer, scheduler, loss_fn, sd,\n",
        "                         device, num_epochs=50, batch_size=64):\n",
        "    \"\"\"\n",
        "    Train the baseline model and log training/validation metrics.\n",
        "\n",
        "    Args:\n",
        "        model: The model to train.\n",
        "        optimizer: Optimizer for the model.\n",
        "        loss_fn: Loss function (e.g., CrossEntropyLoss).\n",
        "        training_loader: DataLoader for the training set.\n",
        "        validation_loader: DataLoader for the validation set.\n",
        "        device: Device to run the training on (e.g., 'cuda' or 'cpu').\n",
        "        num_epochs: Number of training epochs (default: 10).\n",
        "        batch_size: Batch size used in training (default: 64).\n",
        "        lr: Learning rate for the optimizer (default: 0.001).\n",
        "\n",
        "    Returns:\n",
        "        training_logs_student: A dictionary with training/validation loss and accuracy logs.\n",
        "    \"\"\"\n",
        "\n",
        "    training_loader, validation_loader = get_loader(sd)\n",
        "    t_0 = time.time()\n",
        "    best_vloss = 1_000_000.  # Initialize best validation loss to a large value\n",
        "    training_logs_student = {\"train_student_loss\": [], \"val_student_loss\": [], \"train_student_acc\": [], \"val_student_acc\": []}\n",
        "    model_name = f\"Baseline_{sd}.pth\"\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_correct = 0, 0\n",
        "        model.train()  # Set model to training mode\n",
        "        print(f'Epoch {epoch+1:04d} / {num_epochs:04d}', end='\\n============\\n')\n",
        "\n",
        "\n",
        "        for i, data in enumerate(training_loader):\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "            X, y = data[0].to(device), data[1].to(device)  # Send data to the device\n",
        "            yhat = model(X)  # Forward pass\n",
        "            loss = loss_fn(yhat, y)  # Calculate loss\n",
        "\n",
        "            loss.backward()  # Backward pass\n",
        "            optimizer.step()  # Optimize model parameters\n",
        "            scheduler.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                class_prediction = model(X)  # Predictions for accuracy calculation\n",
        "\n",
        "            print(f'[{i+1}/{len(training_loader)}] student class loss: {loss.item():.4f}')\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            train_correct += (class_prediction.argmax(1) == y).float().sum().item()\n",
        "\n",
        "        # Log training metrics\n",
        "        training_logs_student[\"train_student_loss\"].append(train_loss / len(training_loader))\n",
        "        training_logs_student[\"train_student_acc\"].append(train_correct / len(training_loader.dataset))\n",
        "\n",
        "        # Validation step\n",
        "        valid_loss, valid_correct = 0, 0\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            for i, vdata in enumerate(validation_loader):\n",
        "                vinputs, vlabels = vdata[0].to(device), vdata[1].to(device)\n",
        "                voutputs = model(vinputs)\n",
        "                vloss = loss_fn(voutputs, vlabels)\n",
        "                valid_loss += vloss.item()\n",
        "                valid_correct += (voutputs.argmax(1) == vlabels).float().sum().item()\n",
        "\n",
        "        # Log validation metrics\n",
        "        training_logs_student[\"val_student_loss\"].append(valid_loss / len(validation_loader))\n",
        "        training_logs_student[\"val_student_acc\"].append(valid_correct / len(validation_loader.dataset))\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f'Epochs {epoch+1}'.ljust(10),\n",
        "              f'train loss {training_logs_student[\"train_student_loss\"][-1]:.5f}',\n",
        "              f'train acc {training_logs_student[\"train_student_acc\"][-1]:.5f}',\n",
        "              f'validate loss {training_logs_student[\"val_student_loss\"][-1]:.5f}',\n",
        "              f'validate acc {training_logs_student[\"val_student_acc\"][-1]:.5f}',\n",
        "              )\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        # Save the best model based on validation loss\n",
        "        if valid_loss < best_vloss:\n",
        "            best_vloss = valid_loss\n",
        "            path_save_cp = './cp/'\n",
        "            if not os.path.exists(path_save_cp):\n",
        "                os.mkdir(path_save_cp)\n",
        "            torch.save(model.state_dict(), os.path.join(path_save_cp, model_name))\n",
        "\n",
        "    t_end = time.time() - t_0\n",
        "    print(f\"Time consumption for student net (device:{device}): {t_end} sec\")\n",
        "\n",
        "    # Save the training logs to a JSON file\n",
        "    save_training_logs(training_logs_student, f\"LOG_BL_{sd}\")\n",
        "\n",
        "\n",
        "    return training_logs_student"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfHVchzXZp1-",
        "outputId": "5a4fc627-31ce-43a3-c948-a3c3e78d800d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 16, 112, 112]             432\n",
            "       BatchNorm2d-2         [-1, 16, 112, 112]              32\n",
            "         Hardswish-3         [-1, 16, 112, 112]               0\n",
            "            Conv2d-4           [-1, 16, 56, 56]             144\n",
            "       BatchNorm2d-5           [-1, 16, 56, 56]              32\n",
            "              ReLU-6           [-1, 16, 56, 56]               0\n",
            " AdaptiveAvgPool2d-7             [-1, 16, 1, 1]               0\n",
            "            Conv2d-8              [-1, 8, 1, 1]             136\n",
            "              ReLU-9              [-1, 8, 1, 1]               0\n",
            "           Conv2d-10             [-1, 16, 1, 1]             144\n",
            "      Hardsigmoid-11             [-1, 16, 1, 1]               0\n",
            "SqueezeExcitation-12           [-1, 16, 56, 56]               0\n",
            "           Conv2d-13           [-1, 16, 56, 56]             256\n",
            "      BatchNorm2d-14           [-1, 16, 56, 56]              32\n",
            " InvertedResidual-15           [-1, 16, 56, 56]               0\n",
            "           Conv2d-16           [-1, 72, 56, 56]           1,152\n",
            "      BatchNorm2d-17           [-1, 72, 56, 56]             144\n",
            "             ReLU-18           [-1, 72, 56, 56]               0\n",
            "           Conv2d-19           [-1, 72, 28, 28]             648\n",
            "      BatchNorm2d-20           [-1, 72, 28, 28]             144\n",
            "             ReLU-21           [-1, 72, 28, 28]               0\n",
            "           Conv2d-22           [-1, 24, 28, 28]           1,728\n",
            "      BatchNorm2d-23           [-1, 24, 28, 28]              48\n",
            " InvertedResidual-24           [-1, 24, 28, 28]               0\n",
            "           Conv2d-25           [-1, 88, 28, 28]           2,112\n",
            "      BatchNorm2d-26           [-1, 88, 28, 28]             176\n",
            "             ReLU-27           [-1, 88, 28, 28]               0\n",
            "           Conv2d-28           [-1, 88, 28, 28]             792\n",
            "      BatchNorm2d-29           [-1, 88, 28, 28]             176\n",
            "             ReLU-30           [-1, 88, 28, 28]               0\n",
            "           Conv2d-31           [-1, 24, 28, 28]           2,112\n",
            "      BatchNorm2d-32           [-1, 24, 28, 28]              48\n",
            " InvertedResidual-33           [-1, 24, 28, 28]               0\n",
            "           Conv2d-34           [-1, 96, 28, 28]           2,304\n",
            "      BatchNorm2d-35           [-1, 96, 28, 28]             192\n",
            "        Hardswish-36           [-1, 96, 28, 28]               0\n",
            "           Conv2d-37           [-1, 96, 14, 14]           2,400\n",
            "      BatchNorm2d-38           [-1, 96, 14, 14]             192\n",
            "        Hardswish-39           [-1, 96, 14, 14]               0\n",
            "AdaptiveAvgPool2d-40             [-1, 96, 1, 1]               0\n",
            "           Conv2d-41             [-1, 24, 1, 1]           2,328\n",
            "             ReLU-42             [-1, 24, 1, 1]               0\n",
            "           Conv2d-43             [-1, 96, 1, 1]           2,400\n",
            "      Hardsigmoid-44             [-1, 96, 1, 1]               0\n",
            "SqueezeExcitation-45           [-1, 96, 14, 14]               0\n",
            "           Conv2d-46           [-1, 40, 14, 14]           3,840\n",
            "      BatchNorm2d-47           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-48           [-1, 40, 14, 14]               0\n",
            "           Conv2d-49          [-1, 240, 14, 14]           9,600\n",
            "      BatchNorm2d-50          [-1, 240, 14, 14]             480\n",
            "        Hardswish-51          [-1, 240, 14, 14]               0\n",
            "           Conv2d-52          [-1, 240, 14, 14]           6,000\n",
            "      BatchNorm2d-53          [-1, 240, 14, 14]             480\n",
            "        Hardswish-54          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-55            [-1, 240, 1, 1]               0\n",
            "           Conv2d-56             [-1, 64, 1, 1]          15,424\n",
            "             ReLU-57             [-1, 64, 1, 1]               0\n",
            "           Conv2d-58            [-1, 240, 1, 1]          15,600\n",
            "      Hardsigmoid-59            [-1, 240, 1, 1]               0\n",
            "SqueezeExcitation-60          [-1, 240, 14, 14]               0\n",
            "           Conv2d-61           [-1, 40, 14, 14]           9,600\n",
            "      BatchNorm2d-62           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-63           [-1, 40, 14, 14]               0\n",
            "           Conv2d-64          [-1, 240, 14, 14]           9,600\n",
            "      BatchNorm2d-65          [-1, 240, 14, 14]             480\n",
            "        Hardswish-66          [-1, 240, 14, 14]               0\n",
            "           Conv2d-67          [-1, 240, 14, 14]           6,000\n",
            "      BatchNorm2d-68          [-1, 240, 14, 14]             480\n",
            "        Hardswish-69          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-70            [-1, 240, 1, 1]               0\n",
            "           Conv2d-71             [-1, 64, 1, 1]          15,424\n",
            "             ReLU-72             [-1, 64, 1, 1]               0\n",
            "           Conv2d-73            [-1, 240, 1, 1]          15,600\n",
            "      Hardsigmoid-74            [-1, 240, 1, 1]               0\n",
            "SqueezeExcitation-75          [-1, 240, 14, 14]               0\n",
            "           Conv2d-76           [-1, 40, 14, 14]           9,600\n",
            "      BatchNorm2d-77           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-78           [-1, 40, 14, 14]               0\n",
            "           Conv2d-79          [-1, 120, 14, 14]           4,800\n",
            "      BatchNorm2d-80          [-1, 120, 14, 14]             240\n",
            "        Hardswish-81          [-1, 120, 14, 14]               0\n",
            "           Conv2d-82          [-1, 120, 14, 14]           3,000\n",
            "      BatchNorm2d-83          [-1, 120, 14, 14]             240\n",
            "        Hardswish-84          [-1, 120, 14, 14]               0\n",
            "AdaptiveAvgPool2d-85            [-1, 120, 1, 1]               0\n",
            "           Conv2d-86             [-1, 32, 1, 1]           3,872\n",
            "             ReLU-87             [-1, 32, 1, 1]               0\n",
            "           Conv2d-88            [-1, 120, 1, 1]           3,960\n",
            "      Hardsigmoid-89            [-1, 120, 1, 1]               0\n",
            "SqueezeExcitation-90          [-1, 120, 14, 14]               0\n",
            "           Conv2d-91           [-1, 48, 14, 14]           5,760\n",
            "      BatchNorm2d-92           [-1, 48, 14, 14]              96\n",
            " InvertedResidual-93           [-1, 48, 14, 14]               0\n",
            "           Conv2d-94          [-1, 144, 14, 14]           6,912\n",
            "      BatchNorm2d-95          [-1, 144, 14, 14]             288\n",
            "        Hardswish-96          [-1, 144, 14, 14]               0\n",
            "           Conv2d-97          [-1, 144, 14, 14]           3,600\n",
            "      BatchNorm2d-98          [-1, 144, 14, 14]             288\n",
            "        Hardswish-99          [-1, 144, 14, 14]               0\n",
            "AdaptiveAvgPool2d-100            [-1, 144, 1, 1]               0\n",
            "          Conv2d-101             [-1, 40, 1, 1]           5,800\n",
            "            ReLU-102             [-1, 40, 1, 1]               0\n",
            "          Conv2d-103            [-1, 144, 1, 1]           5,904\n",
            "     Hardsigmoid-104            [-1, 144, 1, 1]               0\n",
            "SqueezeExcitation-105          [-1, 144, 14, 14]               0\n",
            "          Conv2d-106           [-1, 48, 14, 14]           6,912\n",
            "     BatchNorm2d-107           [-1, 48, 14, 14]              96\n",
            "InvertedResidual-108           [-1, 48, 14, 14]               0\n",
            "          Conv2d-109          [-1, 288, 14, 14]          13,824\n",
            "     BatchNorm2d-110          [-1, 288, 14, 14]             576\n",
            "       Hardswish-111          [-1, 288, 14, 14]               0\n",
            "          Conv2d-112            [-1, 288, 7, 7]           7,200\n",
            "     BatchNorm2d-113            [-1, 288, 7, 7]             576\n",
            "       Hardswish-114            [-1, 288, 7, 7]               0\n",
            "AdaptiveAvgPool2d-115            [-1, 288, 1, 1]               0\n",
            "          Conv2d-116             [-1, 72, 1, 1]          20,808\n",
            "            ReLU-117             [-1, 72, 1, 1]               0\n",
            "          Conv2d-118            [-1, 288, 1, 1]          21,024\n",
            "     Hardsigmoid-119            [-1, 288, 1, 1]               0\n",
            "SqueezeExcitation-120            [-1, 288, 7, 7]               0\n",
            "          Conv2d-121             [-1, 96, 7, 7]          27,648\n",
            "     BatchNorm2d-122             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-123             [-1, 96, 7, 7]               0\n",
            "          Conv2d-124            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-125            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-126            [-1, 576, 7, 7]               0\n",
            "          Conv2d-127            [-1, 576, 7, 7]          14,400\n",
            "     BatchNorm2d-128            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-129            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-130            [-1, 576, 1, 1]               0\n",
            "          Conv2d-131            [-1, 144, 1, 1]          83,088\n",
            "            ReLU-132            [-1, 144, 1, 1]               0\n",
            "          Conv2d-133            [-1, 576, 1, 1]          83,520\n",
            "     Hardsigmoid-134            [-1, 576, 1, 1]               0\n",
            "SqueezeExcitation-135            [-1, 576, 7, 7]               0\n",
            "          Conv2d-136             [-1, 96, 7, 7]          55,296\n",
            "     BatchNorm2d-137             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-138             [-1, 96, 7, 7]               0\n",
            "          Conv2d-139            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-140            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-141            [-1, 576, 7, 7]               0\n",
            "          Conv2d-142            [-1, 576, 7, 7]          14,400\n",
            "     BatchNorm2d-143            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-144            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-145            [-1, 576, 1, 1]               0\n",
            "          Conv2d-146            [-1, 144, 1, 1]          83,088\n",
            "            ReLU-147            [-1, 144, 1, 1]               0\n",
            "          Conv2d-148            [-1, 576, 1, 1]          83,520\n",
            "     Hardsigmoid-149            [-1, 576, 1, 1]               0\n",
            "SqueezeExcitation-150            [-1, 576, 7, 7]               0\n",
            "          Conv2d-151             [-1, 96, 7, 7]          55,296\n",
            "     BatchNorm2d-152             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-153             [-1, 96, 7, 7]               0\n",
            "          Conv2d-154            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-155            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-156            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-157            [-1, 576, 1, 1]               0\n",
            "          Linear-158                 [-1, 1024]         590,848\n",
            "       Hardswish-159                 [-1, 1024]               0\n",
            "         Dropout-160                 [-1, 1024]               0\n",
            "          Linear-161                 [-1, 1000]       1,025,000\n",
            "================================================================\n",
            "Total params: 2,542,856\n",
            "Trainable params: 2,542,856\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 34.61\n",
            "Params size (MB): 9.70\n",
            "Estimated Total Size (MB): 44.88\n",
            "----------------------------------------------------------------\n",
            "Epoch 0001 / 0050\n",
            "============\n",
            "[1/33] student class loss: 11.1387\n",
            "[2/33] student class loss: 10.2724\n",
            "[3/33] student class loss: 11.1534\n",
            "[4/33] student class loss: 10.3997\n",
            "[5/33] student class loss: 10.3349\n",
            "[6/33] student class loss: 9.9961\n",
            "[7/33] student class loss: 10.1903\n",
            "[8/33] student class loss: 10.1648\n",
            "[9/33] student class loss: 9.4419\n",
            "[10/33] student class loss: 9.4883\n",
            "[11/33] student class loss: 9.4460\n",
            "[12/33] student class loss: 9.0677\n",
            "[13/33] student class loss: 8.8765\n",
            "[14/33] student class loss: 8.7217\n",
            "[15/33] student class loss: 8.8554\n",
            "[16/33] student class loss: 9.6015\n",
            "[17/33] student class loss: 8.7614\n",
            "[18/33] student class loss: 8.8411\n",
            "[19/33] student class loss: 7.9607\n",
            "[20/33] student class loss: 8.6767\n",
            "[21/33] student class loss: 7.9333\n",
            "[22/33] student class loss: 8.2074\n",
            "[23/33] student class loss: 8.2375\n",
            "[24/33] student class loss: 7.9188\n",
            "[25/33] student class loss: 8.1416\n",
            "[26/33] student class loss: 7.9777\n",
            "[27/33] student class loss: 7.7293\n",
            "[28/33] student class loss: 7.5667\n",
            "[29/33] student class loss: 7.4412\n",
            "[30/33] student class loss: 7.4175\n",
            "[31/33] student class loss: 7.3623\n",
            "[32/33] student class loss: 7.1527\n",
            "[33/33] student class loss: 6.8390\n",
            "Epochs 1   train loss 8.82770 train acc 0.00663 validate loss 8.61006 validate acc 0.00567\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0002 / 0050\n",
            "============\n",
            "[1/33] student class loss: 6.2482\n",
            "[2/33] student class loss: 6.2703\n",
            "[3/33] student class loss: 7.0165\n",
            "[4/33] student class loss: 5.9899\n",
            "[5/33] student class loss: 5.6088\n",
            "[6/33] student class loss: 5.9839\n",
            "[7/33] student class loss: 6.1351\n",
            "[8/33] student class loss: 6.1376\n",
            "[9/33] student class loss: 5.1468\n",
            "[10/33] student class loss: 5.2437\n",
            "[11/33] student class loss: 5.5798\n",
            "[12/33] student class loss: 5.5228\n",
            "[13/33] student class loss: 5.3176\n",
            "[14/33] student class loss: 5.1355\n",
            "[15/33] student class loss: 5.4853\n",
            "[16/33] student class loss: 4.8143\n",
            "[17/33] student class loss: 4.9885\n",
            "[18/33] student class loss: 4.5165\n",
            "[19/33] student class loss: 4.4631\n",
            "[20/33] student class loss: 4.6552\n",
            "[21/33] student class loss: 4.7613\n",
            "[22/33] student class loss: 4.3756\n",
            "[23/33] student class loss: 4.4832\n",
            "[24/33] student class loss: 4.6839\n",
            "[25/33] student class loss: 4.9423\n",
            "[26/33] student class loss: 4.2438\n",
            "[27/33] student class loss: 4.2474\n",
            "[28/33] student class loss: 4.8643\n",
            "[29/33] student class loss: 4.1028\n",
            "[30/33] student class loss: 4.4949\n",
            "[31/33] student class loss: 4.4759\n",
            "[32/33] student class loss: 4.7380\n",
            "[33/33] student class loss: 3.7732\n",
            "Epochs 2   train loss 5.10442 train acc 0.10133 validate loss 4.29059 validate acc 0.17589\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0003 / 0050\n",
            "============\n",
            "[1/33] student class loss: 3.7441\n",
            "[2/33] student class loss: 3.4973\n",
            "[3/33] student class loss: 3.5285\n",
            "[4/33] student class loss: 3.8090\n",
            "[5/33] student class loss: 3.6465\n",
            "[6/33] student class loss: 3.9739\n",
            "[7/33] student class loss: 3.6350\n",
            "[8/33] student class loss: 3.6478\n",
            "[9/33] student class loss: 3.6553\n",
            "[10/33] student class loss: 3.3223\n",
            "[11/33] student class loss: 3.0609\n",
            "[12/33] student class loss: 3.1019\n",
            "[13/33] student class loss: 3.2285\n",
            "[14/33] student class loss: 2.8153\n",
            "[15/33] student class loss: 2.8042\n",
            "[16/33] student class loss: 3.1905\n",
            "[17/33] student class loss: 2.5246\n",
            "[18/33] student class loss: 3.2401\n",
            "[19/33] student class loss: 2.9077\n",
            "[20/33] student class loss: 2.8785\n",
            "[21/33] student class loss: 3.5025\n",
            "[22/33] student class loss: 3.2816\n",
            "[23/33] student class loss: 2.6930\n",
            "[24/33] student class loss: 2.7429\n",
            "[25/33] student class loss: 2.8770\n",
            "[26/33] student class loss: 2.5714\n",
            "[27/33] student class loss: 2.4789\n",
            "[28/33] student class loss: 2.6063\n",
            "[29/33] student class loss: 2.5189\n",
            "[30/33] student class loss: 2.6578\n",
            "[31/33] student class loss: 2.5522\n",
            "[32/33] student class loss: 2.5112\n",
            "[33/33] student class loss: 2.9266\n",
            "Epochs 3   train loss 3.09492 train acc 0.29261 validate loss 2.52427 validate acc 0.36738\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0004 / 0050\n",
            "============\n",
            "[1/33] student class loss: 2.3620\n",
            "[2/33] student class loss: 2.3861\n",
            "[3/33] student class loss: 2.1849\n",
            "[4/33] student class loss: 1.9808\n",
            "[5/33] student class loss: 1.7854\n",
            "[6/33] student class loss: 2.1808\n",
            "[7/33] student class loss: 1.9347\n",
            "[8/33] student class loss: 2.2610\n",
            "[9/33] student class loss: 1.8373\n",
            "[10/33] student class loss: 2.0495\n",
            "[11/33] student class loss: 1.9773\n",
            "[12/33] student class loss: 1.8559\n",
            "[13/33] student class loss: 1.8532\n",
            "[14/33] student class loss: 2.0339\n",
            "[15/33] student class loss: 2.0133\n",
            "[16/33] student class loss: 2.0822\n",
            "[17/33] student class loss: 1.5036\n",
            "[18/33] student class loss: 1.7784\n",
            "[19/33] student class loss: 1.7618\n",
            "[20/33] student class loss: 2.0964\n",
            "[21/33] student class loss: 1.8131\n",
            "[22/33] student class loss: 1.8272\n",
            "[23/33] student class loss: 1.5989\n",
            "[24/33] student class loss: 1.4081\n",
            "[25/33] student class loss: 1.5893\n",
            "[26/33] student class loss: 1.8480\n",
            "[27/33] student class loss: 1.4518\n",
            "[28/33] student class loss: 1.8162\n",
            "[29/33] student class loss: 1.2806\n",
            "[30/33] student class loss: 1.9350\n",
            "[31/33] student class loss: 1.8153\n",
            "[32/33] student class loss: 1.6202\n",
            "[33/33] student class loss: 1.2906\n",
            "Epochs 4   train loss 1.85494 train acc 0.53172 validate loss 1.67735 validate acc 0.55461\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0005 / 0050\n",
            "============\n",
            "[1/33] student class loss: 1.5620\n",
            "[2/33] student class loss: 1.1753\n",
            "[3/33] student class loss: 1.3637\n",
            "[4/33] student class loss: 1.1572\n",
            "[5/33] student class loss: 1.0507\n",
            "[6/33] student class loss: 1.1457\n",
            "[7/33] student class loss: 1.0394\n",
            "[8/33] student class loss: 1.3045\n",
            "[9/33] student class loss: 0.9708\n",
            "[10/33] student class loss: 1.5497\n",
            "[11/33] student class loss: 1.2994\n",
            "[12/33] student class loss: 1.2209\n",
            "[13/33] student class loss: 1.1733\n",
            "[14/33] student class loss: 1.2681\n",
            "[15/33] student class loss: 1.0149\n",
            "[16/33] student class loss: 1.3402\n",
            "[17/33] student class loss: 1.0824\n",
            "[18/33] student class loss: 0.9747\n",
            "[19/33] student class loss: 0.9413\n",
            "[20/33] student class loss: 1.1640\n",
            "[21/33] student class loss: 1.4098\n",
            "[22/33] student class loss: 1.0230\n",
            "[23/33] student class loss: 1.0455\n",
            "[24/33] student class loss: 0.8974\n",
            "[25/33] student class loss: 0.9978\n",
            "[26/33] student class loss: 0.7883\n",
            "[27/33] student class loss: 1.1073\n",
            "[28/33] student class loss: 1.0091\n",
            "[29/33] student class loss: 1.0659\n",
            "[30/33] student class loss: 0.9245\n",
            "[31/33] student class loss: 0.9191\n",
            "[32/33] student class loss: 0.9241\n",
            "[33/33] student class loss: 1.0723\n",
            "Epochs 5   train loss 1.12068 train acc 0.71496 validate loss 1.27093 validate acc 0.66099\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0006 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.9618\n",
            "[2/33] student class loss: 0.6681\n",
            "[3/33] student class loss: 0.6279\n",
            "[4/33] student class loss: 0.5647\n",
            "[5/33] student class loss: 0.6874\n",
            "[6/33] student class loss: 0.8684\n",
            "[7/33] student class loss: 0.6458\n",
            "[8/33] student class loss: 0.5885\n",
            "[9/33] student class loss: 0.8688\n",
            "[10/33] student class loss: 0.8437\n",
            "[11/33] student class loss: 0.6872\n",
            "[12/33] student class loss: 0.5048\n",
            "[13/33] student class loss: 0.7741\n",
            "[14/33] student class loss: 0.5471\n",
            "[15/33] student class loss: 0.7682\n",
            "[16/33] student class loss: 0.4622\n",
            "[17/33] student class loss: 0.5629\n",
            "[18/33] student class loss: 0.6507\n",
            "[19/33] student class loss: 0.6684\n",
            "[20/33] student class loss: 0.7838\n",
            "[21/33] student class loss: 0.5921\n",
            "[22/33] student class loss: 0.7956\n",
            "[23/33] student class loss: 0.7322\n",
            "[24/33] student class loss: 0.6324\n",
            "[25/33] student class loss: 0.8263\n",
            "[26/33] student class loss: 0.9630\n",
            "[27/33] student class loss: 0.9247\n",
            "[28/33] student class loss: 0.6954\n",
            "[29/33] student class loss: 0.6193\n",
            "[30/33] student class loss: 0.6434\n",
            "[31/33] student class loss: 0.7207\n",
            "[32/33] student class loss: 0.8449\n",
            "[33/33] student class loss: 0.5188\n",
            "Epochs 6   train loss 0.70434 train acc 0.83523 validate loss 1.06629 validate acc 0.70355\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0007 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.4534\n",
            "[2/33] student class loss: 0.3080\n",
            "[3/33] student class loss: 0.4172\n",
            "[4/33] student class loss: 0.3101\n",
            "[5/33] student class loss: 0.4734\n",
            "[6/33] student class loss: 0.4763\n",
            "[7/33] student class loss: 0.2227\n",
            "[8/33] student class loss: 0.5949\n",
            "[9/33] student class loss: 0.3526\n",
            "[10/33] student class loss: 0.4677\n",
            "[11/33] student class loss: 0.3465\n",
            "[12/33] student class loss: 0.3966\n",
            "[13/33] student class loss: 0.3076\n",
            "[14/33] student class loss: 0.5355\n",
            "[15/33] student class loss: 0.3733\n",
            "[16/33] student class loss: 0.4792\n",
            "[17/33] student class loss: 0.2934\n",
            "[18/33] student class loss: 0.3942\n",
            "[19/33] student class loss: 0.4660\n",
            "[20/33] student class loss: 0.4202\n",
            "[21/33] student class loss: 0.1819\n",
            "[22/33] student class loss: 0.3635\n",
            "[23/33] student class loss: 0.4755\n",
            "[24/33] student class loss: 0.5273\n",
            "[25/33] student class loss: 0.3228\n",
            "[26/33] student class loss: 0.4101\n",
            "[27/33] student class loss: 0.3947\n",
            "[28/33] student class loss: 0.4809\n",
            "[29/33] student class loss: 0.3923\n",
            "[30/33] student class loss: 0.4019\n",
            "[31/33] student class loss: 0.6448\n",
            "[32/33] student class loss: 0.4889\n",
            "[33/33] student class loss: 0.4141\n",
            "Epochs 7   train loss 0.41174 train acc 0.90625 validate loss 1.00841 validate acc 0.71489\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0008 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.2672\n",
            "[2/33] student class loss: 0.1299\n",
            "[3/33] student class loss: 0.1605\n",
            "[4/33] student class loss: 0.2155\n",
            "[5/33] student class loss: 0.1812\n",
            "[6/33] student class loss: 0.1406\n",
            "[7/33] student class loss: 0.2688\n",
            "[8/33] student class loss: 0.2211\n",
            "[9/33] student class loss: 0.2541\n",
            "[10/33] student class loss: 0.1483\n",
            "[11/33] student class loss: 0.2217\n",
            "[12/33] student class loss: 0.2899\n",
            "[13/33] student class loss: 0.1881\n",
            "[14/33] student class loss: 0.2680\n",
            "[15/33] student class loss: 0.2494\n",
            "[16/33] student class loss: 0.1775\n",
            "[17/33] student class loss: 0.3045\n",
            "[18/33] student class loss: 0.2315\n",
            "[19/33] student class loss: 0.2295\n",
            "[20/33] student class loss: 0.1507\n",
            "[21/33] student class loss: 0.1917\n",
            "[22/33] student class loss: 0.2140\n",
            "[23/33] student class loss: 0.1907\n",
            "[24/33] student class loss: 0.2754\n",
            "[25/33] student class loss: 0.2085\n",
            "[26/33] student class loss: 0.2618\n",
            "[27/33] student class loss: 0.2706\n",
            "[28/33] student class loss: 0.2280\n",
            "[29/33] student class loss: 0.1876\n",
            "[30/33] student class loss: 0.2492\n",
            "[31/33] student class loss: 0.2786\n",
            "[32/33] student class loss: 0.1960\n",
            "[33/33] student class loss: 0.2269\n",
            "Epochs 8   train loss 0.22052 train acc 0.96780 validate loss 1.03983 validate acc 0.71206\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0009 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.2195\n",
            "[2/33] student class loss: 0.1265\n",
            "[3/33] student class loss: 0.0505\n",
            "[4/33] student class loss: 0.0489\n",
            "[5/33] student class loss: 0.1171\n",
            "[6/33] student class loss: 0.1179\n",
            "[7/33] student class loss: 0.1765\n",
            "[8/33] student class loss: 0.0714\n",
            "[9/33] student class loss: 0.0485\n",
            "[10/33] student class loss: 0.1203\n",
            "[11/33] student class loss: 0.1294\n",
            "[12/33] student class loss: 0.0617\n",
            "[13/33] student class loss: 0.1143\n",
            "[14/33] student class loss: 0.1024\n",
            "[15/33] student class loss: 0.1112\n",
            "[16/33] student class loss: 0.1279\n",
            "[17/33] student class loss: 0.0684\n",
            "[18/33] student class loss: 0.0831\n",
            "[19/33] student class loss: 0.1146\n",
            "[20/33] student class loss: 0.1573\n",
            "[21/33] student class loss: 0.1249\n",
            "[22/33] student class loss: 0.0804\n",
            "[23/33] student class loss: 0.1083\n",
            "[24/33] student class loss: 0.1166\n",
            "[25/33] student class loss: 0.0436\n",
            "[26/33] student class loss: 0.1223\n",
            "[27/33] student class loss: 0.1533\n",
            "[28/33] student class loss: 0.1602\n",
            "[29/33] student class loss: 0.2599\n",
            "[30/33] student class loss: 0.1064\n",
            "[31/33] student class loss: 0.1711\n",
            "[32/33] student class loss: 0.0904\n",
            "[33/33] student class loss: 0.1433\n",
            "Epochs 9   train loss 0.11661 train acc 0.98674 validate loss 0.92570 validate acc 0.76454\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0010 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0734\n",
            "[2/33] student class loss: 0.1262\n",
            "[3/33] student class loss: 0.1089\n",
            "[4/33] student class loss: 0.0608\n",
            "[5/33] student class loss: 0.0423\n",
            "[6/33] student class loss: 0.0443\n",
            "[7/33] student class loss: 0.0718\n",
            "[8/33] student class loss: 0.0712\n",
            "[9/33] student class loss: 0.0232\n",
            "[10/33] student class loss: 0.1421\n",
            "[11/33] student class loss: 0.0945\n",
            "[12/33] student class loss: 0.1173\n",
            "[13/33] student class loss: 0.1073\n",
            "[14/33] student class loss: 0.0677\n",
            "[15/33] student class loss: 0.0659\n",
            "[16/33] student class loss: 0.0672\n",
            "[17/33] student class loss: 0.0548\n",
            "[18/33] student class loss: 0.0630\n",
            "[19/33] student class loss: 0.0277\n",
            "[20/33] student class loss: 0.0499\n",
            "[21/33] student class loss: 0.0752\n",
            "[22/33] student class loss: 0.0510\n",
            "[23/33] student class loss: 0.0648\n",
            "[24/33] student class loss: 0.0547\n",
            "[25/33] student class loss: 0.0645\n",
            "[26/33] student class loss: 0.1008\n",
            "[27/33] student class loss: 0.0652\n",
            "[28/33] student class loss: 0.0592\n",
            "[29/33] student class loss: 0.0962\n",
            "[30/33] student class loss: 0.0731\n",
            "[31/33] student class loss: 0.0908\n",
            "[32/33] student class loss: 0.1849\n",
            "[33/33] student class loss: 0.0760\n",
            "Epochs 10  train loss 0.07685 train acc 0.99290 validate loss 1.05360 validate acc 0.74752\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0011 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0143\n",
            "[2/33] student class loss: 0.0433\n",
            "[3/33] student class loss: 0.0727\n",
            "[4/33] student class loss: 0.0256\n",
            "[5/33] student class loss: 0.0470\n",
            "[6/33] student class loss: 0.0685\n",
            "[7/33] student class loss: 0.0573\n",
            "[8/33] student class loss: 0.0394\n",
            "[9/33] student class loss: 0.1337\n",
            "[10/33] student class loss: 0.0679\n",
            "[11/33] student class loss: 0.0426\n",
            "[12/33] student class loss: 0.0272\n",
            "[13/33] student class loss: 0.0260\n",
            "[14/33] student class loss: 0.0401\n",
            "[15/33] student class loss: 0.1075\n",
            "[16/33] student class loss: 0.0434\n",
            "[17/33] student class loss: 0.0516\n",
            "[18/33] student class loss: 0.0483\n",
            "[19/33] student class loss: 0.0803\n",
            "[20/33] student class loss: 0.0410\n",
            "[21/33] student class loss: 0.0495\n",
            "[22/33] student class loss: 0.0375\n",
            "[23/33] student class loss: 0.0459\n",
            "[24/33] student class loss: 0.0364\n",
            "[25/33] student class loss: 0.0482\n",
            "[26/33] student class loss: 0.0476\n",
            "[27/33] student class loss: 0.0861\n",
            "[28/33] student class loss: 0.0601\n",
            "[29/33] student class loss: 0.0464\n",
            "[30/33] student class loss: 0.0530\n",
            "[31/33] student class loss: 0.1089\n",
            "[32/33] student class loss: 0.0870\n",
            "[33/33] student class loss: 0.0838\n",
            "Epochs 11  train loss 0.05662 train acc 0.99669 validate loss 1.01092 validate acc 0.75745\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0012 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0348\n",
            "[2/33] student class loss: 0.0272\n",
            "[3/33] student class loss: 0.1009\n",
            "[4/33] student class loss: 0.0414\n",
            "[5/33] student class loss: 0.0349\n",
            "[6/33] student class loss: 0.0189\n",
            "[7/33] student class loss: 0.0772\n",
            "[8/33] student class loss: 0.0766\n",
            "[9/33] student class loss: 0.0636\n",
            "[10/33] student class loss: 0.0543\n",
            "[11/33] student class loss: 0.0289\n",
            "[12/33] student class loss: 0.0325\n",
            "[13/33] student class loss: 0.1573\n",
            "[14/33] student class loss: 0.0278\n",
            "[15/33] student class loss: 0.0831\n",
            "[16/33] student class loss: 0.0671\n",
            "[17/33] student class loss: 0.0715\n",
            "[18/33] student class loss: 0.0891\n",
            "[19/33] student class loss: 0.0628\n",
            "[20/33] student class loss: 0.1023\n",
            "[21/33] student class loss: 0.0527\n",
            "[22/33] student class loss: 0.1561\n",
            "[23/33] student class loss: 0.0163\n",
            "[24/33] student class loss: 0.0944\n",
            "[25/33] student class loss: 0.0762\n",
            "[26/33] student class loss: 0.1142\n",
            "[27/33] student class loss: 0.0495\n",
            "[28/33] student class loss: 0.1490\n",
            "[29/33] student class loss: 0.1276\n",
            "[30/33] student class loss: 0.1186\n",
            "[31/33] student class loss: 0.1684\n",
            "[32/33] student class loss: 0.1351\n",
            "[33/33] student class loss: 0.0881\n",
            "Epochs 12  train loss 0.07874 train acc 0.99716 validate loss 1.18122 validate acc 0.70638\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0013 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0588\n",
            "[2/33] student class loss: 0.1186\n",
            "[3/33] student class loss: 0.0602\n",
            "[4/33] student class loss: 0.0766\n",
            "[5/33] student class loss: 0.1608\n",
            "[6/33] student class loss: 0.1229\n",
            "[7/33] student class loss: 0.0853\n",
            "[8/33] student class loss: 0.0720\n",
            "[9/33] student class loss: 0.1580\n",
            "[10/33] student class loss: 0.1084\n",
            "[11/33] student class loss: 0.0583\n",
            "[12/33] student class loss: 0.1178\n",
            "[13/33] student class loss: 0.0632\n",
            "[14/33] student class loss: 0.0664\n",
            "[15/33] student class loss: 0.1698\n",
            "[16/33] student class loss: 0.2133\n",
            "[17/33] student class loss: 0.1422\n",
            "[18/33] student class loss: 0.1812\n",
            "[19/33] student class loss: 0.1389\n",
            "[20/33] student class loss: 0.1177\n",
            "[21/33] student class loss: 0.1123\n",
            "[22/33] student class loss: 0.0638\n",
            "[23/33] student class loss: 0.1107\n",
            "[24/33] student class loss: 0.2934\n",
            "[25/33] student class loss: 0.1403\n",
            "[26/33] student class loss: 0.1470\n",
            "[27/33] student class loss: 0.1085\n",
            "[28/33] student class loss: 0.3580\n",
            "[29/33] student class loss: 0.4735\n",
            "[30/33] student class loss: 0.3194\n",
            "[31/33] student class loss: 0.1858\n",
            "[32/33] student class loss: 0.1016\n",
            "[33/33] student class loss: 0.1635\n",
            "Epochs 13  train loss 0.14752 train acc 0.99290 validate loss 1.11362 validate acc 0.75461\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0014 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.1663\n",
            "[2/33] student class loss: 0.2390\n",
            "[3/33] student class loss: 0.1153\n",
            "[4/33] student class loss: 0.0791\n",
            "[5/33] student class loss: 0.1566\n",
            "[6/33] student class loss: 0.0644\n",
            "[7/33] student class loss: 0.2009\n",
            "[8/33] student class loss: 0.0818\n",
            "[9/33] student class loss: 0.2217\n",
            "[10/33] student class loss: 0.2377\n",
            "[11/33] student class loss: 0.1359\n",
            "[12/33] student class loss: 0.1946\n",
            "[13/33] student class loss: 0.0661\n",
            "[14/33] student class loss: 0.0864\n",
            "[15/33] student class loss: 0.3163\n",
            "[16/33] student class loss: 0.2927\n",
            "[17/33] student class loss: 0.0828\n",
            "[18/33] student class loss: 0.2351\n",
            "[19/33] student class loss: 0.1366\n",
            "[20/33] student class loss: 0.2060\n",
            "[21/33] student class loss: 0.3396\n",
            "[22/33] student class loss: 0.0845\n",
            "[23/33] student class loss: 0.4188\n",
            "[24/33] student class loss: 0.1027\n",
            "[25/33] student class loss: 0.0613\n",
            "[26/33] student class loss: 0.2326\n",
            "[27/33] student class loss: 0.0911\n",
            "[28/33] student class loss: 0.4922\n",
            "[29/33] student class loss: 0.1933\n",
            "[30/33] student class loss: 0.3186\n",
            "[31/33] student class loss: 0.2952\n",
            "[32/33] student class loss: 0.1904\n",
            "[33/33] student class loss: 0.0805\n",
            "Epochs 14  train loss 0.18836 train acc 0.98674 validate loss 1.58946 validate acc 0.66950\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0015 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.2061\n",
            "[2/33] student class loss: 0.1517\n",
            "[3/33] student class loss: 0.1578\n",
            "[4/33] student class loss: 0.1452\n",
            "[5/33] student class loss: 0.1229\n",
            "[6/33] student class loss: 0.2696\n",
            "[7/33] student class loss: 0.1739\n",
            "[8/33] student class loss: 0.2025\n",
            "[9/33] student class loss: 0.1038\n",
            "[10/33] student class loss: 0.1225\n",
            "[11/33] student class loss: 0.1260\n",
            "[12/33] student class loss: 0.1857\n",
            "[13/33] student class loss: 0.1604\n",
            "[14/33] student class loss: 0.0686\n",
            "[15/33] student class loss: 0.4211\n",
            "[16/33] student class loss: 0.0909\n",
            "[17/33] student class loss: 0.1649\n",
            "[18/33] student class loss: 0.1132\n",
            "[19/33] student class loss: 0.0777\n",
            "[20/33] student class loss: 0.1891\n",
            "[21/33] student class loss: 0.0882\n",
            "[22/33] student class loss: 0.0641\n",
            "[23/33] student class loss: 0.1304\n",
            "[24/33] student class loss: 0.2612\n",
            "[25/33] student class loss: 0.0613\n",
            "[26/33] student class loss: 0.3180\n",
            "[27/33] student class loss: 0.1544\n",
            "[28/33] student class loss: 0.1343\n",
            "[29/33] student class loss: 0.0923\n",
            "[30/33] student class loss: 0.0661\n",
            "[31/33] student class loss: 0.0856\n",
            "[32/33] student class loss: 0.1223\n",
            "[33/33] student class loss: 0.1381\n",
            "Epochs 15  train loss 0.15059 train acc 0.99337 validate loss 1.47612 validate acc 0.70355\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0016 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0145\n",
            "[2/33] student class loss: 0.0665\n",
            "[3/33] student class loss: 0.1314\n",
            "[4/33] student class loss: 0.0445\n",
            "[5/33] student class loss: 0.0481\n",
            "[6/33] student class loss: 0.0828\n",
            "[7/33] student class loss: 0.0315\n",
            "[8/33] student class loss: 0.0926\n",
            "[9/33] student class loss: 0.0724\n",
            "[10/33] student class loss: 0.0189\n",
            "[11/33] student class loss: 0.1406\n",
            "[12/33] student class loss: 0.0887\n",
            "[13/33] student class loss: 0.1214\n",
            "[14/33] student class loss: 0.0467\n",
            "[15/33] student class loss: 0.0682\n",
            "[16/33] student class loss: 0.1032\n",
            "[17/33] student class loss: 0.1318\n",
            "[18/33] student class loss: 0.0515\n",
            "[19/33] student class loss: 0.0389\n",
            "[20/33] student class loss: 0.0634\n",
            "[21/33] student class loss: 0.1167\n",
            "[22/33] student class loss: 0.0313\n",
            "[23/33] student class loss: 0.1128\n",
            "[24/33] student class loss: 0.0149\n",
            "[25/33] student class loss: 0.0318\n",
            "[26/33] student class loss: 0.1772\n",
            "[27/33] student class loss: 0.0756\n",
            "[28/33] student class loss: 0.0466\n",
            "[29/33] student class loss: 0.0154\n",
            "[30/33] student class loss: 0.1423\n",
            "[31/33] student class loss: 0.0358\n",
            "[32/33] student class loss: 0.0398\n",
            "[33/33] student class loss: 0.0583\n",
            "Epochs 16  train loss 0.07140 train acc 0.99716 validate loss 1.07886 validate acc 0.76454\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0017 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0354\n",
            "[2/33] student class loss: 0.0514\n",
            "[3/33] student class loss: 0.0143\n",
            "[4/33] student class loss: 0.0276\n",
            "[5/33] student class loss: 0.0805\n",
            "[6/33] student class loss: 0.0586\n",
            "[7/33] student class loss: 0.1112\n",
            "[8/33] student class loss: 0.0195\n",
            "[9/33] student class loss: 0.0104\n",
            "[10/33] student class loss: 0.0279\n",
            "[11/33] student class loss: 0.0158\n",
            "[12/33] student class loss: 0.0167\n",
            "[13/33] student class loss: 0.0973\n",
            "[14/33] student class loss: 0.0677\n",
            "[15/33] student class loss: 0.0238\n",
            "[16/33] student class loss: 0.0132\n",
            "[17/33] student class loss: 0.0816\n",
            "[18/33] student class loss: 0.0663\n",
            "[19/33] student class loss: 0.0889\n",
            "[20/33] student class loss: 0.0728\n",
            "[21/33] student class loss: 0.0247\n",
            "[22/33] student class loss: 0.0699\n",
            "[23/33] student class loss: 0.0377\n",
            "[24/33] student class loss: 0.1013\n",
            "[25/33] student class loss: 0.0067\n",
            "[26/33] student class loss: 0.0441\n",
            "[27/33] student class loss: 0.0395\n",
            "[28/33] student class loss: 0.0079\n",
            "[29/33] student class loss: 0.0413\n",
            "[30/33] student class loss: 0.0117\n",
            "[31/33] student class loss: 0.0872\n",
            "[32/33] student class loss: 0.0414\n",
            "[33/33] student class loss: 0.0480\n",
            "Epochs 17  train loss 0.04674 train acc 0.99953 validate loss 1.13211 validate acc 0.74184\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0018 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0880\n",
            "[2/33] student class loss: 0.0537\n",
            "[3/33] student class loss: 0.0677\n",
            "[4/33] student class loss: 0.0234\n",
            "[5/33] student class loss: 0.0354\n",
            "[6/33] student class loss: 0.0145\n",
            "[7/33] student class loss: 0.0201\n",
            "[8/33] student class loss: 0.0163\n",
            "[9/33] student class loss: 0.0159\n",
            "[10/33] student class loss: 0.1553\n",
            "[11/33] student class loss: 0.0146\n",
            "[12/33] student class loss: 0.0138\n",
            "[13/33] student class loss: 0.0212\n",
            "[14/33] student class loss: 0.0791\n",
            "[15/33] student class loss: 0.0643\n",
            "[16/33] student class loss: 0.0315\n",
            "[17/33] student class loss: 0.0529\n",
            "[18/33] student class loss: 0.0250\n",
            "[19/33] student class loss: 0.0093\n",
            "[20/33] student class loss: 0.0521\n",
            "[21/33] student class loss: 0.0042\n",
            "[22/33] student class loss: 0.0129\n",
            "[23/33] student class loss: 0.0037\n",
            "[24/33] student class loss: 0.0151\n",
            "[25/33] student class loss: 0.0728\n",
            "[26/33] student class loss: 0.0295\n",
            "[27/33] student class loss: 0.0248\n",
            "[28/33] student class loss: 0.0202\n",
            "[29/33] student class loss: 0.0388\n",
            "[30/33] student class loss: 0.0347\n",
            "[31/33] student class loss: 0.0537\n",
            "[32/33] student class loss: 0.0338\n",
            "[33/33] student class loss: 0.0620\n",
            "Epochs 18  train loss 0.03819 train acc 0.99811 validate loss 1.05805 validate acc 0.76879\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0019 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0039\n",
            "[2/33] student class loss: 0.0285\n",
            "[3/33] student class loss: 0.0126\n",
            "[4/33] student class loss: 0.0049\n",
            "[5/33] student class loss: 0.0074\n",
            "[6/33] student class loss: 0.1157\n",
            "[7/33] student class loss: 0.0030\n",
            "[8/33] student class loss: 0.0055\n",
            "[9/33] student class loss: 0.0158\n",
            "[10/33] student class loss: 0.0135\n",
            "[11/33] student class loss: 0.0211\n",
            "[12/33] student class loss: 0.0070\n",
            "[13/33] student class loss: 0.0079\n",
            "[14/33] student class loss: 0.0109\n",
            "[15/33] student class loss: 0.0178\n",
            "[16/33] student class loss: 0.0160\n",
            "[17/33] student class loss: 0.0181\n",
            "[18/33] student class loss: 0.0064\n",
            "[19/33] student class loss: 0.0137\n",
            "[20/33] student class loss: 0.0081\n",
            "[21/33] student class loss: 0.0125\n",
            "[22/33] student class loss: 0.0140\n",
            "[23/33] student class loss: 0.0163\n",
            "[24/33] student class loss: 0.0814\n",
            "[25/33] student class loss: 0.0133\n",
            "[26/33] student class loss: 0.0166\n",
            "[27/33] student class loss: 0.0283\n",
            "[28/33] student class loss: 0.0237\n",
            "[29/33] student class loss: 0.0097\n",
            "[30/33] student class loss: 0.0091\n",
            "[31/33] student class loss: 0.0114\n",
            "[32/33] student class loss: 0.0047\n",
            "[33/33] student class loss: 0.0061\n",
            "Epochs 19  train loss 0.01773 train acc 0.99953 validate loss 0.96970 validate acc 0.79716\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0020 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0077\n",
            "[2/33] student class loss: 0.0015\n",
            "[3/33] student class loss: 0.0105\n",
            "[4/33] student class loss: 0.0136\n",
            "[5/33] student class loss: 0.0044\n",
            "[6/33] student class loss: 0.0065\n",
            "[7/33] student class loss: 0.0073\n",
            "[8/33] student class loss: 0.0365\n",
            "[9/33] student class loss: 0.0025\n",
            "[10/33] student class loss: 0.0077\n",
            "[11/33] student class loss: 0.0027\n",
            "[12/33] student class loss: 0.0029\n",
            "[13/33] student class loss: 0.0055\n",
            "[14/33] student class loss: 0.0038\n",
            "[15/33] student class loss: 0.0033\n",
            "[16/33] student class loss: 0.0014\n",
            "[17/33] student class loss: 0.0065\n",
            "[18/33] student class loss: 0.0234\n",
            "[19/33] student class loss: 0.0023\n",
            "[20/33] student class loss: 0.0093\n",
            "[21/33] student class loss: 0.0139\n",
            "[22/33] student class loss: 0.0029\n",
            "[23/33] student class loss: 0.0027\n",
            "[24/33] student class loss: 0.1473\n",
            "[25/33] student class loss: 0.0017\n",
            "[26/33] student class loss: 0.0103\n",
            "[27/33] student class loss: 0.0096\n",
            "[28/33] student class loss: 0.0113\n",
            "[29/33] student class loss: 0.0255\n",
            "[30/33] student class loss: 0.0323\n",
            "[31/33] student class loss: 0.0165\n",
            "[32/33] student class loss: 0.0160\n",
            "[33/33] student class loss: 0.0100\n",
            "Epochs 20  train loss 0.01392 train acc 1.00000 validate loss 1.04280 validate acc 0.78582\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0021 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0653\n",
            "[2/33] student class loss: 0.0069\n",
            "[3/33] student class loss: 0.0085\n",
            "[4/33] student class loss: 0.0063\n",
            "[5/33] student class loss: 0.0021\n",
            "[6/33] student class loss: 0.0201\n",
            "[7/33] student class loss: 0.0170\n",
            "[8/33] student class loss: 0.0018\n",
            "[9/33] student class loss: 0.0008\n",
            "[10/33] student class loss: 0.0056\n",
            "[11/33] student class loss: 0.0135\n",
            "[12/33] student class loss: 0.0071\n",
            "[13/33] student class loss: 0.0196\n",
            "[14/33] student class loss: 0.0027\n",
            "[15/33] student class loss: 0.0034\n",
            "[16/33] student class loss: 0.0041\n",
            "[17/33] student class loss: 0.0074\n",
            "[18/33] student class loss: 0.0069\n",
            "[19/33] student class loss: 0.0091\n",
            "[20/33] student class loss: 0.0027\n",
            "[21/33] student class loss: 0.0008\n",
            "[22/33] student class loss: 0.0014\n",
            "[23/33] student class loss: 0.0069\n",
            "[24/33] student class loss: 0.0066\n",
            "[25/33] student class loss: 0.0016\n",
            "[26/33] student class loss: 0.0026\n",
            "[27/33] student class loss: 0.0012\n",
            "[28/33] student class loss: 0.0071\n",
            "[29/33] student class loss: 0.0016\n",
            "[30/33] student class loss: 0.0637\n",
            "[31/33] student class loss: 0.0144\n",
            "[32/33] student class loss: 0.0074\n",
            "[33/33] student class loss: 0.0025\n",
            "Epochs 21  train loss 0.00997 train acc 0.99953 validate loss 0.98073 validate acc 0.80000\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0022 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0007\n",
            "[2/33] student class loss: 0.0014\n",
            "[3/33] student class loss: 0.0050\n",
            "[4/33] student class loss: 0.0131\n",
            "[5/33] student class loss: 0.0006\n",
            "[6/33] student class loss: 0.0028\n",
            "[7/33] student class loss: 0.0147\n",
            "[8/33] student class loss: 0.0025\n",
            "[9/33] student class loss: 0.0105\n",
            "[10/33] student class loss: 0.0030\n",
            "[11/33] student class loss: 0.0074\n",
            "[12/33] student class loss: 0.0120\n",
            "[13/33] student class loss: 0.0047\n",
            "[14/33] student class loss: 0.0251\n",
            "[15/33] student class loss: 0.0022\n",
            "[16/33] student class loss: 0.0065\n",
            "[17/33] student class loss: 0.0059\n",
            "[18/33] student class loss: 0.0082\n",
            "[19/33] student class loss: 0.0204\n",
            "[20/33] student class loss: 0.0054\n",
            "[21/33] student class loss: 0.0067\n",
            "[22/33] student class loss: 0.0049\n",
            "[23/33] student class loss: 0.0045\n",
            "[24/33] student class loss: 0.0209\n",
            "[25/33] student class loss: 0.0357\n",
            "[26/33] student class loss: 0.0130\n",
            "[27/33] student class loss: 0.0057\n",
            "[28/33] student class loss: 0.0041\n",
            "[29/33] student class loss: 0.0028\n",
            "[30/33] student class loss: 0.0033\n",
            "[31/33] student class loss: 0.0014\n",
            "[32/33] student class loss: 0.0042\n",
            "[33/33] student class loss: 0.0014\n",
            "Epochs 22  train loss 0.00790 train acc 1.00000 validate loss 1.00207 validate acc 0.78865\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0023 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0033\n",
            "[2/33] student class loss: 0.0027\n",
            "[3/33] student class loss: 0.0015\n",
            "[4/33] student class loss: 0.0028\n",
            "[5/33] student class loss: 0.0068\n",
            "[6/33] student class loss: 0.0097\n",
            "[7/33] student class loss: 0.0013\n",
            "[8/33] student class loss: 0.0010\n",
            "[9/33] student class loss: 0.0014\n",
            "[10/33] student class loss: 0.0095\n",
            "[11/33] student class loss: 0.0007\n",
            "[12/33] student class loss: 0.0057\n",
            "[13/33] student class loss: 0.0028\n",
            "[14/33] student class loss: 0.0016\n",
            "[15/33] student class loss: 0.0009\n",
            "[16/33] student class loss: 0.0011\n",
            "[17/33] student class loss: 0.0013\n",
            "[18/33] student class loss: 0.0006\n",
            "[19/33] student class loss: 0.0024\n",
            "[20/33] student class loss: 0.0027\n",
            "[21/33] student class loss: 0.0012\n",
            "[22/33] student class loss: 0.0088\n",
            "[23/33] student class loss: 0.0025\n",
            "[24/33] student class loss: 0.0205\n",
            "[25/33] student class loss: 0.0088\n",
            "[26/33] student class loss: 0.0009\n",
            "[27/33] student class loss: 0.0018\n",
            "[28/33] student class loss: 0.0024\n",
            "[29/33] student class loss: 0.0050\n",
            "[30/33] student class loss: 0.0025\n",
            "[31/33] student class loss: 0.0028\n",
            "[32/33] student class loss: 0.0009\n",
            "[33/33] student class loss: 0.0020\n",
            "Epochs 23  train loss 0.00364 train acc 1.00000 validate loss 0.96569 validate acc 0.80000\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0024 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0048\n",
            "[2/33] student class loss: 0.0005\n",
            "[3/33] student class loss: 0.0007\n",
            "[4/33] student class loss: 0.0011\n",
            "[5/33] student class loss: 0.0005\n",
            "[6/33] student class loss: 0.0010\n",
            "[7/33] student class loss: 0.0017\n",
            "[8/33] student class loss: 0.0033\n",
            "[9/33] student class loss: 0.0004\n",
            "[10/33] student class loss: 0.0008\n",
            "[11/33] student class loss: 0.0010\n",
            "[12/33] student class loss: 0.0073\n",
            "[13/33] student class loss: 0.0013\n",
            "[14/33] student class loss: 0.0007\n",
            "[15/33] student class loss: 0.0010\n",
            "[16/33] student class loss: 0.0022\n",
            "[17/33] student class loss: 0.0010\n",
            "[18/33] student class loss: 0.0083\n",
            "[19/33] student class loss: 0.0009\n",
            "[20/33] student class loss: 0.0020\n",
            "[21/33] student class loss: 0.0011\n",
            "[22/33] student class loss: 0.0028\n",
            "[23/33] student class loss: 0.0050\n",
            "[24/33] student class loss: 0.0011\n",
            "[25/33] student class loss: 0.0023\n",
            "[26/33] student class loss: 0.0026\n",
            "[27/33] student class loss: 0.0005\n",
            "[28/33] student class loss: 0.0020\n",
            "[29/33] student class loss: 0.0045\n",
            "[30/33] student class loss: 0.0024\n",
            "[31/33] student class loss: 0.0003\n",
            "[32/33] student class loss: 0.0018\n",
            "[33/33] student class loss: 0.0017\n",
            "Epochs 24  train loss 0.00208 train acc 1.00000 validate loss 0.91639 validate acc 0.80993\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0025 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0010\n",
            "[2/33] student class loss: 0.0006\n",
            "[3/33] student class loss: 0.0007\n",
            "[4/33] student class loss: 0.0004\n",
            "[5/33] student class loss: 0.0006\n",
            "[6/33] student class loss: 0.0010\n",
            "[7/33] student class loss: 0.0014\n",
            "[8/33] student class loss: 0.0002\n",
            "[9/33] student class loss: 0.0053\n",
            "[10/33] student class loss: 0.0010\n",
            "[11/33] student class loss: 0.0015\n",
            "[12/33] student class loss: 0.0016\n",
            "[13/33] student class loss: 0.0011\n",
            "[14/33] student class loss: 0.0013\n",
            "[15/33] student class loss: 0.0021\n",
            "[16/33] student class loss: 0.0008\n",
            "[17/33] student class loss: 0.0004\n",
            "[18/33] student class loss: 0.0008\n",
            "[19/33] student class loss: 0.0029\n",
            "[20/33] student class loss: 0.0006\n",
            "[21/33] student class loss: 0.0009\n",
            "[22/33] student class loss: 0.0004\n",
            "[23/33] student class loss: 0.0015\n",
            "[24/33] student class loss: 0.0002\n",
            "[25/33] student class loss: 0.0007\n",
            "[26/33] student class loss: 0.0025\n",
            "[27/33] student class loss: 0.0008\n",
            "[28/33] student class loss: 0.0024\n",
            "[29/33] student class loss: 0.0001\n",
            "[30/33] student class loss: 0.0010\n",
            "[31/33] student class loss: 0.0017\n",
            "[32/33] student class loss: 0.0011\n",
            "[33/33] student class loss: 0.0004\n",
            "Epochs 25  train loss 0.00118 train acc 1.00000 validate loss 0.91518 validate acc 0.81135\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0026 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0009\n",
            "[2/33] student class loss: 0.0003\n",
            "[3/33] student class loss: 0.0005\n",
            "[4/33] student class loss: 0.0002\n",
            "[5/33] student class loss: 0.0010\n",
            "[6/33] student class loss: 0.0002\n",
            "[7/33] student class loss: 0.0015\n",
            "[8/33] student class loss: 0.0002\n",
            "[9/33] student class loss: 0.0005\n",
            "[10/33] student class loss: 0.0030\n",
            "[11/33] student class loss: 0.0009\n",
            "[12/33] student class loss: 0.0004\n",
            "[13/33] student class loss: 0.0010\n",
            "[14/33] student class loss: 0.0002\n",
            "[15/33] student class loss: 0.0003\n",
            "[16/33] student class loss: 0.0007\n",
            "[17/33] student class loss: 0.0003\n",
            "[18/33] student class loss: 0.0004\n",
            "[19/33] student class loss: 0.0006\n",
            "[20/33] student class loss: 0.0003\n",
            "[21/33] student class loss: 0.0003\n",
            "[22/33] student class loss: 0.0008\n",
            "[23/33] student class loss: 0.0003\n",
            "[24/33] student class loss: 0.0013\n",
            "[25/33] student class loss: 0.0004\n",
            "[26/33] student class loss: 0.0001\n",
            "[27/33] student class loss: 0.0042\n",
            "[28/33] student class loss: 0.0009\n",
            "[29/33] student class loss: 0.0019\n",
            "[30/33] student class loss: 0.0004\n",
            "[31/33] student class loss: 0.0005\n",
            "[32/33] student class loss: 0.0004\n",
            "[33/33] student class loss: 0.0010\n",
            "Epochs 26  train loss 0.00079 train acc 1.00000 validate loss 0.90550 validate acc 0.81560\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0027 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0005\n",
            "[2/33] student class loss: 0.0009\n",
            "[3/33] student class loss: 0.0004\n",
            "[4/33] student class loss: 0.0013\n",
            "[5/33] student class loss: 0.0003\n",
            "[6/33] student class loss: 0.0010\n",
            "[7/33] student class loss: 0.0011\n",
            "[8/33] student class loss: 0.0017\n",
            "[9/33] student class loss: 0.0010\n",
            "[10/33] student class loss: 0.0017\n",
            "[11/33] student class loss: 0.0008\n",
            "[12/33] student class loss: 0.0005\n",
            "[13/33] student class loss: 0.0036\n",
            "[14/33] student class loss: 0.0002\n",
            "[15/33] student class loss: 0.0006\n",
            "[16/33] student class loss: 0.0011\n",
            "[17/33] student class loss: 0.0011\n",
            "[18/33] student class loss: 0.0006\n",
            "[19/33] student class loss: 0.0004\n",
            "[20/33] student class loss: 0.0008\n",
            "[21/33] student class loss: 0.0014\n",
            "[22/33] student class loss: 0.0014\n",
            "[23/33] student class loss: 0.0002\n",
            "[24/33] student class loss: 0.0005\n",
            "[25/33] student class loss: 0.0008\n",
            "[26/33] student class loss: 0.0002\n",
            "[27/33] student class loss: 0.0004\n",
            "[28/33] student class loss: 0.0008\n",
            "[29/33] student class loss: 0.0003\n",
            "[30/33] student class loss: 0.0006\n",
            "[31/33] student class loss: 0.0002\n",
            "[32/33] student class loss: 0.0016\n",
            "[33/33] student class loss: 0.0033\n",
            "Epochs 27  train loss 0.00095 train acc 1.00000 validate loss 0.90951 validate acc 0.82128\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0028 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0005\n",
            "[2/33] student class loss: 0.0003\n",
            "[3/33] student class loss: 0.0008\n",
            "[4/33] student class loss: 0.0009\n",
            "[5/33] student class loss: 0.0005\n",
            "[6/33] student class loss: 0.0008\n",
            "[7/33] student class loss: 0.0015\n",
            "[8/33] student class loss: 0.0002\n",
            "[9/33] student class loss: 0.0003\n",
            "[10/33] student class loss: 0.0006\n",
            "[11/33] student class loss: 0.0007\n",
            "[12/33] student class loss: 0.0002\n",
            "[13/33] student class loss: 0.0003\n",
            "[14/33] student class loss: 0.0054\n",
            "[15/33] student class loss: 0.0003\n",
            "[16/33] student class loss: 0.0002\n",
            "[17/33] student class loss: 0.0009\n",
            "[18/33] student class loss: 0.0023\n",
            "[19/33] student class loss: 0.0005\n",
            "[20/33] student class loss: 0.0004\n",
            "[21/33] student class loss: 0.0007\n",
            "[22/33] student class loss: 0.0117\n",
            "[23/33] student class loss: 0.0004\n",
            "[24/33] student class loss: 0.0003\n",
            "[25/33] student class loss: 0.0006\n",
            "[26/33] student class loss: 0.0005\n",
            "[27/33] student class loss: 0.0010\n",
            "[28/33] student class loss: 0.0009\n",
            "[29/33] student class loss: 0.0004\n",
            "[30/33] student class loss: 0.0035\n",
            "[31/33] student class loss: 0.0015\n",
            "[32/33] student class loss: 0.0019\n",
            "[33/33] student class loss: 0.0008\n",
            "Epochs 28  train loss 0.00127 train acc 1.00000 validate loss 0.92059 validate acc 0.81986\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0029 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0006\n",
            "[2/33] student class loss: 0.0019\n",
            "[3/33] student class loss: 0.0021\n",
            "[4/33] student class loss: 0.0002\n",
            "[5/33] student class loss: 0.0015\n",
            "[6/33] student class loss: 0.0007\n",
            "[7/33] student class loss: 0.0207\n",
            "[8/33] student class loss: 0.0003\n",
            "[9/33] student class loss: 0.0003\n",
            "[10/33] student class loss: 0.0001\n",
            "[11/33] student class loss: 0.0012\n",
            "[12/33] student class loss: 0.0006\n",
            "[13/33] student class loss: 0.0010\n",
            "[14/33] student class loss: 0.0003\n",
            "[15/33] student class loss: 0.0003\n",
            "[16/33] student class loss: 0.0004\n",
            "[17/33] student class loss: 0.0004\n",
            "[18/33] student class loss: 0.0002\n",
            "[19/33] student class loss: 0.0001\n",
            "[20/33] student class loss: 0.0006\n",
            "[21/33] student class loss: 0.0005\n",
            "[22/33] student class loss: 0.0007\n",
            "[23/33] student class loss: 0.0007\n",
            "[24/33] student class loss: 0.0009\n",
            "[25/33] student class loss: 0.0002\n",
            "[26/33] student class loss: 0.0003\n",
            "[27/33] student class loss: 0.0016\n",
            "[28/33] student class loss: 0.0008\n",
            "[29/33] student class loss: 0.0007\n",
            "[30/33] student class loss: 0.0003\n",
            "[31/33] student class loss: 0.0545\n",
            "[32/33] student class loss: 0.0002\n",
            "[33/33] student class loss: 0.0009\n",
            "Epochs 29  train loss 0.00291 train acc 1.00000 validate loss 0.94069 validate acc 0.81560\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0030 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0007\n",
            "[2/33] student class loss: 0.0003\n",
            "[3/33] student class loss: 0.0005\n",
            "[4/33] student class loss: 0.0020\n",
            "[5/33] student class loss: 0.0004\n",
            "[6/33] student class loss: 0.0012\n",
            "[7/33] student class loss: 0.0002\n",
            "[8/33] student class loss: 0.0002\n",
            "[9/33] student class loss: 0.0006\n",
            "[10/33] student class loss: 0.0002\n",
            "[11/33] student class loss: 0.0003\n",
            "[12/33] student class loss: 0.0007\n",
            "[13/33] student class loss: 0.0002\n",
            "[14/33] student class loss: 0.0005\n",
            "[15/33] student class loss: 0.0002\n",
            "[16/33] student class loss: 0.0002\n",
            "[17/33] student class loss: 0.0024\n",
            "[18/33] student class loss: 0.0005\n",
            "[19/33] student class loss: 0.0007\n",
            "[20/33] student class loss: 0.0005\n",
            "[21/33] student class loss: 0.0001\n",
            "[22/33] student class loss: 0.0004\n",
            "[23/33] student class loss: 0.0002\n",
            "[24/33] student class loss: 0.0005\n",
            "[25/33] student class loss: 0.0006\n",
            "[26/33] student class loss: 0.0003\n",
            "[27/33] student class loss: 0.0002\n",
            "[28/33] student class loss: 0.0005\n",
            "[29/33] student class loss: 0.0008\n",
            "[30/33] student class loss: 0.0002\n",
            "[31/33] student class loss: 0.0009\n",
            "[32/33] student class loss: 0.0002\n",
            "[33/33] student class loss: 0.0008\n",
            "Epochs 30  train loss 0.00056 train acc 1.00000 validate loss 0.92383 validate acc 0.81986\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0031 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0004\n",
            "[2/33] student class loss: 0.0009\n",
            "[3/33] student class loss: 0.0002\n",
            "[4/33] student class loss: 0.0070\n",
            "[5/33] student class loss: 0.0007\n",
            "[6/33] student class loss: 0.0005\n",
            "[7/33] student class loss: 0.0005\n",
            "[8/33] student class loss: 0.0003\n",
            "[9/33] student class loss: 0.0002\n",
            "[10/33] student class loss: 0.0011\n",
            "[11/33] student class loss: 0.0006\n",
            "[12/33] student class loss: 0.0001\n",
            "[13/33] student class loss: 0.0003\n",
            "[14/33] student class loss: 0.0003\n",
            "[15/33] student class loss: 0.0003\n",
            "[16/33] student class loss: 0.0002\n",
            "[17/33] student class loss: 0.0002\n",
            "[18/33] student class loss: 0.0009\n",
            "[19/33] student class loss: 0.0005\n",
            "[20/33] student class loss: 0.0012\n",
            "[21/33] student class loss: 0.0007\n",
            "[22/33] student class loss: 0.0016\n",
            "[23/33] student class loss: 0.0007\n",
            "[24/33] student class loss: 0.0003\n",
            "[25/33] student class loss: 0.0002\n",
            "[26/33] student class loss: 0.0003\n",
            "[27/33] student class loss: 0.0006\n",
            "[28/33] student class loss: 0.0005\n",
            "[29/33] student class loss: 0.0003\n",
            "[30/33] student class loss: 0.0006\n",
            "[31/33] student class loss: 0.0020\n",
            "[32/33] student class loss: 0.0003\n",
            "[33/33] student class loss: 0.0008\n",
            "Epochs 31  train loss 0.00077 train acc 1.00000 validate loss 0.91406 validate acc 0.81844\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0032 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0004\n",
            "[2/33] student class loss: 0.0003\n",
            "[3/33] student class loss: 0.0004\n",
            "[4/33] student class loss: 0.0002\n",
            "[5/33] student class loss: 0.0009\n",
            "[6/33] student class loss: 0.0004\n",
            "[7/33] student class loss: 0.0005\n",
            "[8/33] student class loss: 0.0018\n",
            "[9/33] student class loss: 0.0017\n",
            "[10/33] student class loss: 0.0003\n",
            "[11/33] student class loss: 0.0015\n",
            "[12/33] student class loss: 0.0003\n",
            "[13/33] student class loss: 0.0005\n",
            "[14/33] student class loss: 0.0002\n",
            "[15/33] student class loss: 0.0001\n",
            "[16/33] student class loss: 0.0002\n",
            "[17/33] student class loss: 0.0011\n",
            "[18/33] student class loss: 0.0017\n",
            "[19/33] student class loss: 0.0002\n",
            "[20/33] student class loss: 0.0001\n",
            "[21/33] student class loss: 0.0005\n",
            "[22/33] student class loss: 0.0015\n",
            "[23/33] student class loss: 0.0003\n",
            "[24/33] student class loss: 0.0012\n",
            "[25/33] student class loss: 0.0002\n",
            "[26/33] student class loss: 0.0002\n",
            "[27/33] student class loss: 0.0003\n",
            "[28/33] student class loss: 0.0006\n",
            "[29/33] student class loss: 0.0001\n",
            "[30/33] student class loss: 0.0006\n",
            "[31/33] student class loss: 0.0004\n",
            "[32/33] student class loss: 0.0004\n",
            "[33/33] student class loss: 0.0005\n",
            "Epochs 32  train loss 0.00060 train acc 1.00000 validate loss 0.91023 validate acc 0.82128\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0033 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0009\n",
            "[2/33] student class loss: 0.0006\n",
            "[3/33] student class loss: 0.0008\n",
            "[4/33] student class loss: 0.0003\n",
            "[5/33] student class loss: 0.0006\n",
            "[6/33] student class loss: 0.0004\n",
            "[7/33] student class loss: 0.0002\n",
            "[8/33] student class loss: 0.0004\n",
            "[9/33] student class loss: 0.0003\n",
            "[10/33] student class loss: 0.0002\n",
            "[11/33] student class loss: 0.0002\n",
            "[12/33] student class loss: 0.0004\n",
            "[13/33] student class loss: 0.0002\n",
            "[14/33] student class loss: 0.0007\n",
            "[15/33] student class loss: 0.0007\n",
            "[16/33] student class loss: 0.0006\n",
            "[17/33] student class loss: 0.0003\n",
            "[18/33] student class loss: 0.0001\n",
            "[19/33] student class loss: 0.0001\n",
            "[20/33] student class loss: 0.0003\n",
            "[21/33] student class loss: 0.0006\n",
            "[22/33] student class loss: 0.0002\n",
            "[23/33] student class loss: 0.0023\n",
            "[24/33] student class loss: 0.0004\n",
            "[25/33] student class loss: 0.0005\n",
            "[26/33] student class loss: 0.0006\n",
            "[27/33] student class loss: 0.0002\n",
            "[28/33] student class loss: 0.0013\n",
            "[29/33] student class loss: 0.0002\n",
            "[30/33] student class loss: 0.0003\n",
            "[31/33] student class loss: 0.0005\n",
            "[32/33] student class loss: 0.0002\n",
            "[33/33] student class loss: 0.0003\n",
            "Epochs 33  train loss 0.00048 train acc 1.00000 validate loss 0.91378 validate acc 0.82128\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0034 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0005\n",
            "[2/33] student class loss: 0.0001\n",
            "[3/33] student class loss: 0.0003\n",
            "[4/33] student class loss: 0.0001\n",
            "[5/33] student class loss: 0.0007\n",
            "[6/33] student class loss: 0.0001\n",
            "[7/33] student class loss: 0.0008\n",
            "[8/33] student class loss: 0.0007\n",
            "[9/33] student class loss: 0.0002\n",
            "[10/33] student class loss: 0.0013\n",
            "[11/33] student class loss: 0.0003\n",
            "[12/33] student class loss: 0.0010\n",
            "[13/33] student class loss: 0.0004\n",
            "[14/33] student class loss: 0.0004\n",
            "[15/33] student class loss: 0.0007\n",
            "[16/33] student class loss: 0.0014\n",
            "[17/33] student class loss: 0.0005\n",
            "[18/33] student class loss: 0.0004\n",
            "[19/33] student class loss: 0.0002\n",
            "[20/33] student class loss: 0.0003\n",
            "[21/33] student class loss: 0.0006\n",
            "[22/33] student class loss: 0.0004\n",
            "[23/33] student class loss: 0.0004\n",
            "[24/33] student class loss: 0.0001\n",
            "[25/33] student class loss: 0.0022\n",
            "[26/33] student class loss: 0.0016\n",
            "[27/33] student class loss: 0.0004\n",
            "[28/33] student class loss: 0.0003\n",
            "[29/33] student class loss: 0.0003\n",
            "[30/33] student class loss: 0.0003\n",
            "[31/33] student class loss: 0.0005\n",
            "[32/33] student class loss: 0.0003\n",
            "[33/33] student class loss: 0.0004\n",
            "Epochs 34  train loss 0.00056 train acc 1.00000 validate loss 0.91649 validate acc 0.81986\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0035 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0002\n",
            "[2/33] student class loss: 0.0001\n",
            "[3/33] student class loss: 0.0003\n",
            "[4/33] student class loss: 0.0013\n",
            "[5/33] student class loss: 0.0004\n",
            "[6/33] student class loss: 0.0005\n",
            "[7/33] student class loss: 0.0004\n",
            "[8/33] student class loss: 0.0002\n",
            "[9/33] student class loss: 0.0004\n",
            "[10/33] student class loss: 0.0002\n",
            "[11/33] student class loss: 0.0001\n",
            "[12/33] student class loss: 0.0001\n",
            "[13/33] student class loss: 0.0001\n",
            "[14/33] student class loss: 0.0010\n",
            "[15/33] student class loss: 0.0001\n",
            "[16/33] student class loss: 0.0003\n",
            "[17/33] student class loss: 0.0001\n",
            "[18/33] student class loss: 0.0001\n",
            "[19/33] student class loss: 0.0004\n",
            "[20/33] student class loss: 0.0001\n",
            "[21/33] student class loss: 0.0005\n",
            "[22/33] student class loss: 0.0009\n",
            "[23/33] student class loss: 0.0002\n",
            "[24/33] student class loss: 0.0004\n",
            "[25/33] student class loss: 0.0004\n",
            "[26/33] student class loss: 0.0001\n",
            "[27/33] student class loss: 0.0030\n",
            "[28/33] student class loss: 0.0042\n",
            "[29/33] student class loss: 0.0002\n",
            "[30/33] student class loss: 0.0002\n",
            "[31/33] student class loss: 0.0003\n",
            "[32/33] student class loss: 0.0004\n",
            "[33/33] student class loss: 0.0003\n",
            "Epochs 35  train loss 0.00054 train acc 0.99953 validate loss 0.91445 validate acc 0.82128\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0036 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0002\n",
            "[2/33] student class loss: 0.0003\n",
            "[3/33] student class loss: 0.0005\n",
            "[4/33] student class loss: 0.0001\n",
            "[5/33] student class loss: 0.0002\n",
            "[6/33] student class loss: 0.0003\n",
            "[7/33] student class loss: 0.0008\n",
            "[8/33] student class loss: 0.0001\n",
            "[9/33] student class loss: 0.0007\n",
            "[10/33] student class loss: 0.0003\n",
            "[11/33] student class loss: 0.0003\n",
            "[12/33] student class loss: 0.0024\n",
            "[13/33] student class loss: 0.0002\n",
            "[14/33] student class loss: 0.0007\n",
            "[15/33] student class loss: 0.0003\n",
            "[16/33] student class loss: 0.0004\n",
            "[17/33] student class loss: 0.0004\n",
            "[18/33] student class loss: 0.0003\n",
            "[19/33] student class loss: 0.0002\n",
            "[20/33] student class loss: 0.0008\n",
            "[21/33] student class loss: 0.0003\n",
            "[22/33] student class loss: 0.0002\n",
            "[23/33] student class loss: 0.0002\n",
            "[24/33] student class loss: 0.0003\n",
            "[25/33] student class loss: 0.0003\n",
            "[26/33] student class loss: 0.0004\n",
            "[27/33] student class loss: 0.0004\n",
            "[28/33] student class loss: 0.0002\n",
            "[29/33] student class loss: 0.0003\n",
            "[30/33] student class loss: 0.0001\n",
            "[31/33] student class loss: 0.0004\n",
            "[32/33] student class loss: 0.0006\n",
            "[33/33] student class loss: 0.0002\n",
            "Epochs 36  train loss 0.00041 train acc 1.00000 validate loss 0.91509 validate acc 0.82128\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0037 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0006\n",
            "[2/33] student class loss: 0.0007\n",
            "[3/33] student class loss: 0.0003\n",
            "[4/33] student class loss: 0.0003\n",
            "[5/33] student class loss: 0.0003\n",
            "[6/33] student class loss: 0.0003\n",
            "[7/33] student class loss: 0.0003\n",
            "[8/33] student class loss: 0.0010\n",
            "[9/33] student class loss: 0.0002\n",
            "[10/33] student class loss: 0.0002\n",
            "[11/33] student class loss: 0.0002\n",
            "[12/33] student class loss: 0.0008\n",
            "[13/33] student class loss: 0.0001\n",
            "[14/33] student class loss: 0.0003\n",
            "[15/33] student class loss: 0.0009\n",
            "[16/33] student class loss: 0.0010\n",
            "[17/33] student class loss: 0.0009\n",
            "[18/33] student class loss: 0.0001\n",
            "[19/33] student class loss: 0.0001\n",
            "[20/33] student class loss: 0.0001\n",
            "[21/33] student class loss: 0.0002\n",
            "[22/33] student class loss: 0.0003\n",
            "[23/33] student class loss: 0.0017\n",
            "[24/33] student class loss: 0.0000\n",
            "[25/33] student class loss: 0.0010\n",
            "[26/33] student class loss: 0.0002\n",
            "[27/33] student class loss: 0.0004\n",
            "[28/33] student class loss: 0.0003\n",
            "[29/33] student class loss: 0.0006\n",
            "[30/33] student class loss: 0.0019\n",
            "[31/33] student class loss: 0.0011\n",
            "[32/33] student class loss: 0.0002\n",
            "[33/33] student class loss: 0.0005\n",
            "Epochs 37  train loss 0.00051 train acc 1.00000 validate loss 0.91270 validate acc 0.82411\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0038 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0002\n",
            "[2/33] student class loss: 0.0003\n",
            "[3/33] student class loss: 0.0004\n",
            "[4/33] student class loss: 0.0002\n",
            "[5/33] student class loss: 0.0001\n",
            "[6/33] student class loss: 0.0023\n",
            "[7/33] student class loss: 0.0001\n",
            "[8/33] student class loss: 0.0008\n",
            "[9/33] student class loss: 0.0002\n",
            "[10/33] student class loss: 0.0002\n",
            "[11/33] student class loss: 0.0003\n",
            "[12/33] student class loss: 0.0002\n",
            "[13/33] student class loss: 0.0002\n",
            "[14/33] student class loss: 0.0002\n",
            "[15/33] student class loss: 0.0001\n",
            "[16/33] student class loss: 0.0008\n",
            "[17/33] student class loss: 0.0002\n",
            "[18/33] student class loss: 0.0004\n",
            "[19/33] student class loss: 0.0014\n",
            "[20/33] student class loss: 0.0002\n",
            "[21/33] student class loss: 0.0005\n",
            "[22/33] student class loss: 0.0002\n",
            "[23/33] student class loss: 0.0001\n",
            "[24/33] student class loss: 0.0003\n",
            "[25/33] student class loss: 0.0003\n",
            "[26/33] student class loss: 0.0007\n",
            "[27/33] student class loss: 0.0005\n",
            "[28/33] student class loss: 0.0002\n",
            "[29/33] student class loss: 0.0005\n",
            "[30/33] student class loss: 0.0001\n",
            "[31/33] student class loss: 0.0006\n",
            "[32/33] student class loss: 0.0003\n",
            "[33/33] student class loss: 0.0005\n",
            "Epochs 38  train loss 0.00041 train acc 1.00000 validate loss 0.91855 validate acc 0.81986\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0039 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0002\n",
            "[2/33] student class loss: 0.0002\n",
            "[3/33] student class loss: 0.0003\n",
            "[4/33] student class loss: 0.0001\n",
            "[5/33] student class loss: 0.0002\n",
            "[6/33] student class loss: 0.0005\n",
            "[7/33] student class loss: 0.0005\n",
            "[8/33] student class loss: 0.0001\n",
            "[9/33] student class loss: 0.0005\n",
            "[10/33] student class loss: 0.0003\n",
            "[11/33] student class loss: 0.0006\n",
            "[12/33] student class loss: 0.0006\n",
            "[13/33] student class loss: 0.0007\n",
            "[14/33] student class loss: 0.0004\n",
            "[15/33] student class loss: 0.0001\n",
            "[16/33] student class loss: 0.0010\n",
            "[17/33] student class loss: 0.0001\n",
            "[18/33] student class loss: 0.0003\n",
            "[19/33] student class loss: 0.0003\n",
            "[20/33] student class loss: 0.0002\n",
            "[21/33] student class loss: 0.0002\n",
            "[22/33] student class loss: 0.0003\n",
            "[23/33] student class loss: 0.0010\n",
            "[24/33] student class loss: 0.0004\n",
            "[25/33] student class loss: 0.0002\n",
            "[26/33] student class loss: 0.0001\n",
            "[27/33] student class loss: 0.0004\n",
            "[28/33] student class loss: 0.0004\n",
            "[29/33] student class loss: 0.0015\n",
            "[30/33] student class loss: 0.0002\n",
            "[31/33] student class loss: 0.0002\n",
            "[32/33] student class loss: 0.0007\n",
            "[33/33] student class loss: 0.0001\n",
            "Epochs 39  train loss 0.00039 train acc 1.00000 validate loss 0.92032 validate acc 0.82270\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0040 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0003\n",
            "[2/33] student class loss: 0.0007\n",
            "[3/33] student class loss: 0.0001\n",
            "[4/33] student class loss: 0.0007\n",
            "[5/33] student class loss: 0.0001\n",
            "[6/33] student class loss: 0.0001\n",
            "[7/33] student class loss: 0.0003\n",
            "[8/33] student class loss: 0.0003\n",
            "[9/33] student class loss: 0.0003\n",
            "[10/33] student class loss: 0.0012\n",
            "[11/33] student class loss: 0.0001\n",
            "[12/33] student class loss: 0.0001\n",
            "[13/33] student class loss: 0.0002\n",
            "[14/33] student class loss: 0.0009\n",
            "[15/33] student class loss: 0.0003\n",
            "[16/33] student class loss: 0.0002\n",
            "[17/33] student class loss: 0.0002\n",
            "[18/33] student class loss: 0.0005\n",
            "[19/33] student class loss: 0.0003\n",
            "[20/33] student class loss: 0.0003\n",
            "[21/33] student class loss: 0.0011\n",
            "[22/33] student class loss: 0.0004\n",
            "[23/33] student class loss: 0.0009\n",
            "[24/33] student class loss: 0.0002\n",
            "[25/33] student class loss: 0.0003\n",
            "[26/33] student class loss: 0.0002\n",
            "[27/33] student class loss: 0.0002\n",
            "[28/33] student class loss: 0.0005\n",
            "[29/33] student class loss: 0.0002\n",
            "[30/33] student class loss: 0.0001\n",
            "[31/33] student class loss: 0.0003\n",
            "[32/33] student class loss: 0.0002\n",
            "[33/33] student class loss: 0.0002\n",
            "Epochs 40  train loss 0.00036 train acc 1.00000 validate loss 0.92144 validate acc 0.82411\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0041 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0010\n",
            "[2/33] student class loss: 0.0002\n",
            "[3/33] student class loss: 0.0001\n",
            "[4/33] student class loss: 0.0002\n",
            "[5/33] student class loss: 0.0001\n",
            "[6/33] student class loss: 0.0005\n",
            "[7/33] student class loss: 0.0001\n",
            "[8/33] student class loss: 0.0002\n",
            "[9/33] student class loss: 0.0001\n",
            "[10/33] student class loss: 0.0005\n",
            "[11/33] student class loss: 0.0001\n",
            "[12/33] student class loss: 0.0023\n",
            "[13/33] student class loss: 0.0002\n",
            "[14/33] student class loss: 0.0003\n",
            "[15/33] student class loss: 0.0010\n",
            "[16/33] student class loss: 0.0001\n",
            "[17/33] student class loss: 0.0005\n",
            "[18/33] student class loss: 0.0003\n",
            "[19/33] student class loss: 0.0006\n",
            "[20/33] student class loss: 0.0000\n",
            "[21/33] student class loss: 0.0004\n",
            "[22/33] student class loss: 0.0002\n",
            "[23/33] student class loss: 0.0002\n",
            "[24/33] student class loss: 0.0002\n",
            "[25/33] student class loss: 0.0002\n",
            "[26/33] student class loss: 0.0003\n",
            "[27/33] student class loss: 0.0005\n",
            "[28/33] student class loss: 0.0011\n",
            "[29/33] student class loss: 0.0001\n",
            "[30/33] student class loss: 0.0006\n",
            "[31/33] student class loss: 0.0008\n",
            "[32/33] student class loss: 0.0004\n",
            "[33/33] student class loss: 0.0004\n",
            "Epochs 41  train loss 0.00042 train acc 1.00000 validate loss 0.92111 validate acc 0.82270\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0042 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0001\n",
            "[2/33] student class loss: 0.0001\n",
            "[3/33] student class loss: 0.0004\n",
            "[4/33] student class loss: 0.0002\n",
            "[5/33] student class loss: 0.0002\n",
            "[6/33] student class loss: 0.0008\n",
            "[7/33] student class loss: 0.0013\n",
            "[8/33] student class loss: 0.0002\n",
            "[9/33] student class loss: 0.0008\n",
            "[10/33] student class loss: 0.0002\n",
            "[11/33] student class loss: 0.0003\n",
            "[12/33] student class loss: 0.0009\n",
            "[13/33] student class loss: 0.0002\n",
            "[14/33] student class loss: 0.0004\n",
            "[15/33] student class loss: 0.0006\n",
            "[16/33] student class loss: 0.0004\n",
            "[17/33] student class loss: 0.0002\n",
            "[18/33] student class loss: 0.0001\n",
            "[19/33] student class loss: 0.0002\n",
            "[20/33] student class loss: 0.0002\n",
            "[21/33] student class loss: 0.0003\n",
            "[22/33] student class loss: 0.0006\n",
            "[23/33] student class loss: 0.0005\n",
            "[24/33] student class loss: 0.0002\n",
            "[25/33] student class loss: 0.0003\n",
            "[26/33] student class loss: 0.0004\n",
            "[27/33] student class loss: 0.0002\n",
            "[28/33] student class loss: 0.0006\n",
            "[29/33] student class loss: 0.0001\n",
            "[30/33] student class loss: 0.0002\n",
            "[31/33] student class loss: 0.0002\n",
            "[32/33] student class loss: 0.0002\n",
            "[33/33] student class loss: 0.0004\n",
            "Epochs 42  train loss 0.00036 train acc 1.00000 validate loss 0.92007 validate acc 0.82411\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0043 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0002\n",
            "[2/33] student class loss: 0.0003\n",
            "[3/33] student class loss: 0.0007\n",
            "[4/33] student class loss: 0.0001\n",
            "[5/33] student class loss: 0.0002\n",
            "[6/33] student class loss: 0.0007\n",
            "[7/33] student class loss: 0.0004\n",
            "[8/33] student class loss: 0.0003\n",
            "[9/33] student class loss: 0.0006\n",
            "[10/33] student class loss: 0.0001\n",
            "[11/33] student class loss: 0.0001\n",
            "[12/33] student class loss: 0.0002\n",
            "[13/33] student class loss: 0.0002\n",
            "[14/33] student class loss: 0.0003\n",
            "[15/33] student class loss: 0.0003\n",
            "[16/33] student class loss: 0.0007\n",
            "[17/33] student class loss: 0.0003\n",
            "[18/33] student class loss: 0.0004\n",
            "[19/33] student class loss: 0.0002\n",
            "[20/33] student class loss: 0.0006\n",
            "[21/33] student class loss: 0.0003\n",
            "[22/33] student class loss: 0.0002\n",
            "[23/33] student class loss: 0.0062\n",
            "[24/33] student class loss: 0.0002\n",
            "[25/33] student class loss: 0.0001\n",
            "[26/33] student class loss: 0.0003\n",
            "[27/33] student class loss: 0.0008\n",
            "[28/33] student class loss: 0.0001\n",
            "[29/33] student class loss: 0.0002\n",
            "[30/33] student class loss: 0.0002\n",
            "[31/33] student class loss: 0.0003\n",
            "[32/33] student class loss: 0.0004\n",
            "[33/33] student class loss: 0.0002\n",
            "Epochs 43  train loss 0.00050 train acc 1.00000 validate loss 0.91938 validate acc 0.82411\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0044 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0001\n",
            "[2/33] student class loss: 0.0004\n",
            "[3/33] student class loss: 0.0004\n",
            "[4/33] student class loss: 0.0007\n",
            "[5/33] student class loss: 0.0001\n",
            "[6/33] student class loss: 0.0011\n",
            "[7/33] student class loss: 0.0002\n",
            "[8/33] student class loss: 0.0002\n",
            "[9/33] student class loss: 0.0002\n",
            "[10/33] student class loss: 0.0005\n",
            "[11/33] student class loss: 0.0002\n",
            "[12/33] student class loss: 0.0003\n",
            "[13/33] student class loss: 0.0023\n",
            "[14/33] student class loss: 0.0002\n",
            "[15/33] student class loss: 0.0002\n",
            "[16/33] student class loss: 0.0001\n",
            "[17/33] student class loss: 0.0003\n",
            "[18/33] student class loss: 0.0003\n",
            "[19/33] student class loss: 0.0004\n",
            "[20/33] student class loss: 0.0003\n",
            "[21/33] student class loss: 0.0001\n",
            "[22/33] student class loss: 0.0005\n",
            "[23/33] student class loss: 0.0006\n",
            "[24/33] student class loss: 0.0001\n",
            "[25/33] student class loss: 0.0002\n",
            "[26/33] student class loss: 0.0008\n",
            "[27/33] student class loss: 0.0001\n",
            "[28/33] student class loss: 0.0010\n",
            "[29/33] student class loss: 0.0002\n",
            "[30/33] student class loss: 0.0005\n",
            "[31/33] student class loss: 0.0002\n",
            "[32/33] student class loss: 0.0002\n",
            "[33/33] student class loss: 0.0003\n",
            "Epochs 44  train loss 0.00041 train acc 1.00000 validate loss 0.91890 validate acc 0.82270\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0045 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0005\n",
            "[2/33] student class loss: 0.0001\n",
            "[3/33] student class loss: 0.0001\n",
            "[4/33] student class loss: 0.0002\n",
            "[5/33] student class loss: 0.0001\n",
            "[6/33] student class loss: 0.0001\n",
            "[7/33] student class loss: 0.0003\n",
            "[8/33] student class loss: 0.0005\n",
            "[9/33] student class loss: 0.0002\n",
            "[10/33] student class loss: 0.0006\n",
            "[11/33] student class loss: 0.0001\n",
            "[12/33] student class loss: 0.0002\n",
            "[13/33] student class loss: 0.0007\n",
            "[14/33] student class loss: 0.0001\n",
            "[15/33] student class loss: 0.0002\n",
            "[16/33] student class loss: 0.0001\n",
            "[17/33] student class loss: 0.0003\n",
            "[18/33] student class loss: 0.0002\n",
            "[19/33] student class loss: 0.0002\n",
            "[20/33] student class loss: 0.0004\n",
            "[21/33] student class loss: 0.0012\n",
            "[22/33] student class loss: 0.0011\n",
            "[23/33] student class loss: 0.0012\n",
            "[24/33] student class loss: 0.0002\n",
            "[25/33] student class loss: 0.0001\n",
            "[26/33] student class loss: 0.0007\n",
            "[27/33] student class loss: 0.0005\n",
            "[28/33] student class loss: 0.0002\n",
            "[29/33] student class loss: 0.0001\n",
            "[30/33] student class loss: 0.0001\n",
            "[31/33] student class loss: 0.0002\n",
            "[32/33] student class loss: 0.0018\n",
            "[33/33] student class loss: 0.0005\n",
            "Epochs 45  train loss 0.00039 train acc 1.00000 validate loss 0.91917 validate acc 0.82270\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0046 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0001\n",
            "[2/33] student class loss: 0.0001\n",
            "[3/33] student class loss: 0.0007\n",
            "[4/33] student class loss: 0.0010\n",
            "[5/33] student class loss: 0.0002\n",
            "[6/33] student class loss: 0.0002\n",
            "[7/33] student class loss: 0.0002\n",
            "[8/33] student class loss: 0.0006\n",
            "[9/33] student class loss: 0.0002\n",
            "[10/33] student class loss: 0.0003\n",
            "[11/33] student class loss: 0.0002\n",
            "[12/33] student class loss: 0.0004\n",
            "[13/33] student class loss: 0.0003\n",
            "[14/33] student class loss: 0.0003\n",
            "[15/33] student class loss: 0.0004\n",
            "[16/33] student class loss: 0.0002\n",
            "[17/33] student class loss: 0.0002\n",
            "[18/33] student class loss: 0.0004\n",
            "[19/33] student class loss: 0.0012\n",
            "[20/33] student class loss: 0.0001\n",
            "[21/33] student class loss: 0.0001\n",
            "[22/33] student class loss: 0.0003\n",
            "[23/33] student class loss: 0.0003\n",
            "[24/33] student class loss: 0.0005\n",
            "[25/33] student class loss: 0.0002\n",
            "[26/33] student class loss: 0.0001\n",
            "[27/33] student class loss: 0.0003\n",
            "[28/33] student class loss: 0.0002\n",
            "[29/33] student class loss: 0.0004\n",
            "[30/33] student class loss: 0.0005\n",
            "[31/33] student class loss: 0.0003\n",
            "[32/33] student class loss: 0.0005\n",
            "[33/33] student class loss: 0.0001\n",
            "Epochs 46  train loss 0.00033 train acc 1.00000 validate loss 0.91819 validate acc 0.82270\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0047 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0004\n",
            "[2/33] student class loss: 0.0006\n",
            "[3/33] student class loss: 0.0003\n",
            "[4/33] student class loss: 0.0002\n",
            "[5/33] student class loss: 0.0002\n",
            "[6/33] student class loss: 0.0002\n",
            "[7/33] student class loss: 0.0002\n",
            "[8/33] student class loss: 0.0004\n",
            "[9/33] student class loss: 0.0001\n",
            "[10/33] student class loss: 0.0003\n",
            "[11/33] student class loss: 0.0002\n",
            "[12/33] student class loss: 0.0002\n",
            "[13/33] student class loss: 0.0002\n",
            "[14/33] student class loss: 0.0001\n",
            "[15/33] student class loss: 0.0001\n",
            "[16/33] student class loss: 0.0042\n",
            "[17/33] student class loss: 0.0001\n",
            "[18/33] student class loss: 0.0005\n",
            "[19/33] student class loss: 0.0001\n",
            "[20/33] student class loss: 0.0012\n",
            "[21/33] student class loss: 0.0002\n",
            "[22/33] student class loss: 0.0005\n",
            "[23/33] student class loss: 0.0026\n",
            "[24/33] student class loss: 0.0007\n",
            "[25/33] student class loss: 0.0002\n",
            "[26/33] student class loss: 0.0001\n",
            "[27/33] student class loss: 0.0011\n",
            "[28/33] student class loss: 0.0002\n",
            "[29/33] student class loss: 0.0009\n",
            "[30/33] student class loss: 0.0005\n",
            "[31/33] student class loss: 0.0006\n",
            "[32/33] student class loss: 0.0001\n",
            "[33/33] student class loss: 0.0003\n",
            "Epochs 47  train loss 0.00054 train acc 1.00000 validate loss 0.91869 validate acc 0.82270\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0048 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0003\n",
            "[2/33] student class loss: 0.0002\n",
            "[3/33] student class loss: 0.0004\n",
            "[4/33] student class loss: 0.0001\n",
            "[5/33] student class loss: 0.0003\n",
            "[6/33] student class loss: 0.0004\n",
            "[7/33] student class loss: 0.0004\n",
            "[8/33] student class loss: 0.0001\n",
            "[9/33] student class loss: 0.0002\n",
            "[10/33] student class loss: 0.0008\n",
            "[11/33] student class loss: 0.0002\n",
            "[12/33] student class loss: 0.0015\n",
            "[13/33] student class loss: 0.0002\n",
            "[14/33] student class loss: 0.0002\n",
            "[15/33] student class loss: 0.0001\n",
            "[16/33] student class loss: 0.0004\n",
            "[17/33] student class loss: 0.0003\n",
            "[18/33] student class loss: 0.0008\n",
            "[19/33] student class loss: 0.0003\n",
            "[20/33] student class loss: 0.0010\n",
            "[21/33] student class loss: 0.0001\n",
            "[22/33] student class loss: 0.0009\n",
            "[23/33] student class loss: 0.0002\n",
            "[24/33] student class loss: 0.0001\n",
            "[25/33] student class loss: 0.0005\n",
            "[26/33] student class loss: 0.0002\n",
            "[27/33] student class loss: 0.0001\n",
            "[28/33] student class loss: 0.0003\n",
            "[29/33] student class loss: 0.0001\n",
            "[30/33] student class loss: 0.0002\n",
            "[31/33] student class loss: 0.0002\n",
            "[32/33] student class loss: 0.0003\n",
            "[33/33] student class loss: 0.0001\n",
            "Epochs 48  train loss 0.00034 train acc 1.00000 validate loss 0.92014 validate acc 0.82270\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0049 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0001\n",
            "[2/33] student class loss: 0.0001\n",
            "[3/33] student class loss: 0.0002\n",
            "[4/33] student class loss: 0.0001\n",
            "[5/33] student class loss: 0.0002\n",
            "[6/33] student class loss: 0.0002\n",
            "[7/33] student class loss: 0.0004\n",
            "[8/33] student class loss: 0.0002\n",
            "[9/33] student class loss: 0.0015\n",
            "[10/33] student class loss: 0.0003\n",
            "[11/33] student class loss: 0.0005\n",
            "[12/33] student class loss: 0.0001\n",
            "[13/33] student class loss: 0.0002\n",
            "[14/33] student class loss: 0.0003\n",
            "[15/33] student class loss: 0.0012\n",
            "[16/33] student class loss: 0.0002\n",
            "[17/33] student class loss: 0.0016\n",
            "[18/33] student class loss: 0.0003\n",
            "[19/33] student class loss: 0.0002\n",
            "[20/33] student class loss: 0.0002\n",
            "[21/33] student class loss: 0.0002\n",
            "[22/33] student class loss: 0.0002\n",
            "[23/33] student class loss: 0.0006\n",
            "[24/33] student class loss: 0.0006\n",
            "[25/33] student class loss: 0.0001\n",
            "[26/33] student class loss: 0.0000\n",
            "[27/33] student class loss: 0.0009\n",
            "[28/33] student class loss: 0.0002\n",
            "[29/33] student class loss: 0.0004\n",
            "[30/33] student class loss: 0.0013\n",
            "[31/33] student class loss: 0.0004\n",
            "[32/33] student class loss: 0.0001\n",
            "[33/33] student class loss: 0.0002\n",
            "Epochs 49  train loss 0.00040 train acc 1.00000 validate loss 0.91990 validate acc 0.82270\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0050 / 0050\n",
            "============\n",
            "[1/33] student class loss: 0.0002\n",
            "[2/33] student class loss: 0.0007\n",
            "[3/33] student class loss: 0.0002\n",
            "[4/33] student class loss: 0.0001\n",
            "[5/33] student class loss: 0.0020\n",
            "[6/33] student class loss: 0.0004\n",
            "[7/33] student class loss: 0.0004\n",
            "[8/33] student class loss: 0.0001\n",
            "[9/33] student class loss: 0.0002\n",
            "[10/33] student class loss: 0.0001\n",
            "[11/33] student class loss: 0.0007\n",
            "[12/33] student class loss: 0.0003\n",
            "[13/33] student class loss: 0.0002\n",
            "[14/33] student class loss: 0.0002\n",
            "[15/33] student class loss: 0.0002\n",
            "[16/33] student class loss: 0.0002\n",
            "[17/33] student class loss: 0.0002\n",
            "[18/33] student class loss: 0.0003\n",
            "[19/33] student class loss: 0.0006\n",
            "[20/33] student class loss: 0.0001\n",
            "[21/33] student class loss: 0.0003\n",
            "[22/33] student class loss: 0.0002\n",
            "[23/33] student class loss: 0.0001\n",
            "[24/33] student class loss: 0.0006\n",
            "[25/33] student class loss: 0.0002\n",
            "[26/33] student class loss: 0.0004\n",
            "[27/33] student class loss: 0.0006\n",
            "[28/33] student class loss: 0.0001\n",
            "[29/33] student class loss: 0.0005\n",
            "[30/33] student class loss: 0.0002\n",
            "[31/33] student class loss: 0.0003\n",
            "[32/33] student class loss: 0.0002\n",
            "[33/33] student class loss: 0.0003\n",
            "Epochs 50  train loss 0.00034 train acc 1.00000 validate loss 0.92112 validate acc 0.82411\n",
            "--------------------------------------------------------------------------------\n",
            "Time consumption for student net (device:cuda): 500.2702443599701 sec\n",
            "Training logs saved to LOG_BL_amazon\n",
            "Epoch 0001 / 0050\n",
            "============\n",
            "[1/6] student class loss: 10.7392\n",
            "[2/6] student class loss: 10.6144\n",
            "[3/6] student class loss: 10.6384\n",
            "[4/6] student class loss: 9.8813\n",
            "[5/6] student class loss: 10.5923\n",
            "[6/6] student class loss: 9.7397\n",
            "Epochs 1   train loss 10.36755 train acc 0.00000 validate loss 14.81487 validate acc 0.00000\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0002 / 0050\n",
            "============\n",
            "[1/6] student class loss: 9.5215\n",
            "[2/6] student class loss: 8.9981\n",
            "[3/6] student class loss: 8.8480\n",
            "[4/6] student class loss: 8.7588\n",
            "[5/6] student class loss: 9.0430\n",
            "[6/6] student class loss: 8.7837\n",
            "Epochs 2   train loss 8.99218 train acc 0.00804 validate loss 12.33019 validate acc 0.00000\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0003 / 0050\n",
            "============\n",
            "[1/6] student class loss: 7.9919\n",
            "[2/6] student class loss: 7.6842\n",
            "[3/6] student class loss: 7.4579\n",
            "[4/6] student class loss: 6.7393\n",
            "[5/6] student class loss: 6.7355\n",
            "[6/6] student class loss: 6.5576\n",
            "Epochs 3   train loss 7.19440 train acc 0.03753 validate loss 9.25855 validate acc 0.00000\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0004 / 0050\n",
            "============\n",
            "[1/6] student class loss: 5.9506\n",
            "[2/6] student class loss: 5.0068\n",
            "[3/6] student class loss: 5.3954\n",
            "[4/6] student class loss: 5.1298\n",
            "[5/6] student class loss: 4.0060\n",
            "[6/6] student class loss: 4.1230\n",
            "Epochs 4   train loss 4.93527 train acc 0.15818 validate loss 6.16399 validate acc 0.06400\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0005 / 0050\n",
            "============\n",
            "[1/6] student class loss: 3.5993\n",
            "[2/6] student class loss: 3.6027\n",
            "[3/6] student class loss: 2.6543\n",
            "[4/6] student class loss: 3.0080\n",
            "[5/6] student class loss: 2.5322\n",
            "[6/6] student class loss: 2.7581\n",
            "Epochs 5   train loss 3.02578 train acc 0.44772 validate loss 4.16349 validate acc 0.20000\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0006 / 0050\n",
            "============\n",
            "[1/6] student class loss: 1.8111\n",
            "[2/6] student class loss: 1.7708\n",
            "[3/6] student class loss: 2.1786\n",
            "[4/6] student class loss: 1.3236\n",
            "[5/6] student class loss: 1.2008\n",
            "[6/6] student class loss: 1.5879\n",
            "Epochs 6   train loss 1.64546 train acc 0.65684 validate loss 3.11739 validate acc 0.29600\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0007 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.9059\n",
            "[2/6] student class loss: 0.9519\n",
            "[3/6] student class loss: 0.7038\n",
            "[4/6] student class loss: 0.7306\n",
            "[5/6] student class loss: 0.6095\n",
            "[6/6] student class loss: 0.5060\n",
            "Epochs 7   train loss 0.73462 train acc 0.86059 validate loss 2.51348 validate acc 0.38400\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0008 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.2699\n",
            "[2/6] student class loss: 0.4482\n",
            "[3/6] student class loss: 0.3107\n",
            "[4/6] student class loss: 0.1183\n",
            "[5/6] student class loss: 0.2018\n",
            "[6/6] student class loss: 0.3151\n",
            "Epochs 8   train loss 0.27733 train acc 0.95442 validate loss 1.90872 validate acc 0.48800\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0009 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.1147\n",
            "[2/6] student class loss: 0.1207\n",
            "[3/6] student class loss: 0.0792\n",
            "[4/6] student class loss: 0.0643\n",
            "[5/6] student class loss: 0.1267\n",
            "[6/6] student class loss: 0.0819\n",
            "Epochs 9   train loss 0.09792 train acc 0.99196 validate loss 1.38863 validate acc 0.62400\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0010 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.1210\n",
            "[2/6] student class loss: 0.0563\n",
            "[3/6] student class loss: 0.0439\n",
            "[4/6] student class loss: 0.0259\n",
            "[5/6] student class loss: 0.0238\n",
            "[6/6] student class loss: 0.0260\n",
            "Epochs 10  train loss 0.04948 train acc 0.99732 validate loss 1.13957 validate acc 0.71200\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0011 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0265\n",
            "[2/6] student class loss: 0.0202\n",
            "[3/6] student class loss: 0.0224\n",
            "[4/6] student class loss: 0.0091\n",
            "[5/6] student class loss: 0.0085\n",
            "[6/6] student class loss: 0.0119\n",
            "Epochs 11  train loss 0.01642 train acc 1.00000 validate loss 1.00858 validate acc 0.71200\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0012 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0119\n",
            "[2/6] student class loss: 0.0090\n",
            "[3/6] student class loss: 0.0121\n",
            "[4/6] student class loss: 0.0104\n",
            "[5/6] student class loss: 0.0071\n",
            "[6/6] student class loss: 0.0089\n",
            "Epochs 12  train loss 0.00991 train acc 1.00000 validate loss 0.94585 validate acc 0.74400\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0013 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0084\n",
            "[2/6] student class loss: 0.0166\n",
            "[3/6] student class loss: 0.0064\n",
            "[4/6] student class loss: 0.0045\n",
            "[5/6] student class loss: 0.0049\n",
            "[6/6] student class loss: 0.0058\n",
            "Epochs 13  train loss 0.00778 train acc 1.00000 validate loss 0.90061 validate acc 0.76800\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0014 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0038\n",
            "[2/6] student class loss: 0.0069\n",
            "[3/6] student class loss: 0.0072\n",
            "[4/6] student class loss: 0.0165\n",
            "[5/6] student class loss: 0.0107\n",
            "[6/6] student class loss: 0.0072\n",
            "Epochs 14  train loss 0.00871 train acc 1.00000 validate loss 0.84245 validate acc 0.76800\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0015 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0050\n",
            "[2/6] student class loss: 0.0062\n",
            "[3/6] student class loss: 0.0045\n",
            "[4/6] student class loss: 0.0039\n",
            "[5/6] student class loss: 0.0031\n",
            "[6/6] student class loss: 0.0034\n",
            "Epochs 15  train loss 0.00436 train acc 1.00000 validate loss 0.79839 validate acc 0.77600\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0016 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0121\n",
            "[2/6] student class loss: 0.0019\n",
            "[3/6] student class loss: 0.0032\n",
            "[4/6] student class loss: 0.0025\n",
            "[5/6] student class loss: 0.0042\n",
            "[6/6] student class loss: 0.0086\n",
            "Epochs 16  train loss 0.00541 train acc 1.00000 validate loss 0.76983 validate acc 0.80800\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0017 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0044\n",
            "[2/6] student class loss: 0.0039\n",
            "[3/6] student class loss: 0.0049\n",
            "[4/6] student class loss: 0.0022\n",
            "[5/6] student class loss: 0.0037\n",
            "[6/6] student class loss: 0.0021\n",
            "Epochs 17  train loss 0.00354 train acc 1.00000 validate loss 0.75228 validate acc 0.81600\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0018 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0026\n",
            "[2/6] student class loss: 0.0033\n",
            "[3/6] student class loss: 0.0038\n",
            "[4/6] student class loss: 0.0019\n",
            "[5/6] student class loss: 0.0023\n",
            "[6/6] student class loss: 0.0025\n",
            "Epochs 18  train loss 0.00273 train acc 1.00000 validate loss 0.73661 validate acc 0.82400\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0019 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0019\n",
            "[2/6] student class loss: 0.0021\n",
            "[3/6] student class loss: 0.0021\n",
            "[4/6] student class loss: 0.0027\n",
            "[5/6] student class loss: 0.0031\n",
            "[6/6] student class loss: 0.0050\n",
            "Epochs 19  train loss 0.00283 train acc 1.00000 validate loss 0.72032 validate acc 0.81600\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0020 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0028\n",
            "[2/6] student class loss: 0.0038\n",
            "[3/6] student class loss: 0.0033\n",
            "[4/6] student class loss: 0.0033\n",
            "[5/6] student class loss: 0.0020\n",
            "[6/6] student class loss: 0.0044\n",
            "Epochs 20  train loss 0.00327 train acc 1.00000 validate loss 0.69100 validate acc 0.82400\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0021 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0013\n",
            "[2/6] student class loss: 0.0020\n",
            "[3/6] student class loss: 0.0022\n",
            "[4/6] student class loss: 0.0022\n",
            "[5/6] student class loss: 0.0025\n",
            "[6/6] student class loss: 0.0022\n",
            "Epochs 21  train loss 0.00205 train acc 1.00000 validate loss 0.66941 validate acc 0.81600\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0022 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0023\n",
            "[2/6] student class loss: 0.0025\n",
            "[3/6] student class loss: 0.0026\n",
            "[4/6] student class loss: 0.0035\n",
            "[5/6] student class loss: 0.0034\n",
            "[6/6] student class loss: 0.0037\n",
            "Epochs 22  train loss 0.00299 train acc 1.00000 validate loss 0.64964 validate acc 0.84000\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0023 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0028\n",
            "[2/6] student class loss: 0.0017\n",
            "[3/6] student class loss: 0.0021\n",
            "[4/6] student class loss: 0.0016\n",
            "[5/6] student class loss: 0.0017\n",
            "[6/6] student class loss: 0.0013\n",
            "Epochs 23  train loss 0.00187 train acc 1.00000 validate loss 0.63701 validate acc 0.84800\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0024 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0013\n",
            "[2/6] student class loss: 0.0018\n",
            "[3/6] student class loss: 0.0013\n",
            "[4/6] student class loss: 0.0016\n",
            "[5/6] student class loss: 0.0020\n",
            "[6/6] student class loss: 0.0019\n",
            "Epochs 24  train loss 0.00167 train acc 1.00000 validate loss 0.62884 validate acc 0.84000\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0025 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0019\n",
            "[2/6] student class loss: 0.0017\n",
            "[3/6] student class loss: 0.0011\n",
            "[4/6] student class loss: 0.0014\n",
            "[5/6] student class loss: 0.0012\n",
            "[6/6] student class loss: 0.0024\n",
            "Epochs 25  train loss 0.00163 train acc 1.00000 validate loss 0.62511 validate acc 0.84000\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0026 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0013\n",
            "[2/6] student class loss: 0.0016\n",
            "[3/6] student class loss: 0.0026\n",
            "[4/6] student class loss: 0.0017\n",
            "[5/6] student class loss: 0.0010\n",
            "[6/6] student class loss: 0.0017\n",
            "Epochs 26  train loss 0.00165 train acc 1.00000 validate loss 0.61862 validate acc 0.84800\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0027 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0017\n",
            "[2/6] student class loss: 0.0025\n",
            "[3/6] student class loss: 0.0012\n",
            "[4/6] student class loss: 0.0016\n",
            "[5/6] student class loss: 0.0010\n",
            "[6/6] student class loss: 0.0015\n",
            "Epochs 27  train loss 0.00160 train acc 1.00000 validate loss 0.61286 validate acc 0.84000\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0028 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0012\n",
            "[2/6] student class loss: 0.0021\n",
            "[3/6] student class loss: 0.0017\n",
            "[4/6] student class loss: 0.0013\n",
            "[5/6] student class loss: 0.0018\n",
            "[6/6] student class loss: 0.0015\n",
            "Epochs 28  train loss 0.00157 train acc 1.00000 validate loss 0.61040 validate acc 0.84000\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0029 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0012\n",
            "[2/6] student class loss: 0.0019\n",
            "[3/6] student class loss: 0.0010\n",
            "[4/6] student class loss: 0.0025\n",
            "[5/6] student class loss: 0.0015\n",
            "[6/6] student class loss: 0.0017\n",
            "Epochs 29  train loss 0.00162 train acc 1.00000 validate loss 0.60547 validate acc 0.84000\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0030 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0013\n",
            "[2/6] student class loss: 0.0017\n",
            "[3/6] student class loss: 0.0012\n",
            "[4/6] student class loss: 0.0010\n",
            "[5/6] student class loss: 0.0010\n",
            "[6/6] student class loss: 0.0015\n",
            "Epochs 30  train loss 0.00127 train acc 1.00000 validate loss 0.60030 validate acc 0.84800\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0031 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0012\n",
            "[2/6] student class loss: 0.0011\n",
            "[3/6] student class loss: 0.0014\n",
            "[4/6] student class loss: 0.0010\n",
            "[5/6] student class loss: 0.0013\n",
            "[6/6] student class loss: 0.0008\n",
            "Epochs 31  train loss 0.00115 train acc 1.00000 validate loss 0.59467 validate acc 0.85600\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0032 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0020\n",
            "[2/6] student class loss: 0.0013\n",
            "[3/6] student class loss: 0.0019\n",
            "[4/6] student class loss: 0.0009\n",
            "[5/6] student class loss: 0.0008\n",
            "[6/6] student class loss: 0.0017\n",
            "Epochs 32  train loss 0.00145 train acc 1.00000 validate loss 0.58874 validate acc 0.84800\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0033 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0012\n",
            "[2/6] student class loss: 0.0015\n",
            "[3/6] student class loss: 0.0012\n",
            "[4/6] student class loss: 0.0012\n",
            "[5/6] student class loss: 0.0013\n",
            "[6/6] student class loss: 0.0025\n",
            "Epochs 33  train loss 0.00147 train acc 1.00000 validate loss 0.58352 validate acc 0.85600\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0034 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0009\n",
            "[2/6] student class loss: 0.0014\n",
            "[3/6] student class loss: 0.0008\n",
            "[4/6] student class loss: 0.0015\n",
            "[5/6] student class loss: 0.0025\n",
            "[6/6] student class loss: 0.0011\n",
            "Epochs 34  train loss 0.00136 train acc 1.00000 validate loss 0.58122 validate acc 0.85600\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0035 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0012\n",
            "[2/6] student class loss: 0.0018\n",
            "[3/6] student class loss: 0.0010\n",
            "[4/6] student class loss: 0.0012\n",
            "[5/6] student class loss: 0.0012\n",
            "[6/6] student class loss: 0.0015\n",
            "Epochs 35  train loss 0.00131 train acc 1.00000 validate loss 0.57866 validate acc 0.85600\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0036 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0016\n",
            "[2/6] student class loss: 0.0037\n",
            "[3/6] student class loss: 0.0012\n",
            "[4/6] student class loss: 0.0015\n",
            "[5/6] student class loss: 0.0007\n",
            "[6/6] student class loss: 0.0016\n",
            "Epochs 36  train loss 0.00171 train acc 1.00000 validate loss 0.57547 validate acc 0.84800\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0037 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0011\n",
            "[2/6] student class loss: 0.0012\n",
            "[3/6] student class loss: 0.0010\n",
            "[4/6] student class loss: 0.0015\n",
            "[5/6] student class loss: 0.0018\n",
            "[6/6] student class loss: 0.0012\n",
            "Epochs 37  train loss 0.00129 train acc 1.00000 validate loss 0.57387 validate acc 0.84800\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0038 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0008\n",
            "[2/6] student class loss: 0.0017\n",
            "[3/6] student class loss: 0.0014\n",
            "[4/6] student class loss: 0.0011\n",
            "[5/6] student class loss: 0.0017\n",
            "[6/6] student class loss: 0.0009\n",
            "Epochs 38  train loss 0.00128 train acc 1.00000 validate loss 0.57199 validate acc 0.84800\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0039 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0016\n",
            "[2/6] student class loss: 0.0010\n",
            "[3/6] student class loss: 0.0014\n",
            "[4/6] student class loss: 0.0022\n",
            "[5/6] student class loss: 0.0014\n",
            "[6/6] student class loss: 0.0020\n",
            "Epochs 39  train loss 0.00159 train acc 1.00000 validate loss 0.57176 validate acc 0.84800\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0040 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0013\n",
            "[2/6] student class loss: 0.0022\n",
            "[3/6] student class loss: 0.0010\n",
            "[4/6] student class loss: 0.0009\n",
            "[5/6] student class loss: 0.0006\n",
            "[6/6] student class loss: 0.0009\n",
            "Epochs 40  train loss 0.00116 train acc 1.00000 validate loss 0.57147 validate acc 0.84800\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0041 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0006\n",
            "[2/6] student class loss: 0.0014\n",
            "[3/6] student class loss: 0.0012\n",
            "[4/6] student class loss: 0.0016\n",
            "[5/6] student class loss: 0.0015\n",
            "[6/6] student class loss: 0.0015\n",
            "Epochs 41  train loss 0.00131 train acc 1.00000 validate loss 0.56985 validate acc 0.84800\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0042 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0008\n",
            "[2/6] student class loss: 0.0011\n",
            "[3/6] student class loss: 0.0017\n",
            "[4/6] student class loss: 0.0010\n",
            "[5/6] student class loss: 0.0008\n",
            "[6/6] student class loss: 0.0013\n",
            "Epochs 42  train loss 0.00113 train acc 1.00000 validate loss 0.56923 validate acc 0.85600\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0043 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0016\n",
            "[2/6] student class loss: 0.0013\n",
            "[3/6] student class loss: 0.0013\n",
            "[4/6] student class loss: 0.0011\n",
            "[5/6] student class loss: 0.0012\n",
            "[6/6] student class loss: 0.0009\n",
            "Epochs 43  train loss 0.00125 train acc 1.00000 validate loss 0.56958 validate acc 0.85600\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0044 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0013\n",
            "[2/6] student class loss: 0.0006\n",
            "[3/6] student class loss: 0.0013\n",
            "[4/6] student class loss: 0.0012\n",
            "[5/6] student class loss: 0.0008\n",
            "[6/6] student class loss: 0.0008\n",
            "Epochs 44  train loss 0.00101 train acc 1.00000 validate loss 0.56977 validate acc 0.85600\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0045 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0009\n",
            "[2/6] student class loss: 0.0017\n",
            "[3/6] student class loss: 0.0024\n",
            "[4/6] student class loss: 0.0018\n",
            "[5/6] student class loss: 0.0006\n",
            "[6/6] student class loss: 0.0014\n",
            "Epochs 45  train loss 0.00147 train acc 1.00000 validate loss 0.56920 validate acc 0.85600\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0046 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0007\n",
            "[2/6] student class loss: 0.0012\n",
            "[3/6] student class loss: 0.0014\n",
            "[4/6] student class loss: 0.0013\n",
            "[5/6] student class loss: 0.0007\n",
            "[6/6] student class loss: 0.0008\n",
            "Epochs 46  train loss 0.00102 train acc 1.00000 validate loss 0.56912 validate acc 0.85600\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0047 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0010\n",
            "[2/6] student class loss: 0.0018\n",
            "[3/6] student class loss: 0.0010\n",
            "[4/6] student class loss: 0.0010\n",
            "[5/6] student class loss: 0.0010\n",
            "[6/6] student class loss: 0.0012\n",
            "Epochs 47  train loss 0.00117 train acc 1.00000 validate loss 0.56850 validate acc 0.85600\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0048 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0009\n",
            "[2/6] student class loss: 0.0016\n",
            "[3/6] student class loss: 0.0007\n",
            "[4/6] student class loss: 0.0006\n",
            "[5/6] student class loss: 0.0016\n",
            "[6/6] student class loss: 0.0019\n",
            "Epochs 48  train loss 0.00124 train acc 1.00000 validate loss 0.56781 validate acc 0.85600\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0049 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0026\n",
            "[2/6] student class loss: 0.0021\n",
            "[3/6] student class loss: 0.0010\n",
            "[4/6] student class loss: 0.0005\n",
            "[5/6] student class loss: 0.0021\n",
            "[6/6] student class loss: 0.0011\n",
            "Epochs 49  train loss 0.00158 train acc 1.00000 validate loss 0.56861 validate acc 0.85600\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0050 / 0050\n",
            "============\n",
            "[1/6] student class loss: 0.0008\n",
            "[2/6] student class loss: 0.0008\n",
            "[3/6] student class loss: 0.0010\n",
            "[4/6] student class loss: 0.0020\n",
            "[5/6] student class loss: 0.0013\n",
            "[6/6] student class loss: 0.0011\n",
            "Epochs 50  train loss 0.00116 train acc 1.00000 validate loss 0.56831 validate acc 0.85600\n",
            "--------------------------------------------------------------------------------\n",
            "Time consumption for student net (device:cuda): 285.3307671546936 sec\n",
            "Training logs saved to LOG_BL_dslr\n",
            "Epoch 0001 / 0050\n",
            "============\n",
            "[1/10] student class loss: 10.4069\n",
            "[2/10] student class loss: 10.4404\n",
            "[3/10] student class loss: 9.9628\n",
            "[4/10] student class loss: 10.0032\n",
            "[5/10] student class loss: 9.5198\n",
            "[6/10] student class loss: 10.0890\n",
            "[7/10] student class loss: 9.1252\n",
            "[8/10] student class loss: 9.1672\n",
            "[9/10] student class loss: 9.4217\n",
            "[10/10] student class loss: 9.0001\n",
            "Epochs 1   train loss 9.71363 train acc 0.00000 validate loss 12.38301 validate acc 0.00000\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0002 / 0050\n",
            "============\n",
            "[1/10] student class loss: 8.4319\n",
            "[2/10] student class loss: 8.1709\n",
            "[3/10] student class loss: 8.7053\n",
            "[4/10] student class loss: 8.4083\n",
            "[5/10] student class loss: 6.9616\n",
            "[6/10] student class loss: 7.6156\n",
            "[7/10] student class loss: 7.2548\n",
            "[8/10] student class loss: 7.1327\n",
            "[9/10] student class loss: 7.0871\n",
            "[10/10] student class loss: 6.8482\n",
            "Epochs 2   train loss 7.66164 train acc 0.01846 validate loss 9.19176 validate acc 0.00503\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0003 / 0050\n",
            "============\n",
            "[1/10] student class loss: 6.5311\n",
            "[2/10] student class loss: 6.3940\n",
            "[3/10] student class loss: 5.7957\n",
            "[4/10] student class loss: 5.9588\n",
            "[5/10] student class loss: 5.0058\n",
            "[6/10] student class loss: 5.4695\n",
            "[7/10] student class loss: 4.9202\n",
            "[8/10] student class loss: 5.2126\n",
            "[9/10] student class loss: 4.0297\n",
            "[10/10] student class loss: 4.0861\n",
            "Epochs 3   train loss 5.34034 train acc 0.12584 validate loss 6.35219 validate acc 0.04020\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0004 / 0050\n",
            "============\n",
            "[1/10] student class loss: 3.9133\n",
            "[2/10] student class loss: 4.1558\n",
            "[3/10] student class loss: 3.5797\n",
            "[4/10] student class loss: 3.4549\n",
            "[5/10] student class loss: 3.8461\n",
            "[6/10] student class loss: 3.0574\n",
            "[7/10] student class loss: 2.8152\n",
            "[8/10] student class loss: 2.6431\n",
            "[9/10] student class loss: 3.1402\n",
            "[10/10] student class loss: 3.5774\n",
            "Epochs 4   train loss 3.41832 train acc 0.31879 validate loss 4.39356 validate acc 0.18090\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0005 / 0050\n",
            "============\n",
            "[1/10] student class loss: 2.2335\n",
            "[2/10] student class loss: 2.1403\n",
            "[3/10] student class loss: 2.0088\n",
            "[4/10] student class loss: 1.9252\n",
            "[5/10] student class loss: 2.1280\n",
            "[6/10] student class loss: 1.4986\n",
            "[7/10] student class loss: 1.3540\n",
            "[8/10] student class loss: 1.3244\n",
            "[9/10] student class loss: 1.5869\n",
            "[10/10] student class loss: 1.2543\n",
            "Epochs 5   train loss 1.74542 train acc 0.62081 validate loss 2.93334 validate acc 0.40704\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0006 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.8775\n",
            "[2/10] student class loss: 1.0878\n",
            "[3/10] student class loss: 0.7311\n",
            "[4/10] student class loss: 0.7937\n",
            "[5/10] student class loss: 0.8486\n",
            "[6/10] student class loss: 0.6456\n",
            "[7/10] student class loss: 0.5581\n",
            "[8/10] student class loss: 0.7443\n",
            "[9/10] student class loss: 0.6251\n",
            "[10/10] student class loss: 0.2525\n",
            "Epochs 6   train loss 0.71644 train acc 0.84228 validate loss 1.83174 validate acc 0.57286\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0007 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.4794\n",
            "[2/10] student class loss: 0.3294\n",
            "[3/10] student class loss: 0.3652\n",
            "[4/10] student class loss: 0.3557\n",
            "[5/10] student class loss: 0.4148\n",
            "[6/10] student class loss: 0.1667\n",
            "[7/10] student class loss: 0.1685\n",
            "[8/10] student class loss: 0.2595\n",
            "[9/10] student class loss: 0.1748\n",
            "[10/10] student class loss: 0.2507\n",
            "Epochs 7   train loss 0.29646 train acc 0.94799 validate loss 1.25167 validate acc 0.71357\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0008 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.1749\n",
            "[2/10] student class loss: 0.1037\n",
            "[3/10] student class loss: 0.1316\n",
            "[4/10] student class loss: 0.0898\n",
            "[5/10] student class loss: 0.0849\n",
            "[6/10] student class loss: 0.0432\n",
            "[7/10] student class loss: 0.0933\n",
            "[8/10] student class loss: 0.0683\n",
            "[9/10] student class loss: 0.1606\n",
            "[10/10] student class loss: 0.1176\n",
            "Epochs 8   train loss 0.10680 train acc 0.99161 validate loss 0.94523 validate acc 0.77387\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0009 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0414\n",
            "[2/10] student class loss: 0.0840\n",
            "[3/10] student class loss: 0.0512\n",
            "[4/10] student class loss: 0.0274\n",
            "[5/10] student class loss: 0.0357\n",
            "[6/10] student class loss: 0.0616\n",
            "[7/10] student class loss: 0.0425\n",
            "[8/10] student class loss: 0.0383\n",
            "[9/10] student class loss: 0.0827\n",
            "[10/10] student class loss: 0.0259\n",
            "Epochs 9   train loss 0.04909 train acc 0.99497 validate loss 0.71456 validate acc 0.83417\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0010 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0239\n",
            "[2/10] student class loss: 0.0344\n",
            "[3/10] student class loss: 0.0212\n",
            "[4/10] student class loss: 0.0210\n",
            "[5/10] student class loss: 0.0151\n",
            "[6/10] student class loss: 0.0113\n",
            "[7/10] student class loss: 0.0197\n",
            "[8/10] student class loss: 0.0248\n",
            "[9/10] student class loss: 0.0126\n",
            "[10/10] student class loss: 0.0172\n",
            "Epochs 10  train loss 0.02013 train acc 1.00000 validate loss 0.52755 validate acc 0.85930\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0011 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0180\n",
            "[2/10] student class loss: 0.0224\n",
            "[3/10] student class loss: 0.0161\n",
            "[4/10] student class loss: 0.0169\n",
            "[5/10] student class loss: 0.0080\n",
            "[6/10] student class loss: 0.0115\n",
            "[7/10] student class loss: 0.0100\n",
            "[8/10] student class loss: 0.0106\n",
            "[9/10] student class loss: 0.0100\n",
            "[10/10] student class loss: 0.0286\n",
            "Epochs 11  train loss 0.01520 train acc 1.00000 validate loss 0.44530 validate acc 0.88945\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0012 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0101\n",
            "[2/10] student class loss: 0.0072\n",
            "[3/10] student class loss: 0.0080\n",
            "[4/10] student class loss: 0.0089\n",
            "[5/10] student class loss: 0.0104\n",
            "[6/10] student class loss: 0.0068\n",
            "[7/10] student class loss: 0.0113\n",
            "[8/10] student class loss: 0.0060\n",
            "[9/10] student class loss: 0.0138\n",
            "[10/10] student class loss: 0.0649\n",
            "Epochs 12  train loss 0.01475 train acc 1.00000 validate loss 0.42603 validate acc 0.89950\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0013 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0084\n",
            "[2/10] student class loss: 0.0072\n",
            "[3/10] student class loss: 0.0025\n",
            "[4/10] student class loss: 0.0050\n",
            "[5/10] student class loss: 0.0063\n",
            "[6/10] student class loss: 0.0073\n",
            "[7/10] student class loss: 0.0052\n",
            "[8/10] student class loss: 0.0057\n",
            "[9/10] student class loss: 0.0054\n",
            "[10/10] student class loss: 0.1412\n",
            "Epochs 13  train loss 0.01943 train acc 1.00000 validate loss 0.42003 validate acc 0.89447\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0014 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0055\n",
            "[2/10] student class loss: 0.0057\n",
            "[3/10] student class loss: 0.0133\n",
            "[4/10] student class loss: 0.0189\n",
            "[5/10] student class loss: 0.0071\n",
            "[6/10] student class loss: 0.0125\n",
            "[7/10] student class loss: 0.0082\n",
            "[8/10] student class loss: 0.0175\n",
            "[9/10] student class loss: 0.0072\n",
            "[10/10] student class loss: 0.0118\n",
            "Epochs 14  train loss 0.01078 train acc 1.00000 validate loss 0.42711 validate acc 0.87437\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0015 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0063\n",
            "[2/10] student class loss: 0.0056\n",
            "[3/10] student class loss: 0.0078\n",
            "[4/10] student class loss: 0.0175\n",
            "[5/10] student class loss: 0.0069\n",
            "[6/10] student class loss: 0.0052\n",
            "[7/10] student class loss: 0.0167\n",
            "[8/10] student class loss: 0.0242\n",
            "[9/10] student class loss: 0.0038\n",
            "[10/10] student class loss: 0.0215\n",
            "Epochs 15  train loss 0.01158 train acc 1.00000 validate loss 0.37805 validate acc 0.91457\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0016 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0070\n",
            "[2/10] student class loss: 0.0082\n",
            "[3/10] student class loss: 0.0037\n",
            "[4/10] student class loss: 0.0069\n",
            "[5/10] student class loss: 0.0030\n",
            "[6/10] student class loss: 0.0044\n",
            "[7/10] student class loss: 0.0053\n",
            "[8/10] student class loss: 0.0028\n",
            "[9/10] student class loss: 0.0065\n",
            "[10/10] student class loss: 0.0148\n",
            "Epochs 16  train loss 0.00626 train acc 1.00000 validate loss 0.33963 validate acc 0.91457\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0017 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0025\n",
            "[2/10] student class loss: 0.0031\n",
            "[3/10] student class loss: 0.0053\n",
            "[4/10] student class loss: 0.0032\n",
            "[5/10] student class loss: 0.0036\n",
            "[6/10] student class loss: 0.0033\n",
            "[7/10] student class loss: 0.0033\n",
            "[8/10] student class loss: 0.0046\n",
            "[9/10] student class loss: 0.0079\n",
            "[10/10] student class loss: 0.0133\n",
            "Epochs 17  train loss 0.00499 train acc 1.00000 validate loss 0.28575 validate acc 0.92462\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0018 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0041\n",
            "[2/10] student class loss: 0.0021\n",
            "[3/10] student class loss: 0.0028\n",
            "[4/10] student class loss: 0.0019\n",
            "[5/10] student class loss: 0.0060\n",
            "[6/10] student class loss: 0.0032\n",
            "[7/10] student class loss: 0.0024\n",
            "[8/10] student class loss: 0.0038\n",
            "[9/10] student class loss: 0.0051\n",
            "[10/10] student class loss: 0.0204\n",
            "Epochs 18  train loss 0.00518 train acc 1.00000 validate loss 0.25990 validate acc 0.92965\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0019 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0031\n",
            "[2/10] student class loss: 0.0024\n",
            "[3/10] student class loss: 0.0099\n",
            "[4/10] student class loss: 0.0022\n",
            "[5/10] student class loss: 0.0035\n",
            "[6/10] student class loss: 0.0033\n",
            "[7/10] student class loss: 0.0020\n",
            "[8/10] student class loss: 0.0028\n",
            "[9/10] student class loss: 0.0037\n",
            "[10/10] student class loss: 0.0085\n",
            "Epochs 19  train loss 0.00415 train acc 0.99832 validate loss 0.27603 validate acc 0.91960\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0020 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0088\n",
            "[2/10] student class loss: 0.0031\n",
            "[3/10] student class loss: 0.0020\n",
            "[4/10] student class loss: 0.0020\n",
            "[5/10] student class loss: 0.0024\n",
            "[6/10] student class loss: 0.0020\n",
            "[7/10] student class loss: 0.0034\n",
            "[8/10] student class loss: 0.0021\n",
            "[9/10] student class loss: 0.0433\n",
            "[10/10] student class loss: 0.0251\n",
            "Epochs 20  train loss 0.00941 train acc 1.00000 validate loss 0.27831 validate acc 0.90955\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0021 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0027\n",
            "[2/10] student class loss: 0.0019\n",
            "[3/10] student class loss: 0.0023\n",
            "[4/10] student class loss: 0.0017\n",
            "[5/10] student class loss: 0.0020\n",
            "[6/10] student class loss: 0.0097\n",
            "[7/10] student class loss: 0.0020\n",
            "[8/10] student class loss: 0.0016\n",
            "[9/10] student class loss: 0.0053\n",
            "[10/10] student class loss: 0.0326\n",
            "Epochs 21  train loss 0.00618 train acc 0.99832 validate loss 0.20326 validate acc 0.92965\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0022 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0051\n",
            "[2/10] student class loss: 0.0033\n",
            "[3/10] student class loss: 0.0066\n",
            "[4/10] student class loss: 0.0030\n",
            "[5/10] student class loss: 0.0021\n",
            "[6/10] student class loss: 0.0021\n",
            "[7/10] student class loss: 0.0019\n",
            "[8/10] student class loss: 0.0042\n",
            "[9/10] student class loss: 0.0079\n",
            "[10/10] student class loss: 0.0025\n",
            "Epochs 22  train loss 0.00385 train acc 1.00000 validate loss 0.22761 validate acc 0.92462\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0023 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0036\n",
            "[2/10] student class loss: 0.0021\n",
            "[3/10] student class loss: 0.0024\n",
            "[4/10] student class loss: 0.0034\n",
            "[5/10] student class loss: 0.0031\n",
            "[6/10] student class loss: 0.0222\n",
            "[7/10] student class loss: 0.0016\n",
            "[8/10] student class loss: 0.0015\n",
            "[9/10] student class loss: 0.0010\n",
            "[10/10] student class loss: 0.0040\n",
            "Epochs 23  train loss 0.00450 train acc 1.00000 validate loss 0.19850 validate acc 0.92965\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0024 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0015\n",
            "[2/10] student class loss: 0.0021\n",
            "[3/10] student class loss: 0.0029\n",
            "[4/10] student class loss: 0.0025\n",
            "[5/10] student class loss: 0.0014\n",
            "[6/10] student class loss: 0.0014\n",
            "[7/10] student class loss: 0.0030\n",
            "[8/10] student class loss: 0.0013\n",
            "[9/10] student class loss: 0.0018\n",
            "[10/10] student class loss: 0.0849\n",
            "Epochs 24  train loss 0.01026 train acc 1.00000 validate loss 0.17915 validate acc 0.94472\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0025 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0022\n",
            "[2/10] student class loss: 0.0025\n",
            "[3/10] student class loss: 0.0017\n",
            "[4/10] student class loss: 0.0022\n",
            "[5/10] student class loss: 0.0052\n",
            "[6/10] student class loss: 0.0060\n",
            "[7/10] student class loss: 0.0021\n",
            "[8/10] student class loss: 0.0056\n",
            "[9/10] student class loss: 0.0016\n",
            "[10/10] student class loss: 0.0095\n",
            "Epochs 25  train loss 0.00385 train acc 0.99832 validate loss 0.20777 validate acc 0.93970\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0026 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0290\n",
            "[2/10] student class loss: 0.0021\n",
            "[3/10] student class loss: 0.0025\n",
            "[4/10] student class loss: 0.0015\n",
            "[5/10] student class loss: 0.0051\n",
            "[6/10] student class loss: 0.0016\n",
            "[7/10] student class loss: 0.0022\n",
            "[8/10] student class loss: 0.0021\n",
            "[9/10] student class loss: 0.0025\n",
            "[10/10] student class loss: 0.0076\n",
            "Epochs 26  train loss 0.00561 train acc 1.00000 validate loss 0.22249 validate acc 0.93467\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0027 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0025\n",
            "[2/10] student class loss: 0.0027\n",
            "[3/10] student class loss: 0.0029\n",
            "[4/10] student class loss: 0.0011\n",
            "[5/10] student class loss: 0.0023\n",
            "[6/10] student class loss: 0.0036\n",
            "[7/10] student class loss: 0.0021\n",
            "[8/10] student class loss: 0.0015\n",
            "[9/10] student class loss: 0.0017\n",
            "[10/10] student class loss: 0.0054\n",
            "Epochs 27  train loss 0.00258 train acc 1.00000 validate loss 0.21548 validate acc 0.93467\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0028 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0013\n",
            "[2/10] student class loss: 0.0017\n",
            "[3/10] student class loss: 0.0012\n",
            "[4/10] student class loss: 0.0009\n",
            "[5/10] student class loss: 0.0020\n",
            "[6/10] student class loss: 0.0024\n",
            "[7/10] student class loss: 0.0019\n",
            "[8/10] student class loss: 0.0016\n",
            "[9/10] student class loss: 0.0031\n",
            "[10/10] student class loss: 0.0028\n",
            "Epochs 28  train loss 0.00189 train acc 1.00000 validate loss 0.20259 validate acc 0.93467\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0029 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0017\n",
            "[2/10] student class loss: 0.0019\n",
            "[3/10] student class loss: 0.0026\n",
            "[4/10] student class loss: 0.0013\n",
            "[5/10] student class loss: 0.0008\n",
            "[6/10] student class loss: 0.0008\n",
            "[7/10] student class loss: 0.0047\n",
            "[8/10] student class loss: 0.0019\n",
            "[9/10] student class loss: 0.0022\n",
            "[10/10] student class loss: 0.0034\n",
            "Epochs 29  train loss 0.00213 train acc 1.00000 validate loss 0.19224 validate acc 0.94472\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0030 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0021\n",
            "[2/10] student class loss: 0.0014\n",
            "[3/10] student class loss: 0.0013\n",
            "[4/10] student class loss: 0.0011\n",
            "[5/10] student class loss: 0.0011\n",
            "[6/10] student class loss: 0.0040\n",
            "[7/10] student class loss: 0.0015\n",
            "[8/10] student class loss: 0.0010\n",
            "[9/10] student class loss: 0.0008\n",
            "[10/10] student class loss: 0.0076\n",
            "Epochs 30  train loss 0.00219 train acc 1.00000 validate loss 0.18634 validate acc 0.94472\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0031 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0008\n",
            "[2/10] student class loss: 0.0013\n",
            "[3/10] student class loss: 0.0010\n",
            "[4/10] student class loss: 0.0022\n",
            "[5/10] student class loss: 0.0027\n",
            "[6/10] student class loss: 0.0022\n",
            "[7/10] student class loss: 0.0008\n",
            "[8/10] student class loss: 0.0009\n",
            "[9/10] student class loss: 0.0013\n",
            "[10/10] student class loss: 0.0029\n",
            "Epochs 31  train loss 0.00160 train acc 1.00000 validate loss 0.17828 validate acc 0.93970\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0032 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0011\n",
            "[2/10] student class loss: 0.0010\n",
            "[3/10] student class loss: 0.0006\n",
            "[4/10] student class loss: 0.0020\n",
            "[5/10] student class loss: 0.0010\n",
            "[6/10] student class loss: 0.0014\n",
            "[7/10] student class loss: 0.0007\n",
            "[8/10] student class loss: 0.0022\n",
            "[9/10] student class loss: 0.0023\n",
            "[10/10] student class loss: 0.0141\n",
            "Epochs 32  train loss 0.00263 train acc 1.00000 validate loss 0.17601 validate acc 0.93970\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0033 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0007\n",
            "[2/10] student class loss: 0.0020\n",
            "[3/10] student class loss: 0.0012\n",
            "[4/10] student class loss: 0.0008\n",
            "[5/10] student class loss: 0.0014\n",
            "[6/10] student class loss: 0.0014\n",
            "[7/10] student class loss: 0.0007\n",
            "[8/10] student class loss: 0.0005\n",
            "[9/10] student class loss: 0.0006\n",
            "[10/10] student class loss: 0.0028\n",
            "Epochs 33  train loss 0.00121 train acc 1.00000 validate loss 0.17574 validate acc 0.93970\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0034 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0007\n",
            "[2/10] student class loss: 0.0013\n",
            "[3/10] student class loss: 0.0012\n",
            "[4/10] student class loss: 0.0041\n",
            "[5/10] student class loss: 0.0016\n",
            "[6/10] student class loss: 0.0007\n",
            "[7/10] student class loss: 0.0012\n",
            "[8/10] student class loss: 0.0009\n",
            "[9/10] student class loss: 0.0007\n",
            "[10/10] student class loss: 0.0010\n",
            "Epochs 34  train loss 0.00132 train acc 1.00000 validate loss 0.16961 validate acc 0.93970\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0035 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0004\n",
            "[2/10] student class loss: 0.0013\n",
            "[3/10] student class loss: 0.0011\n",
            "[4/10] student class loss: 0.0008\n",
            "[5/10] student class loss: 0.0008\n",
            "[6/10] student class loss: 0.0008\n",
            "[7/10] student class loss: 0.0009\n",
            "[8/10] student class loss: 0.0014\n",
            "[9/10] student class loss: 0.0020\n",
            "[10/10] student class loss: 0.0024\n",
            "Epochs 35  train loss 0.00121 train acc 1.00000 validate loss 0.16385 validate acc 0.93970\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0036 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0009\n",
            "[2/10] student class loss: 0.0004\n",
            "[3/10] student class loss: 0.0007\n",
            "[4/10] student class loss: 0.0012\n",
            "[5/10] student class loss: 0.0018\n",
            "[6/10] student class loss: 0.0012\n",
            "[7/10] student class loss: 0.0008\n",
            "[8/10] student class loss: 0.0017\n",
            "[9/10] student class loss: 0.0011\n",
            "[10/10] student class loss: 0.0038\n",
            "Epochs 36  train loss 0.00136 train acc 1.00000 validate loss 0.16071 validate acc 0.95477\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0037 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0040\n",
            "[2/10] student class loss: 0.0007\n",
            "[3/10] student class loss: 0.0007\n",
            "[4/10] student class loss: 0.0009\n",
            "[5/10] student class loss: 0.0005\n",
            "[6/10] student class loss: 0.0018\n",
            "[7/10] student class loss: 0.0008\n",
            "[8/10] student class loss: 0.0011\n",
            "[9/10] student class loss: 0.0004\n",
            "[10/10] student class loss: 0.0044\n",
            "Epochs 37  train loss 0.00152 train acc 1.00000 validate loss 0.15543 validate acc 0.95477\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0038 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0044\n",
            "[2/10] student class loss: 0.0008\n",
            "[3/10] student class loss: 0.0008\n",
            "[4/10] student class loss: 0.0006\n",
            "[5/10] student class loss: 0.0041\n",
            "[6/10] student class loss: 0.0015\n",
            "[7/10] student class loss: 0.0004\n",
            "[8/10] student class loss: 0.0009\n",
            "[9/10] student class loss: 0.0014\n",
            "[10/10] student class loss: 0.0062\n",
            "Epochs 38  train loss 0.00209 train acc 1.00000 validate loss 0.15150 validate acc 0.95477\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0039 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0007\n",
            "[2/10] student class loss: 0.0009\n",
            "[3/10] student class loss: 0.0010\n",
            "[4/10] student class loss: 0.0012\n",
            "[5/10] student class loss: 0.0014\n",
            "[6/10] student class loss: 0.0009\n",
            "[7/10] student class loss: 0.0007\n",
            "[8/10] student class loss: 0.0013\n",
            "[9/10] student class loss: 0.0006\n",
            "[10/10] student class loss: 0.0034\n",
            "Epochs 39  train loss 0.00120 train acc 1.00000 validate loss 0.15085 validate acc 0.95477\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0040 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0019\n",
            "[2/10] student class loss: 0.0008\n",
            "[3/10] student class loss: 0.0005\n",
            "[4/10] student class loss: 0.0007\n",
            "[5/10] student class loss: 0.0017\n",
            "[6/10] student class loss: 0.0007\n",
            "[7/10] student class loss: 0.0006\n",
            "[8/10] student class loss: 0.0006\n",
            "[9/10] student class loss: 0.0006\n",
            "[10/10] student class loss: 0.0039\n",
            "Epochs 40  train loss 0.00120 train acc 1.00000 validate loss 0.14915 validate acc 0.95477\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0041 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0007\n",
            "[2/10] student class loss: 0.0008\n",
            "[3/10] student class loss: 0.0005\n",
            "[4/10] student class loss: 0.0006\n",
            "[5/10] student class loss: 0.0006\n",
            "[6/10] student class loss: 0.0009\n",
            "[7/10] student class loss: 0.0006\n",
            "[8/10] student class loss: 0.0008\n",
            "[9/10] student class loss: 0.0006\n",
            "[10/10] student class loss: 0.0035\n",
            "Epochs 41  train loss 0.00097 train acc 1.00000 validate loss 0.14785 validate acc 0.95477\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0042 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0006\n",
            "[2/10] student class loss: 0.0012\n",
            "[3/10] student class loss: 0.0012\n",
            "[4/10] student class loss: 0.0004\n",
            "[5/10] student class loss: 0.0007\n",
            "[6/10] student class loss: 0.0005\n",
            "[7/10] student class loss: 0.0007\n",
            "[8/10] student class loss: 0.0005\n",
            "[9/10] student class loss: 0.0006\n",
            "[10/10] student class loss: 0.0006\n",
            "Epochs 42  train loss 0.00071 train acc 1.00000 validate loss 0.14792 validate acc 0.95477\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0043 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0007\n",
            "[2/10] student class loss: 0.0007\n",
            "[3/10] student class loss: 0.0007\n",
            "[4/10] student class loss: 0.0007\n",
            "[5/10] student class loss: 0.0007\n",
            "[6/10] student class loss: 0.0013\n",
            "[7/10] student class loss: 0.0005\n",
            "[8/10] student class loss: 0.0019\n",
            "[9/10] student class loss: 0.0008\n",
            "[10/10] student class loss: 0.0026\n",
            "Epochs 43  train loss 0.00105 train acc 1.00000 validate loss 0.14674 validate acc 0.95477\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0044 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0006\n",
            "[2/10] student class loss: 0.0010\n",
            "[3/10] student class loss: 0.0005\n",
            "[4/10] student class loss: 0.0004\n",
            "[5/10] student class loss: 0.0007\n",
            "[6/10] student class loss: 0.0009\n",
            "[7/10] student class loss: 0.0010\n",
            "[8/10] student class loss: 0.0012\n",
            "[9/10] student class loss: 0.0007\n",
            "[10/10] student class loss: 0.0028\n",
            "Epochs 44  train loss 0.00096 train acc 1.00000 validate loss 0.14679 validate acc 0.94975\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0045 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0004\n",
            "[2/10] student class loss: 0.0006\n",
            "[3/10] student class loss: 0.0010\n",
            "[4/10] student class loss: 0.0013\n",
            "[5/10] student class loss: 0.0006\n",
            "[6/10] student class loss: 0.0004\n",
            "[7/10] student class loss: 0.0003\n",
            "[8/10] student class loss: 0.0009\n",
            "[9/10] student class loss: 0.0010\n",
            "[10/10] student class loss: 0.0025\n",
            "Epochs 45  train loss 0.00089 train acc 1.00000 validate loss 0.14710 validate acc 0.94975\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0046 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0008\n",
            "[2/10] student class loss: 0.0005\n",
            "[3/10] student class loss: 0.0003\n",
            "[4/10] student class loss: 0.0006\n",
            "[5/10] student class loss: 0.0007\n",
            "[6/10] student class loss: 0.0005\n",
            "[7/10] student class loss: 0.0005\n",
            "[8/10] student class loss: 0.0005\n",
            "[9/10] student class loss: 0.0007\n",
            "[10/10] student class loss: 0.0107\n",
            "Epochs 46  train loss 0.00158 train acc 1.00000 validate loss 0.14709 validate acc 0.94975\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0047 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0009\n",
            "[2/10] student class loss: 0.0008\n",
            "[3/10] student class loss: 0.0004\n",
            "[4/10] student class loss: 0.0006\n",
            "[5/10] student class loss: 0.0005\n",
            "[6/10] student class loss: 0.0007\n",
            "[7/10] student class loss: 0.0003\n",
            "[8/10] student class loss: 0.0004\n",
            "[9/10] student class loss: 0.0005\n",
            "[10/10] student class loss: 0.0026\n",
            "Epochs 47  train loss 0.00078 train acc 1.00000 validate loss 0.14645 validate acc 0.94975\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0048 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0012\n",
            "[2/10] student class loss: 0.0010\n",
            "[3/10] student class loss: 0.0010\n",
            "[4/10] student class loss: 0.0005\n",
            "[5/10] student class loss: 0.0007\n",
            "[6/10] student class loss: 0.0010\n",
            "[7/10] student class loss: 0.0005\n",
            "[8/10] student class loss: 0.0006\n",
            "[9/10] student class loss: 0.0004\n",
            "[10/10] student class loss: 0.0018\n",
            "Epochs 48  train loss 0.00087 train acc 1.00000 validate loss 0.14550 validate acc 0.94975\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0049 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0011\n",
            "[2/10] student class loss: 0.0026\n",
            "[3/10] student class loss: 0.0008\n",
            "[4/10] student class loss: 0.0004\n",
            "[5/10] student class loss: 0.0009\n",
            "[6/10] student class loss: 0.0010\n",
            "[7/10] student class loss: 0.0004\n",
            "[8/10] student class loss: 0.0004\n",
            "[9/10] student class loss: 0.0018\n",
            "[10/10] student class loss: 0.0011\n",
            "Epochs 49  train loss 0.00104 train acc 1.00000 validate loss 0.14558 validate acc 0.94975\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 0050 / 0050\n",
            "============\n",
            "[1/10] student class loss: 0.0006\n",
            "[2/10] student class loss: 0.0005\n",
            "[3/10] student class loss: 0.0007\n",
            "[4/10] student class loss: 0.0013\n",
            "[5/10] student class loss: 0.0005\n",
            "[6/10] student class loss: 0.0008\n",
            "[7/10] student class loss: 0.0010\n",
            "[8/10] student class loss: 0.0008\n",
            "[9/10] student class loss: 0.0007\n",
            "[10/10] student class loss: 0.0060\n",
            "Epochs 50  train loss 0.00129 train acc 1.00000 validate loss 0.14550 validate acc 0.94975\n",
            "--------------------------------------------------------------------------------\n",
            "Time consumption for student net (device:cuda): 198.7241199016571 sec\n",
            "Training logs saved to LOG_BL_webcam\n"
          ]
        }
      ],
      "source": [
        "model = models.mobilenet_v3_small(pretrained=True).to(device)\n",
        "summary(model, input_size=(channel_size, image_size, image_size))\n",
        "num_epochs = 50\n",
        "lr = 1e-3\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "sds = [\"amazon\", \"dslr\", \"webcam\"]\n",
        "for sd in sds:\n",
        "    model = models.mobilenet_v3_small(pretrained=True).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    tl, _ =get_loader(sd)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=lr,\n",
        "        steps_per_epoch=len(tl),\n",
        "        epochs=num_epochs,\n",
        "        anneal_strategy=\"cos\",\n",
        "    )\n",
        "    plot_graph(train_baseline_model(model, optimizer, scheduler, loss_fn, sd, device, num_epochs=num_epochs, batch_size=64), f\"Baseline {sd}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPaLWb95-yqK"
      },
      "source": [
        "## Pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BweP6v0q_xR1"
      },
      "source": [
        "### Funtion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9Tmi6dp_wqj"
      },
      "outputs": [],
      "source": [
        "def calculate_layer_sparsity(model):\n",
        "    total_zeros = 0\n",
        "    total_elements = 0\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "            # Check if the module has a weight attribute\n",
        "            if hasattr(module, 'weight'):\n",
        "                weight = module.weight\n",
        "                zeros = torch.sum(weight == 0)\n",
        "                total_zeros += zeros\n",
        "                total_elements += weight.nelement()\n",
        "                sparsity = 100.0 * float(zeros) / float(weight.nelement())\n",
        "                print(f\"Sparsity in {name}.weight: {sparsity:.2f}%\")\n",
        "\n",
        "    # Global sparsity calculation\n",
        "    global_sparsity = 100.0 * float(total_zeros) / float(total_elements)\n",
        "    print(f\"Global sparsity: {global_sparsity:.2f}%\")\n",
        "\n",
        "def save_pruned_model(model, path):\n",
        "    model_state = model.state_dict()\n",
        "    pruning_params = {}\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "            if prune.is_pruned(module):  # Check if the module has been pruned\n",
        "                pruning_params[name] = {\n",
        "                    'weight_mask': module.weight_mask,\n",
        "                    'original_weight': module.weight_orig\n",
        "                }\n",
        "\n",
        "    # Save both the model state and pruning parameters\n",
        "    torch.save({'model_state_dict': model_state, 'pruning_params': pruning_params}, path)\n",
        "\n",
        "def prune_evaluate_save(model, model_path, train_loader, test_loader, save_path):\n",
        "    # Load the pre-trained weights\n",
        "    PATH_CP = './cp/'\n",
        "    model.load_state_dict(torch.load(PATH_CP + model_path))\n",
        "    model.to(device)\n",
        "    _, _, _, acc = inference(model, test_loader)\n",
        "\n",
        "    # Display model summary\n",
        "    summary(model, input_size=(channel_size, image_size, image_size))\n",
        "\n",
        "    # Count initial FLOPs and parameters\n",
        "    init_flop = count_model_param_flops(model=model.eval(), input_res=224, multiply_adds=True)  / 1e6\n",
        "\n",
        "    # Apply structured pruning\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            prune.ln_structured(module, name='weight', amount=0.2, n=2, dim=0)\n",
        "\n",
        "    # Remove pruning reparameterization\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            prune.remove(module, 'weight')\n",
        "\n",
        "    # Calculate sparsity\n",
        "    calculate_layer_sparsity(model)\n",
        "\n",
        "    # Count FLOPs and parameters after pruning\n",
        "    post_flop = count_model_param_flops(model=model.eval(), input_res=224, multiply_adds=True)  / 1e6\n",
        "\n",
        "    # Save pruned model\n",
        "    save_pruned_model(model, save_path)\n",
        "    _, _, _, acc = inference(model, test_loader)\n",
        "    return init_flop, post_flop, acc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_pruned_model(model_class, path, device):\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(path)\n",
        "\n",
        "    # Initialize the model\n",
        "    model = model_class().to(device)\n",
        "\n",
        "    # Load the state_dict\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Reapply the pruning mask\n",
        "    pruning_params = checkpoint['pruning_params']\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "            if name in pruning_params:\n",
        "                prune.custom_from_mask(module, name='weight', mask=pruning_params[name]['weight_mask'])\n",
        "                # Replace the pruned weight with the original one (reparameterize)\n",
        "                module.weight_orig = pruning_params[name]['original_weight']\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "# student_fn_loaded = load_pruned_model(DANNMBNv3s(), 'pruned_model.pth', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_48Am9P_0T1"
      },
      "source": [
        "### Pruning Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hF0vfwFr_JXx"
      },
      "outputs": [],
      "source": [
        "lst = [\n",
        "    [DANNMBNv3s(), \"DANN(Teacher)_amazon_to_dslr.pth\", amazon_tl, dslr_tl, True],\n",
        "    [DANNMBNv3s(), \"DANN(Teacher)_amazon_to_webcam.pth\", amazon_tl, webcam_tl, True],\n",
        "    [DANNMBNv3s(), \"DANN(Teacher)_dslr_to_amazon.pth\", dslr_tl, amazon_tl, True],\n",
        "    [DANNMBNv3s(), \"DANN(Teacher)_dslr_to_webcam.pth\", dslr_tl, webcam_tl, True],\n",
        "    [DANNMBNv3s(), \"DANN(Teacher)_webcam_to_amazon.pth\", webcam_tl, amazon_tl, True],\n",
        "    [DANNMBNv3s(), \"DANN(Teacher)_webcam_to_dslr.pth\", webcam_tl, dslr_tl, True],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "K-zja7JQACcr",
        "outputId": "68220396-c676-47cf-c02c-adbbeee35b8e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-49-b57f027c97d5>:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(PATH_CP + model_path))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(Inference || test loss: 4.557110786437988] [accuracy_test: 53.080000000000005 %]\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 16, 112, 112]             432\n",
            "       BatchNorm2d-2         [-1, 16, 112, 112]              32\n",
            "         Hardswish-3         [-1, 16, 112, 112]               0\n",
            "            Conv2d-4           [-1, 16, 56, 56]             144\n",
            "       BatchNorm2d-5           [-1, 16, 56, 56]              32\n",
            "              ReLU-6           [-1, 16, 56, 56]               0\n",
            " AdaptiveAvgPool2d-7             [-1, 16, 1, 1]               0\n",
            "            Conv2d-8              [-1, 8, 1, 1]             136\n",
            "              ReLU-9              [-1, 8, 1, 1]               0\n",
            "           Conv2d-10             [-1, 16, 1, 1]             144\n",
            "      Hardsigmoid-11             [-1, 16, 1, 1]               0\n",
            "SqueezeExcitation-12           [-1, 16, 56, 56]               0\n",
            "           Conv2d-13           [-1, 16, 56, 56]             256\n",
            "      BatchNorm2d-14           [-1, 16, 56, 56]              32\n",
            " InvertedResidual-15           [-1, 16, 56, 56]               0\n",
            "           Conv2d-16           [-1, 72, 56, 56]           1,152\n",
            "      BatchNorm2d-17           [-1, 72, 56, 56]             144\n",
            "             ReLU-18           [-1, 72, 56, 56]               0\n",
            "           Conv2d-19           [-1, 72, 28, 28]             648\n",
            "      BatchNorm2d-20           [-1, 72, 28, 28]             144\n",
            "             ReLU-21           [-1, 72, 28, 28]               0\n",
            "           Conv2d-22           [-1, 24, 28, 28]           1,728\n",
            "      BatchNorm2d-23           [-1, 24, 28, 28]              48\n",
            " InvertedResidual-24           [-1, 24, 28, 28]               0\n",
            "           Conv2d-25           [-1, 88, 28, 28]           2,112\n",
            "      BatchNorm2d-26           [-1, 88, 28, 28]             176\n",
            "             ReLU-27           [-1, 88, 28, 28]               0\n",
            "           Conv2d-28           [-1, 88, 28, 28]             792\n",
            "      BatchNorm2d-29           [-1, 88, 28, 28]             176\n",
            "             ReLU-30           [-1, 88, 28, 28]               0\n",
            "           Conv2d-31           [-1, 24, 28, 28]           2,112\n",
            "      BatchNorm2d-32           [-1, 24, 28, 28]              48\n",
            " InvertedResidual-33           [-1, 24, 28, 28]               0\n",
            "           Conv2d-34           [-1, 96, 28, 28]           2,304\n",
            "      BatchNorm2d-35           [-1, 96, 28, 28]             192\n",
            "        Hardswish-36           [-1, 96, 28, 28]               0\n",
            "           Conv2d-37           [-1, 96, 14, 14]           2,400\n",
            "      BatchNorm2d-38           [-1, 96, 14, 14]             192\n",
            "        Hardswish-39           [-1, 96, 14, 14]               0\n",
            "AdaptiveAvgPool2d-40             [-1, 96, 1, 1]               0\n",
            "           Conv2d-41             [-1, 24, 1, 1]           2,328\n",
            "             ReLU-42             [-1, 24, 1, 1]               0\n",
            "           Conv2d-43             [-1, 96, 1, 1]           2,400\n",
            "      Hardsigmoid-44             [-1, 96, 1, 1]               0\n",
            "SqueezeExcitation-45           [-1, 96, 14, 14]               0\n",
            "           Conv2d-46           [-1, 40, 14, 14]           3,840\n",
            "      BatchNorm2d-47           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-48           [-1, 40, 14, 14]               0\n",
            "           Conv2d-49          [-1, 240, 14, 14]           9,600\n",
            "      BatchNorm2d-50          [-1, 240, 14, 14]             480\n",
            "        Hardswish-51          [-1, 240, 14, 14]               0\n",
            "           Conv2d-52          [-1, 240, 14, 14]           6,000\n",
            "      BatchNorm2d-53          [-1, 240, 14, 14]             480\n",
            "        Hardswish-54          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-55            [-1, 240, 1, 1]               0\n",
            "           Conv2d-56             [-1, 64, 1, 1]          15,424\n",
            "             ReLU-57             [-1, 64, 1, 1]               0\n",
            "           Conv2d-58            [-1, 240, 1, 1]          15,600\n",
            "      Hardsigmoid-59            [-1, 240, 1, 1]               0\n",
            "SqueezeExcitation-60          [-1, 240, 14, 14]               0\n",
            "           Conv2d-61           [-1, 40, 14, 14]           9,600\n",
            "      BatchNorm2d-62           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-63           [-1, 40, 14, 14]               0\n",
            "           Conv2d-64          [-1, 240, 14, 14]           9,600\n",
            "      BatchNorm2d-65          [-1, 240, 14, 14]             480\n",
            "        Hardswish-66          [-1, 240, 14, 14]               0\n",
            "           Conv2d-67          [-1, 240, 14, 14]           6,000\n",
            "      BatchNorm2d-68          [-1, 240, 14, 14]             480\n",
            "        Hardswish-69          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-70            [-1, 240, 1, 1]               0\n",
            "           Conv2d-71             [-1, 64, 1, 1]          15,424\n",
            "             ReLU-72             [-1, 64, 1, 1]               0\n",
            "           Conv2d-73            [-1, 240, 1, 1]          15,600\n",
            "      Hardsigmoid-74            [-1, 240, 1, 1]               0\n",
            "SqueezeExcitation-75          [-1, 240, 14, 14]               0\n",
            "           Conv2d-76           [-1, 40, 14, 14]           9,600\n",
            "      BatchNorm2d-77           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-78           [-1, 40, 14, 14]               0\n",
            "           Conv2d-79          [-1, 120, 14, 14]           4,800\n",
            "      BatchNorm2d-80          [-1, 120, 14, 14]             240\n",
            "        Hardswish-81          [-1, 120, 14, 14]               0\n",
            "           Conv2d-82          [-1, 120, 14, 14]           3,000\n",
            "      BatchNorm2d-83          [-1, 120, 14, 14]             240\n",
            "        Hardswish-84          [-1, 120, 14, 14]               0\n",
            "AdaptiveAvgPool2d-85            [-1, 120, 1, 1]               0\n",
            "           Conv2d-86             [-1, 32, 1, 1]           3,872\n",
            "             ReLU-87             [-1, 32, 1, 1]               0\n",
            "           Conv2d-88            [-1, 120, 1, 1]           3,960\n",
            "      Hardsigmoid-89            [-1, 120, 1, 1]               0\n",
            "SqueezeExcitation-90          [-1, 120, 14, 14]               0\n",
            "           Conv2d-91           [-1, 48, 14, 14]           5,760\n",
            "      BatchNorm2d-92           [-1, 48, 14, 14]              96\n",
            " InvertedResidual-93           [-1, 48, 14, 14]               0\n",
            "           Conv2d-94          [-1, 144, 14, 14]           6,912\n",
            "      BatchNorm2d-95          [-1, 144, 14, 14]             288\n",
            "        Hardswish-96          [-1, 144, 14, 14]               0\n",
            "           Conv2d-97          [-1, 144, 14, 14]           3,600\n",
            "      BatchNorm2d-98          [-1, 144, 14, 14]             288\n",
            "        Hardswish-99          [-1, 144, 14, 14]               0\n",
            "AdaptiveAvgPool2d-100            [-1, 144, 1, 1]               0\n",
            "          Conv2d-101             [-1, 40, 1, 1]           5,800\n",
            "            ReLU-102             [-1, 40, 1, 1]               0\n",
            "          Conv2d-103            [-1, 144, 1, 1]           5,904\n",
            "     Hardsigmoid-104            [-1, 144, 1, 1]               0\n",
            "SqueezeExcitation-105          [-1, 144, 14, 14]               0\n",
            "          Conv2d-106           [-1, 48, 14, 14]           6,912\n",
            "     BatchNorm2d-107           [-1, 48, 14, 14]              96\n",
            "InvertedResidual-108           [-1, 48, 14, 14]               0\n",
            "          Conv2d-109          [-1, 288, 14, 14]          13,824\n",
            "     BatchNorm2d-110          [-1, 288, 14, 14]             576\n",
            "       Hardswish-111          [-1, 288, 14, 14]               0\n",
            "          Conv2d-112            [-1, 288, 7, 7]           7,200\n",
            "     BatchNorm2d-113            [-1, 288, 7, 7]             576\n",
            "       Hardswish-114            [-1, 288, 7, 7]               0\n",
            "AdaptiveAvgPool2d-115            [-1, 288, 1, 1]               0\n",
            "          Conv2d-116             [-1, 72, 1, 1]          20,808\n",
            "            ReLU-117             [-1, 72, 1, 1]               0\n",
            "          Conv2d-118            [-1, 288, 1, 1]          21,024\n",
            "     Hardsigmoid-119            [-1, 288, 1, 1]               0\n",
            "SqueezeExcitation-120            [-1, 288, 7, 7]               0\n",
            "          Conv2d-121             [-1, 96, 7, 7]          27,648\n",
            "     BatchNorm2d-122             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-123             [-1, 96, 7, 7]               0\n",
            "          Conv2d-124            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-125            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-126            [-1, 576, 7, 7]               0\n",
            "          Conv2d-127            [-1, 576, 7, 7]          14,400\n",
            "     BatchNorm2d-128            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-129            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-130            [-1, 576, 1, 1]               0\n",
            "          Conv2d-131            [-1, 144, 1, 1]          83,088\n",
            "            ReLU-132            [-1, 144, 1, 1]               0\n",
            "          Conv2d-133            [-1, 576, 1, 1]          83,520\n",
            "     Hardsigmoid-134            [-1, 576, 1, 1]               0\n",
            "SqueezeExcitation-135            [-1, 576, 7, 7]               0\n",
            "          Conv2d-136             [-1, 96, 7, 7]          55,296\n",
            "     BatchNorm2d-137             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-138             [-1, 96, 7, 7]               0\n",
            "          Conv2d-139            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-140            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-141            [-1, 576, 7, 7]               0\n",
            "          Conv2d-142            [-1, 576, 7, 7]          14,400\n",
            "     BatchNorm2d-143            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-144            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-145            [-1, 576, 1, 1]               0\n",
            "          Conv2d-146            [-1, 144, 1, 1]          83,088\n",
            "            ReLU-147            [-1, 144, 1, 1]               0\n",
            "          Conv2d-148            [-1, 576, 1, 1]          83,520\n",
            "     Hardsigmoid-149            [-1, 576, 1, 1]               0\n",
            "SqueezeExcitation-150            [-1, 576, 7, 7]               0\n",
            "          Conv2d-151             [-1, 96, 7, 7]          55,296\n",
            "     BatchNorm2d-152             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-153             [-1, 96, 7, 7]               0\n",
            "          Conv2d-154            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-155            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-156            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-157            [-1, 576, 1, 1]               0\n",
            "        Identity-158                  [-1, 576]               0\n",
            "     MobileNetV3-159                  [-1, 576]               0\n",
            "          Linear-160                 [-1, 1024]         590,848\n",
            "            ReLU-161                 [-1, 1024]               0\n",
            "          Linear-162                    [-1, 2]           2,050\n",
            "      LogSoftmax-163                    [-1, 2]               0\n",
            "          Linear-164                 [-1, 1024]         590,848\n",
            "       Hardswish-165                 [-1, 1024]               0\n",
            "         Dropout-166                 [-1, 1024]               0\n",
            "          Linear-167                   [-1, 31]          31,775\n",
            "      LogSoftmax-168                   [-1, 31]               0\n",
            "================================================================\n",
            "Total params: 2,142,529\n",
            "Trainable params: 2,142,529\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 34.63\n",
            "Params size (MB): 8.17\n",
            "Estimated Total Size (MB): 43.37\n",
            "----------------------------------------------------------------\n",
            "Number of FLOPs: 0.115523 GFLOPs (115.52 MFLOPs)\n",
            "Sparsity in feature_extractor.features.0.0.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.1.block.0.0.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.1.block.1.fc1.weight: 25.00%\n",
            "Sparsity in feature_extractor.features.1.block.1.fc2.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.1.block.2.0.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.2.block.0.0.weight: 19.44%\n",
            "Sparsity in feature_extractor.features.2.block.1.0.weight: 19.44%\n",
            "Sparsity in feature_extractor.features.2.block.2.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.3.block.0.0.weight: 20.45%\n",
            "Sparsity in feature_extractor.features.3.block.1.0.weight: 20.45%\n",
            "Sparsity in feature_extractor.features.3.block.2.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.4.block.0.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.4.block.1.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.4.block.2.fc1.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.4.block.2.fc2.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.4.block.3.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.0.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.1.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.2.fc1.weight: 20.31%\n",
            "Sparsity in feature_extractor.features.5.block.2.fc2.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.3.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.0.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.1.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.2.fc1.weight: 20.31%\n",
            "Sparsity in feature_extractor.features.6.block.2.fc2.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.3.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.0.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.1.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.2.fc1.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.7.block.2.fc2.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.3.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.8.block.0.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.8.block.1.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.8.block.2.fc1.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.8.block.2.fc2.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.8.block.3.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.9.block.0.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.9.block.1.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.9.block.2.fc1.weight: 19.44%\n",
            "Sparsity in feature_extractor.features.9.block.2.fc2.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.9.block.3.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.10.block.0.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.10.block.1.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.10.block.2.fc1.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.10.block.2.fc2.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.10.block.3.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.11.block.0.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.11.block.1.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.11.block.2.fc1.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.11.block.2.fc2.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.11.block.3.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.12.0.weight: 19.97%\n",
            "Sparsity in class_classifier.0.weight: 0.00%\n",
            "Sparsity in class_classifier.3.weight: 0.00%\n",
            "Sparsity in domain_classifier.0.weight: 0.00%\n",
            "Sparsity in domain_classifier.2.weight: 0.00%\n",
            "Global sparsity: 8.58%\n",
            "Number of FLOPs: 0.093694 GFLOPs (93.69 MFLOPs)\n",
            "[(Inference || test loss: 9.901710510253906] [accuracy_test: 1.0999999999999999 %]\n",
            "[(Inference || test loss: 4.282110691070557] [accuracy_test: 55.03 %]\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 16, 112, 112]             432\n",
            "       BatchNorm2d-2         [-1, 16, 112, 112]              32\n",
            "         Hardswish-3         [-1, 16, 112, 112]               0\n",
            "            Conv2d-4           [-1, 16, 56, 56]             144\n",
            "       BatchNorm2d-5           [-1, 16, 56, 56]              32\n",
            "              ReLU-6           [-1, 16, 56, 56]               0\n",
            " AdaptiveAvgPool2d-7             [-1, 16, 1, 1]               0\n",
            "            Conv2d-8              [-1, 8, 1, 1]             136\n",
            "              ReLU-9              [-1, 8, 1, 1]               0\n",
            "           Conv2d-10             [-1, 16, 1, 1]             144\n",
            "      Hardsigmoid-11             [-1, 16, 1, 1]               0\n",
            "SqueezeExcitation-12           [-1, 16, 56, 56]               0\n",
            "           Conv2d-13           [-1, 16, 56, 56]             256\n",
            "      BatchNorm2d-14           [-1, 16, 56, 56]              32\n",
            " InvertedResidual-15           [-1, 16, 56, 56]               0\n",
            "           Conv2d-16           [-1, 72, 56, 56]           1,152\n",
            "      BatchNorm2d-17           [-1, 72, 56, 56]             144\n",
            "             ReLU-18           [-1, 72, 56, 56]               0\n",
            "           Conv2d-19           [-1, 72, 28, 28]             648\n",
            "      BatchNorm2d-20           [-1, 72, 28, 28]             144\n",
            "             ReLU-21           [-1, 72, 28, 28]               0\n",
            "           Conv2d-22           [-1, 24, 28, 28]           1,728\n",
            "      BatchNorm2d-23           [-1, 24, 28, 28]              48\n",
            " InvertedResidual-24           [-1, 24, 28, 28]               0\n",
            "           Conv2d-25           [-1, 88, 28, 28]           2,112\n",
            "      BatchNorm2d-26           [-1, 88, 28, 28]             176\n",
            "             ReLU-27           [-1, 88, 28, 28]               0\n",
            "           Conv2d-28           [-1, 88, 28, 28]             792\n",
            "      BatchNorm2d-29           [-1, 88, 28, 28]             176\n",
            "             ReLU-30           [-1, 88, 28, 28]               0\n",
            "           Conv2d-31           [-1, 24, 28, 28]           2,112\n",
            "      BatchNorm2d-32           [-1, 24, 28, 28]              48\n",
            " InvertedResidual-33           [-1, 24, 28, 28]               0\n",
            "           Conv2d-34           [-1, 96, 28, 28]           2,304\n",
            "      BatchNorm2d-35           [-1, 96, 28, 28]             192\n",
            "        Hardswish-36           [-1, 96, 28, 28]               0\n",
            "           Conv2d-37           [-1, 96, 14, 14]           2,400\n",
            "      BatchNorm2d-38           [-1, 96, 14, 14]             192\n",
            "        Hardswish-39           [-1, 96, 14, 14]               0\n",
            "AdaptiveAvgPool2d-40             [-1, 96, 1, 1]               0\n",
            "           Conv2d-41             [-1, 24, 1, 1]           2,328\n",
            "             ReLU-42             [-1, 24, 1, 1]               0\n",
            "           Conv2d-43             [-1, 96, 1, 1]           2,400\n",
            "      Hardsigmoid-44             [-1, 96, 1, 1]               0\n",
            "SqueezeExcitation-45           [-1, 96, 14, 14]               0\n",
            "           Conv2d-46           [-1, 40, 14, 14]           3,840\n",
            "      BatchNorm2d-47           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-48           [-1, 40, 14, 14]               0\n",
            "           Conv2d-49          [-1, 240, 14, 14]           9,600\n",
            "      BatchNorm2d-50          [-1, 240, 14, 14]             480\n",
            "        Hardswish-51          [-1, 240, 14, 14]               0\n",
            "           Conv2d-52          [-1, 240, 14, 14]           6,000\n",
            "      BatchNorm2d-53          [-1, 240, 14, 14]             480\n",
            "        Hardswish-54          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-55            [-1, 240, 1, 1]               0\n",
            "           Conv2d-56             [-1, 64, 1, 1]          15,424\n",
            "             ReLU-57             [-1, 64, 1, 1]               0\n",
            "           Conv2d-58            [-1, 240, 1, 1]          15,600\n",
            "      Hardsigmoid-59            [-1, 240, 1, 1]               0\n",
            "SqueezeExcitation-60          [-1, 240, 14, 14]               0\n",
            "           Conv2d-61           [-1, 40, 14, 14]           9,600\n",
            "      BatchNorm2d-62           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-63           [-1, 40, 14, 14]               0\n",
            "           Conv2d-64          [-1, 240, 14, 14]           9,600\n",
            "      BatchNorm2d-65          [-1, 240, 14, 14]             480\n",
            "        Hardswish-66          [-1, 240, 14, 14]               0\n",
            "           Conv2d-67          [-1, 240, 14, 14]           6,000\n",
            "      BatchNorm2d-68          [-1, 240, 14, 14]             480\n",
            "        Hardswish-69          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-70            [-1, 240, 1, 1]               0\n",
            "           Conv2d-71             [-1, 64, 1, 1]          15,424\n",
            "             ReLU-72             [-1, 64, 1, 1]               0\n",
            "           Conv2d-73            [-1, 240, 1, 1]          15,600\n",
            "      Hardsigmoid-74            [-1, 240, 1, 1]               0\n",
            "SqueezeExcitation-75          [-1, 240, 14, 14]               0\n",
            "           Conv2d-76           [-1, 40, 14, 14]           9,600\n",
            "      BatchNorm2d-77           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-78           [-1, 40, 14, 14]               0\n",
            "           Conv2d-79          [-1, 120, 14, 14]           4,800\n",
            "      BatchNorm2d-80          [-1, 120, 14, 14]             240\n",
            "        Hardswish-81          [-1, 120, 14, 14]               0\n",
            "           Conv2d-82          [-1, 120, 14, 14]           3,000\n",
            "      BatchNorm2d-83          [-1, 120, 14, 14]             240\n",
            "        Hardswish-84          [-1, 120, 14, 14]               0\n",
            "AdaptiveAvgPool2d-85            [-1, 120, 1, 1]               0\n",
            "           Conv2d-86             [-1, 32, 1, 1]           3,872\n",
            "             ReLU-87             [-1, 32, 1, 1]               0\n",
            "           Conv2d-88            [-1, 120, 1, 1]           3,960\n",
            "      Hardsigmoid-89            [-1, 120, 1, 1]               0\n",
            "SqueezeExcitation-90          [-1, 120, 14, 14]               0\n",
            "           Conv2d-91           [-1, 48, 14, 14]           5,760\n",
            "      BatchNorm2d-92           [-1, 48, 14, 14]              96\n",
            " InvertedResidual-93           [-1, 48, 14, 14]               0\n",
            "           Conv2d-94          [-1, 144, 14, 14]           6,912\n",
            "      BatchNorm2d-95          [-1, 144, 14, 14]             288\n",
            "        Hardswish-96          [-1, 144, 14, 14]               0\n",
            "           Conv2d-97          [-1, 144, 14, 14]           3,600\n",
            "      BatchNorm2d-98          [-1, 144, 14, 14]             288\n",
            "        Hardswish-99          [-1, 144, 14, 14]               0\n",
            "AdaptiveAvgPool2d-100            [-1, 144, 1, 1]               0\n",
            "          Conv2d-101             [-1, 40, 1, 1]           5,800\n",
            "            ReLU-102             [-1, 40, 1, 1]               0\n",
            "          Conv2d-103            [-1, 144, 1, 1]           5,904\n",
            "     Hardsigmoid-104            [-1, 144, 1, 1]               0\n",
            "SqueezeExcitation-105          [-1, 144, 14, 14]               0\n",
            "          Conv2d-106           [-1, 48, 14, 14]           6,912\n",
            "     BatchNorm2d-107           [-1, 48, 14, 14]              96\n",
            "InvertedResidual-108           [-1, 48, 14, 14]               0\n",
            "          Conv2d-109          [-1, 288, 14, 14]          13,824\n",
            "     BatchNorm2d-110          [-1, 288, 14, 14]             576\n",
            "       Hardswish-111          [-1, 288, 14, 14]               0\n",
            "          Conv2d-112            [-1, 288, 7, 7]           7,200\n",
            "     BatchNorm2d-113            [-1, 288, 7, 7]             576\n",
            "       Hardswish-114            [-1, 288, 7, 7]               0\n",
            "AdaptiveAvgPool2d-115            [-1, 288, 1, 1]               0\n",
            "          Conv2d-116             [-1, 72, 1, 1]          20,808\n",
            "            ReLU-117             [-1, 72, 1, 1]               0\n",
            "          Conv2d-118            [-1, 288, 1, 1]          21,024\n",
            "     Hardsigmoid-119            [-1, 288, 1, 1]               0\n",
            "SqueezeExcitation-120            [-1, 288, 7, 7]               0\n",
            "          Conv2d-121             [-1, 96, 7, 7]          27,648\n",
            "     BatchNorm2d-122             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-123             [-1, 96, 7, 7]               0\n",
            "          Conv2d-124            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-125            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-126            [-1, 576, 7, 7]               0\n",
            "          Conv2d-127            [-1, 576, 7, 7]          14,400\n",
            "     BatchNorm2d-128            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-129            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-130            [-1, 576, 1, 1]               0\n",
            "          Conv2d-131            [-1, 144, 1, 1]          83,088\n",
            "            ReLU-132            [-1, 144, 1, 1]               0\n",
            "          Conv2d-133            [-1, 576, 1, 1]          83,520\n",
            "     Hardsigmoid-134            [-1, 576, 1, 1]               0\n",
            "SqueezeExcitation-135            [-1, 576, 7, 7]               0\n",
            "          Conv2d-136             [-1, 96, 7, 7]          55,296\n",
            "     BatchNorm2d-137             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-138             [-1, 96, 7, 7]               0\n",
            "          Conv2d-139            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-140            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-141            [-1, 576, 7, 7]               0\n",
            "          Conv2d-142            [-1, 576, 7, 7]          14,400\n",
            "     BatchNorm2d-143            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-144            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-145            [-1, 576, 1, 1]               0\n",
            "          Conv2d-146            [-1, 144, 1, 1]          83,088\n",
            "            ReLU-147            [-1, 144, 1, 1]               0\n",
            "          Conv2d-148            [-1, 576, 1, 1]          83,520\n",
            "     Hardsigmoid-149            [-1, 576, 1, 1]               0\n",
            "SqueezeExcitation-150            [-1, 576, 7, 7]               0\n",
            "          Conv2d-151             [-1, 96, 7, 7]          55,296\n",
            "     BatchNorm2d-152             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-153             [-1, 96, 7, 7]               0\n",
            "          Conv2d-154            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-155            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-156            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-157            [-1, 576, 1, 1]               0\n",
            "        Identity-158                  [-1, 576]               0\n",
            "     MobileNetV3-159                  [-1, 576]               0\n",
            "          Linear-160                 [-1, 1024]         590,848\n",
            "            ReLU-161                 [-1, 1024]               0\n",
            "          Linear-162                    [-1, 2]           2,050\n",
            "      LogSoftmax-163                    [-1, 2]               0\n",
            "          Linear-164                 [-1, 1024]         590,848\n",
            "       Hardswish-165                 [-1, 1024]               0\n",
            "         Dropout-166                 [-1, 1024]               0\n",
            "          Linear-167                   [-1, 31]          31,775\n",
            "      LogSoftmax-168                   [-1, 31]               0\n",
            "================================================================\n",
            "Total params: 2,142,529\n",
            "Trainable params: 2,142,529\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 34.63\n",
            "Params size (MB): 8.17\n",
            "Estimated Total Size (MB): 43.37\n",
            "----------------------------------------------------------------\n",
            "Number of FLOPs: 0.115523 GFLOPs (115.52 MFLOPs)\n",
            "Sparsity in feature_extractor.features.0.0.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.1.block.0.0.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.1.block.1.fc1.weight: 25.00%\n",
            "Sparsity in feature_extractor.features.1.block.1.fc2.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.1.block.2.0.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.2.block.0.0.weight: 19.44%\n",
            "Sparsity in feature_extractor.features.2.block.1.0.weight: 19.44%\n",
            "Sparsity in feature_extractor.features.2.block.2.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.3.block.0.0.weight: 20.45%\n",
            "Sparsity in feature_extractor.features.3.block.1.0.weight: 20.45%\n",
            "Sparsity in feature_extractor.features.3.block.2.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.4.block.0.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.4.block.1.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.4.block.2.fc1.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.4.block.2.fc2.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.4.block.3.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.0.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.1.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.2.fc1.weight: 20.31%\n",
            "Sparsity in feature_extractor.features.5.block.2.fc2.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.3.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.0.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.1.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.2.fc1.weight: 20.31%\n",
            "Sparsity in feature_extractor.features.6.block.2.fc2.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.3.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.0.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.1.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.2.fc1.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.7.block.2.fc2.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.3.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.8.block.0.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.8.block.1.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.8.block.2.fc1.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.8.block.2.fc2.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.8.block.3.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.9.block.0.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.9.block.1.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.9.block.2.fc1.weight: 19.44%\n",
            "Sparsity in feature_extractor.features.9.block.2.fc2.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.9.block.3.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.10.block.0.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.10.block.1.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.10.block.2.fc1.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.10.block.2.fc2.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.10.block.3.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.11.block.0.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.11.block.1.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.11.block.2.fc1.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.11.block.2.fc2.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.11.block.3.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.12.0.weight: 19.97%\n",
            "Sparsity in class_classifier.0.weight: 0.00%\n",
            "Sparsity in class_classifier.3.weight: 0.00%\n",
            "Sparsity in domain_classifier.0.weight: 0.00%\n",
            "Sparsity in domain_classifier.2.weight: 0.00%\n",
            "Global sparsity: 8.58%\n",
            "Number of FLOPs: 0.093694 GFLOPs (93.69 MFLOPs)\n",
            "[(Inference || test loss: 5.179599285125732] [accuracy_test: 1.41 %]\n",
            "[(Inference || test loss: 2.7180612087249756] [accuracy_test: 32.29 %]\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 16, 112, 112]             432\n",
            "       BatchNorm2d-2         [-1, 16, 112, 112]              32\n",
            "         Hardswish-3         [-1, 16, 112, 112]               0\n",
            "            Conv2d-4           [-1, 16, 56, 56]             144\n",
            "       BatchNorm2d-5           [-1, 16, 56, 56]              32\n",
            "              ReLU-6           [-1, 16, 56, 56]               0\n",
            " AdaptiveAvgPool2d-7             [-1, 16, 1, 1]               0\n",
            "            Conv2d-8              [-1, 8, 1, 1]             136\n",
            "              ReLU-9              [-1, 8, 1, 1]               0\n",
            "           Conv2d-10             [-1, 16, 1, 1]             144\n",
            "      Hardsigmoid-11             [-1, 16, 1, 1]               0\n",
            "SqueezeExcitation-12           [-1, 16, 56, 56]               0\n",
            "           Conv2d-13           [-1, 16, 56, 56]             256\n",
            "      BatchNorm2d-14           [-1, 16, 56, 56]              32\n",
            " InvertedResidual-15           [-1, 16, 56, 56]               0\n",
            "           Conv2d-16           [-1, 72, 56, 56]           1,152\n",
            "      BatchNorm2d-17           [-1, 72, 56, 56]             144\n",
            "             ReLU-18           [-1, 72, 56, 56]               0\n",
            "           Conv2d-19           [-1, 72, 28, 28]             648\n",
            "      BatchNorm2d-20           [-1, 72, 28, 28]             144\n",
            "             ReLU-21           [-1, 72, 28, 28]               0\n",
            "           Conv2d-22           [-1, 24, 28, 28]           1,728\n",
            "      BatchNorm2d-23           [-1, 24, 28, 28]              48\n",
            " InvertedResidual-24           [-1, 24, 28, 28]               0\n",
            "           Conv2d-25           [-1, 88, 28, 28]           2,112\n",
            "      BatchNorm2d-26           [-1, 88, 28, 28]             176\n",
            "             ReLU-27           [-1, 88, 28, 28]               0\n",
            "           Conv2d-28           [-1, 88, 28, 28]             792\n",
            "      BatchNorm2d-29           [-1, 88, 28, 28]             176\n",
            "             ReLU-30           [-1, 88, 28, 28]               0\n",
            "           Conv2d-31           [-1, 24, 28, 28]           2,112\n",
            "      BatchNorm2d-32           [-1, 24, 28, 28]              48\n",
            " InvertedResidual-33           [-1, 24, 28, 28]               0\n",
            "           Conv2d-34           [-1, 96, 28, 28]           2,304\n",
            "      BatchNorm2d-35           [-1, 96, 28, 28]             192\n",
            "        Hardswish-36           [-1, 96, 28, 28]               0\n",
            "           Conv2d-37           [-1, 96, 14, 14]           2,400\n",
            "      BatchNorm2d-38           [-1, 96, 14, 14]             192\n",
            "        Hardswish-39           [-1, 96, 14, 14]               0\n",
            "AdaptiveAvgPool2d-40             [-1, 96, 1, 1]               0\n",
            "           Conv2d-41             [-1, 24, 1, 1]           2,328\n",
            "             ReLU-42             [-1, 24, 1, 1]               0\n",
            "           Conv2d-43             [-1, 96, 1, 1]           2,400\n",
            "      Hardsigmoid-44             [-1, 96, 1, 1]               0\n",
            "SqueezeExcitation-45           [-1, 96, 14, 14]               0\n",
            "           Conv2d-46           [-1, 40, 14, 14]           3,840\n",
            "      BatchNorm2d-47           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-48           [-1, 40, 14, 14]               0\n",
            "           Conv2d-49          [-1, 240, 14, 14]           9,600\n",
            "      BatchNorm2d-50          [-1, 240, 14, 14]             480\n",
            "        Hardswish-51          [-1, 240, 14, 14]               0\n",
            "           Conv2d-52          [-1, 240, 14, 14]           6,000\n",
            "      BatchNorm2d-53          [-1, 240, 14, 14]             480\n",
            "        Hardswish-54          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-55            [-1, 240, 1, 1]               0\n",
            "           Conv2d-56             [-1, 64, 1, 1]          15,424\n",
            "             ReLU-57             [-1, 64, 1, 1]               0\n",
            "           Conv2d-58            [-1, 240, 1, 1]          15,600\n",
            "      Hardsigmoid-59            [-1, 240, 1, 1]               0\n",
            "SqueezeExcitation-60          [-1, 240, 14, 14]               0\n",
            "           Conv2d-61           [-1, 40, 14, 14]           9,600\n",
            "      BatchNorm2d-62           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-63           [-1, 40, 14, 14]               0\n",
            "           Conv2d-64          [-1, 240, 14, 14]           9,600\n",
            "      BatchNorm2d-65          [-1, 240, 14, 14]             480\n",
            "        Hardswish-66          [-1, 240, 14, 14]               0\n",
            "           Conv2d-67          [-1, 240, 14, 14]           6,000\n",
            "      BatchNorm2d-68          [-1, 240, 14, 14]             480\n",
            "        Hardswish-69          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-70            [-1, 240, 1, 1]               0\n",
            "           Conv2d-71             [-1, 64, 1, 1]          15,424\n",
            "             ReLU-72             [-1, 64, 1, 1]               0\n",
            "           Conv2d-73            [-1, 240, 1, 1]          15,600\n",
            "      Hardsigmoid-74            [-1, 240, 1, 1]               0\n",
            "SqueezeExcitation-75          [-1, 240, 14, 14]               0\n",
            "           Conv2d-76           [-1, 40, 14, 14]           9,600\n",
            "      BatchNorm2d-77           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-78           [-1, 40, 14, 14]               0\n",
            "           Conv2d-79          [-1, 120, 14, 14]           4,800\n",
            "      BatchNorm2d-80          [-1, 120, 14, 14]             240\n",
            "        Hardswish-81          [-1, 120, 14, 14]               0\n",
            "           Conv2d-82          [-1, 120, 14, 14]           3,000\n",
            "      BatchNorm2d-83          [-1, 120, 14, 14]             240\n",
            "        Hardswish-84          [-1, 120, 14, 14]               0\n",
            "AdaptiveAvgPool2d-85            [-1, 120, 1, 1]               0\n",
            "           Conv2d-86             [-1, 32, 1, 1]           3,872\n",
            "             ReLU-87             [-1, 32, 1, 1]               0\n",
            "           Conv2d-88            [-1, 120, 1, 1]           3,960\n",
            "      Hardsigmoid-89            [-1, 120, 1, 1]               0\n",
            "SqueezeExcitation-90          [-1, 120, 14, 14]               0\n",
            "           Conv2d-91           [-1, 48, 14, 14]           5,760\n",
            "      BatchNorm2d-92           [-1, 48, 14, 14]              96\n",
            " InvertedResidual-93           [-1, 48, 14, 14]               0\n",
            "           Conv2d-94          [-1, 144, 14, 14]           6,912\n",
            "      BatchNorm2d-95          [-1, 144, 14, 14]             288\n",
            "        Hardswish-96          [-1, 144, 14, 14]               0\n",
            "           Conv2d-97          [-1, 144, 14, 14]           3,600\n",
            "      BatchNorm2d-98          [-1, 144, 14, 14]             288\n",
            "        Hardswish-99          [-1, 144, 14, 14]               0\n",
            "AdaptiveAvgPool2d-100            [-1, 144, 1, 1]               0\n",
            "          Conv2d-101             [-1, 40, 1, 1]           5,800\n",
            "            ReLU-102             [-1, 40, 1, 1]               0\n",
            "          Conv2d-103            [-1, 144, 1, 1]           5,904\n",
            "     Hardsigmoid-104            [-1, 144, 1, 1]               0\n",
            "SqueezeExcitation-105          [-1, 144, 14, 14]               0\n",
            "          Conv2d-106           [-1, 48, 14, 14]           6,912\n",
            "     BatchNorm2d-107           [-1, 48, 14, 14]              96\n",
            "InvertedResidual-108           [-1, 48, 14, 14]               0\n",
            "          Conv2d-109          [-1, 288, 14, 14]          13,824\n",
            "     BatchNorm2d-110          [-1, 288, 14, 14]             576\n",
            "       Hardswish-111          [-1, 288, 14, 14]               0\n",
            "          Conv2d-112            [-1, 288, 7, 7]           7,200\n",
            "     BatchNorm2d-113            [-1, 288, 7, 7]             576\n",
            "       Hardswish-114            [-1, 288, 7, 7]               0\n",
            "AdaptiveAvgPool2d-115            [-1, 288, 1, 1]               0\n",
            "          Conv2d-116             [-1, 72, 1, 1]          20,808\n",
            "            ReLU-117             [-1, 72, 1, 1]               0\n",
            "          Conv2d-118            [-1, 288, 1, 1]          21,024\n",
            "     Hardsigmoid-119            [-1, 288, 1, 1]               0\n",
            "SqueezeExcitation-120            [-1, 288, 7, 7]               0\n",
            "          Conv2d-121             [-1, 96, 7, 7]          27,648\n",
            "     BatchNorm2d-122             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-123             [-1, 96, 7, 7]               0\n",
            "          Conv2d-124            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-125            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-126            [-1, 576, 7, 7]               0\n",
            "          Conv2d-127            [-1, 576, 7, 7]          14,400\n",
            "     BatchNorm2d-128            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-129            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-130            [-1, 576, 1, 1]               0\n",
            "          Conv2d-131            [-1, 144, 1, 1]          83,088\n",
            "            ReLU-132            [-1, 144, 1, 1]               0\n",
            "          Conv2d-133            [-1, 576, 1, 1]          83,520\n",
            "     Hardsigmoid-134            [-1, 576, 1, 1]               0\n",
            "SqueezeExcitation-135            [-1, 576, 7, 7]               0\n",
            "          Conv2d-136             [-1, 96, 7, 7]          55,296\n",
            "     BatchNorm2d-137             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-138             [-1, 96, 7, 7]               0\n",
            "          Conv2d-139            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-140            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-141            [-1, 576, 7, 7]               0\n",
            "          Conv2d-142            [-1, 576, 7, 7]          14,400\n",
            "     BatchNorm2d-143            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-144            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-145            [-1, 576, 1, 1]               0\n",
            "          Conv2d-146            [-1, 144, 1, 1]          83,088\n",
            "            ReLU-147            [-1, 144, 1, 1]               0\n",
            "          Conv2d-148            [-1, 576, 1, 1]          83,520\n",
            "     Hardsigmoid-149            [-1, 576, 1, 1]               0\n",
            "SqueezeExcitation-150            [-1, 576, 7, 7]               0\n",
            "          Conv2d-151             [-1, 96, 7, 7]          55,296\n",
            "     BatchNorm2d-152             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-153             [-1, 96, 7, 7]               0\n",
            "          Conv2d-154            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-155            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-156            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-157            [-1, 576, 1, 1]               0\n",
            "        Identity-158                  [-1, 576]               0\n",
            "     MobileNetV3-159                  [-1, 576]               0\n",
            "          Linear-160                 [-1, 1024]         590,848\n",
            "            ReLU-161                 [-1, 1024]               0\n",
            "          Linear-162                    [-1, 2]           2,050\n",
            "      LogSoftmax-163                    [-1, 2]               0\n",
            "          Linear-164                 [-1, 1024]         590,848\n",
            "       Hardswish-165                 [-1, 1024]               0\n",
            "         Dropout-166                 [-1, 1024]               0\n",
            "          Linear-167                   [-1, 31]          31,775\n",
            "      LogSoftmax-168                   [-1, 31]               0\n",
            "================================================================\n",
            "Total params: 2,142,529\n",
            "Trainable params: 2,142,529\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 34.63\n",
            "Params size (MB): 8.17\n",
            "Estimated Total Size (MB): 43.37\n",
            "----------------------------------------------------------------\n",
            "Number of FLOPs: 0.115523 GFLOPs (115.52 MFLOPs)\n",
            "Sparsity in feature_extractor.features.0.0.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.1.block.0.0.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.1.block.1.fc1.weight: 25.00%\n",
            "Sparsity in feature_extractor.features.1.block.1.fc2.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.1.block.2.0.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.2.block.0.0.weight: 19.44%\n",
            "Sparsity in feature_extractor.features.2.block.1.0.weight: 19.44%\n",
            "Sparsity in feature_extractor.features.2.block.2.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.3.block.0.0.weight: 20.45%\n",
            "Sparsity in feature_extractor.features.3.block.1.0.weight: 20.45%\n",
            "Sparsity in feature_extractor.features.3.block.2.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.4.block.0.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.4.block.1.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.4.block.2.fc1.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.4.block.2.fc2.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.4.block.3.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.0.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.1.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.2.fc1.weight: 20.31%\n",
            "Sparsity in feature_extractor.features.5.block.2.fc2.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.3.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.0.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.1.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.2.fc1.weight: 20.31%\n",
            "Sparsity in feature_extractor.features.6.block.2.fc2.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.3.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.0.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.1.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.2.fc1.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.7.block.2.fc2.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.3.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.8.block.0.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.8.block.1.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.8.block.2.fc1.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.8.block.2.fc2.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.8.block.3.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.9.block.0.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.9.block.1.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.9.block.2.fc1.weight: 19.44%\n",
            "Sparsity in feature_extractor.features.9.block.2.fc2.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.9.block.3.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.10.block.0.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.10.block.1.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.10.block.2.fc1.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.10.block.2.fc2.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.10.block.3.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.11.block.0.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.11.block.1.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.11.block.2.fc1.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.11.block.2.fc2.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.11.block.3.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.12.0.weight: 19.97%\n",
            "Sparsity in class_classifier.0.weight: 0.00%\n",
            "Sparsity in class_classifier.3.weight: 0.00%\n",
            "Sparsity in domain_classifier.0.weight: 0.00%\n",
            "Sparsity in domain_classifier.2.weight: 0.00%\n",
            "Global sparsity: 8.58%\n",
            "Number of FLOPs: 0.093694 GFLOPs (93.69 MFLOPs)\n",
            "[(Inference || test loss: 17.52594566345215] [accuracy_test: 2.8899999999999997 %]\n",
            "[(Inference || test loss: 0.5699880719184875] [accuracy_test: 86.31 %]\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 16, 112, 112]             432\n",
            "       BatchNorm2d-2         [-1, 16, 112, 112]              32\n",
            "         Hardswish-3         [-1, 16, 112, 112]               0\n",
            "            Conv2d-4           [-1, 16, 56, 56]             144\n",
            "       BatchNorm2d-5           [-1, 16, 56, 56]              32\n",
            "              ReLU-6           [-1, 16, 56, 56]               0\n",
            " AdaptiveAvgPool2d-7             [-1, 16, 1, 1]               0\n",
            "            Conv2d-8              [-1, 8, 1, 1]             136\n",
            "              ReLU-9              [-1, 8, 1, 1]               0\n",
            "           Conv2d-10             [-1, 16, 1, 1]             144\n",
            "      Hardsigmoid-11             [-1, 16, 1, 1]               0\n",
            "SqueezeExcitation-12           [-1, 16, 56, 56]               0\n",
            "           Conv2d-13           [-1, 16, 56, 56]             256\n",
            "      BatchNorm2d-14           [-1, 16, 56, 56]              32\n",
            " InvertedResidual-15           [-1, 16, 56, 56]               0\n",
            "           Conv2d-16           [-1, 72, 56, 56]           1,152\n",
            "      BatchNorm2d-17           [-1, 72, 56, 56]             144\n",
            "             ReLU-18           [-1, 72, 56, 56]               0\n",
            "           Conv2d-19           [-1, 72, 28, 28]             648\n",
            "      BatchNorm2d-20           [-1, 72, 28, 28]             144\n",
            "             ReLU-21           [-1, 72, 28, 28]               0\n",
            "           Conv2d-22           [-1, 24, 28, 28]           1,728\n",
            "      BatchNorm2d-23           [-1, 24, 28, 28]              48\n",
            " InvertedResidual-24           [-1, 24, 28, 28]               0\n",
            "           Conv2d-25           [-1, 88, 28, 28]           2,112\n",
            "      BatchNorm2d-26           [-1, 88, 28, 28]             176\n",
            "             ReLU-27           [-1, 88, 28, 28]               0\n",
            "           Conv2d-28           [-1, 88, 28, 28]             792\n",
            "      BatchNorm2d-29           [-1, 88, 28, 28]             176\n",
            "             ReLU-30           [-1, 88, 28, 28]               0\n",
            "           Conv2d-31           [-1, 24, 28, 28]           2,112\n",
            "      BatchNorm2d-32           [-1, 24, 28, 28]              48\n",
            " InvertedResidual-33           [-1, 24, 28, 28]               0\n",
            "           Conv2d-34           [-1, 96, 28, 28]           2,304\n",
            "      BatchNorm2d-35           [-1, 96, 28, 28]             192\n",
            "        Hardswish-36           [-1, 96, 28, 28]               0\n",
            "           Conv2d-37           [-1, 96, 14, 14]           2,400\n",
            "      BatchNorm2d-38           [-1, 96, 14, 14]             192\n",
            "        Hardswish-39           [-1, 96, 14, 14]               0\n",
            "AdaptiveAvgPool2d-40             [-1, 96, 1, 1]               0\n",
            "           Conv2d-41             [-1, 24, 1, 1]           2,328\n",
            "             ReLU-42             [-1, 24, 1, 1]               0\n",
            "           Conv2d-43             [-1, 96, 1, 1]           2,400\n",
            "      Hardsigmoid-44             [-1, 96, 1, 1]               0\n",
            "SqueezeExcitation-45           [-1, 96, 14, 14]               0\n",
            "           Conv2d-46           [-1, 40, 14, 14]           3,840\n",
            "      BatchNorm2d-47           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-48           [-1, 40, 14, 14]               0\n",
            "           Conv2d-49          [-1, 240, 14, 14]           9,600\n",
            "      BatchNorm2d-50          [-1, 240, 14, 14]             480\n",
            "        Hardswish-51          [-1, 240, 14, 14]               0\n",
            "           Conv2d-52          [-1, 240, 14, 14]           6,000\n",
            "      BatchNorm2d-53          [-1, 240, 14, 14]             480\n",
            "        Hardswish-54          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-55            [-1, 240, 1, 1]               0\n",
            "           Conv2d-56             [-1, 64, 1, 1]          15,424\n",
            "             ReLU-57             [-1, 64, 1, 1]               0\n",
            "           Conv2d-58            [-1, 240, 1, 1]          15,600\n",
            "      Hardsigmoid-59            [-1, 240, 1, 1]               0\n",
            "SqueezeExcitation-60          [-1, 240, 14, 14]               0\n",
            "           Conv2d-61           [-1, 40, 14, 14]           9,600\n",
            "      BatchNorm2d-62           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-63           [-1, 40, 14, 14]               0\n",
            "           Conv2d-64          [-1, 240, 14, 14]           9,600\n",
            "      BatchNorm2d-65          [-1, 240, 14, 14]             480\n",
            "        Hardswish-66          [-1, 240, 14, 14]               0\n",
            "           Conv2d-67          [-1, 240, 14, 14]           6,000\n",
            "      BatchNorm2d-68          [-1, 240, 14, 14]             480\n",
            "        Hardswish-69          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-70            [-1, 240, 1, 1]               0\n",
            "           Conv2d-71             [-1, 64, 1, 1]          15,424\n",
            "             ReLU-72             [-1, 64, 1, 1]               0\n",
            "           Conv2d-73            [-1, 240, 1, 1]          15,600\n",
            "      Hardsigmoid-74            [-1, 240, 1, 1]               0\n",
            "SqueezeExcitation-75          [-1, 240, 14, 14]               0\n",
            "           Conv2d-76           [-1, 40, 14, 14]           9,600\n",
            "      BatchNorm2d-77           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-78           [-1, 40, 14, 14]               0\n",
            "           Conv2d-79          [-1, 120, 14, 14]           4,800\n",
            "      BatchNorm2d-80          [-1, 120, 14, 14]             240\n",
            "        Hardswish-81          [-1, 120, 14, 14]               0\n",
            "           Conv2d-82          [-1, 120, 14, 14]           3,000\n",
            "      BatchNorm2d-83          [-1, 120, 14, 14]             240\n",
            "        Hardswish-84          [-1, 120, 14, 14]               0\n",
            "AdaptiveAvgPool2d-85            [-1, 120, 1, 1]               0\n",
            "           Conv2d-86             [-1, 32, 1, 1]           3,872\n",
            "             ReLU-87             [-1, 32, 1, 1]               0\n",
            "           Conv2d-88            [-1, 120, 1, 1]           3,960\n",
            "      Hardsigmoid-89            [-1, 120, 1, 1]               0\n",
            "SqueezeExcitation-90          [-1, 120, 14, 14]               0\n",
            "           Conv2d-91           [-1, 48, 14, 14]           5,760\n",
            "      BatchNorm2d-92           [-1, 48, 14, 14]              96\n",
            " InvertedResidual-93           [-1, 48, 14, 14]               0\n",
            "           Conv2d-94          [-1, 144, 14, 14]           6,912\n",
            "      BatchNorm2d-95          [-1, 144, 14, 14]             288\n",
            "        Hardswish-96          [-1, 144, 14, 14]               0\n",
            "           Conv2d-97          [-1, 144, 14, 14]           3,600\n",
            "      BatchNorm2d-98          [-1, 144, 14, 14]             288\n",
            "        Hardswish-99          [-1, 144, 14, 14]               0\n",
            "AdaptiveAvgPool2d-100            [-1, 144, 1, 1]               0\n",
            "          Conv2d-101             [-1, 40, 1, 1]           5,800\n",
            "            ReLU-102             [-1, 40, 1, 1]               0\n",
            "          Conv2d-103            [-1, 144, 1, 1]           5,904\n",
            "     Hardsigmoid-104            [-1, 144, 1, 1]               0\n",
            "SqueezeExcitation-105          [-1, 144, 14, 14]               0\n",
            "          Conv2d-106           [-1, 48, 14, 14]           6,912\n",
            "     BatchNorm2d-107           [-1, 48, 14, 14]              96\n",
            "InvertedResidual-108           [-1, 48, 14, 14]               0\n",
            "          Conv2d-109          [-1, 288, 14, 14]          13,824\n",
            "     BatchNorm2d-110          [-1, 288, 14, 14]             576\n",
            "       Hardswish-111          [-1, 288, 14, 14]               0\n",
            "          Conv2d-112            [-1, 288, 7, 7]           7,200\n",
            "     BatchNorm2d-113            [-1, 288, 7, 7]             576\n",
            "       Hardswish-114            [-1, 288, 7, 7]               0\n",
            "AdaptiveAvgPool2d-115            [-1, 288, 1, 1]               0\n",
            "          Conv2d-116             [-1, 72, 1, 1]          20,808\n",
            "            ReLU-117             [-1, 72, 1, 1]               0\n",
            "          Conv2d-118            [-1, 288, 1, 1]          21,024\n",
            "     Hardsigmoid-119            [-1, 288, 1, 1]               0\n",
            "SqueezeExcitation-120            [-1, 288, 7, 7]               0\n",
            "          Conv2d-121             [-1, 96, 7, 7]          27,648\n",
            "     BatchNorm2d-122             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-123             [-1, 96, 7, 7]               0\n",
            "          Conv2d-124            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-125            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-126            [-1, 576, 7, 7]               0\n",
            "          Conv2d-127            [-1, 576, 7, 7]          14,400\n",
            "     BatchNorm2d-128            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-129            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-130            [-1, 576, 1, 1]               0\n",
            "          Conv2d-131            [-1, 144, 1, 1]          83,088\n",
            "            ReLU-132            [-1, 144, 1, 1]               0\n",
            "          Conv2d-133            [-1, 576, 1, 1]          83,520\n",
            "     Hardsigmoid-134            [-1, 576, 1, 1]               0\n",
            "SqueezeExcitation-135            [-1, 576, 7, 7]               0\n",
            "          Conv2d-136             [-1, 96, 7, 7]          55,296\n",
            "     BatchNorm2d-137             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-138             [-1, 96, 7, 7]               0\n",
            "          Conv2d-139            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-140            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-141            [-1, 576, 7, 7]               0\n",
            "          Conv2d-142            [-1, 576, 7, 7]          14,400\n",
            "     BatchNorm2d-143            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-144            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-145            [-1, 576, 1, 1]               0\n",
            "          Conv2d-146            [-1, 144, 1, 1]          83,088\n",
            "            ReLU-147            [-1, 144, 1, 1]               0\n",
            "          Conv2d-148            [-1, 576, 1, 1]          83,520\n",
            "     Hardsigmoid-149            [-1, 576, 1, 1]               0\n",
            "SqueezeExcitation-150            [-1, 576, 7, 7]               0\n",
            "          Conv2d-151             [-1, 96, 7, 7]          55,296\n",
            "     BatchNorm2d-152             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-153             [-1, 96, 7, 7]               0\n",
            "          Conv2d-154            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-155            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-156            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-157            [-1, 576, 1, 1]               0\n",
            "        Identity-158                  [-1, 576]               0\n",
            "     MobileNetV3-159                  [-1, 576]               0\n",
            "          Linear-160                 [-1, 1024]         590,848\n",
            "            ReLU-161                 [-1, 1024]               0\n",
            "          Linear-162                    [-1, 2]           2,050\n",
            "      LogSoftmax-163                    [-1, 2]               0\n",
            "          Linear-164                 [-1, 1024]         590,848\n",
            "       Hardswish-165                 [-1, 1024]               0\n",
            "         Dropout-166                 [-1, 1024]               0\n",
            "          Linear-167                   [-1, 31]          31,775\n",
            "      LogSoftmax-168                   [-1, 31]               0\n",
            "================================================================\n",
            "Total params: 2,142,529\n",
            "Trainable params: 2,142,529\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 34.63\n",
            "Params size (MB): 8.17\n",
            "Estimated Total Size (MB): 43.37\n",
            "----------------------------------------------------------------\n",
            "Number of FLOPs: 0.115523 GFLOPs (115.52 MFLOPs)\n",
            "Sparsity in feature_extractor.features.0.0.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.1.block.0.0.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.1.block.1.fc1.weight: 25.00%\n",
            "Sparsity in feature_extractor.features.1.block.1.fc2.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.1.block.2.0.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.2.block.0.0.weight: 19.44%\n",
            "Sparsity in feature_extractor.features.2.block.1.0.weight: 19.44%\n",
            "Sparsity in feature_extractor.features.2.block.2.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.3.block.0.0.weight: 20.45%\n",
            "Sparsity in feature_extractor.features.3.block.1.0.weight: 20.45%\n",
            "Sparsity in feature_extractor.features.3.block.2.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.4.block.0.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.4.block.1.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.4.block.2.fc1.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.4.block.2.fc2.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.4.block.3.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.0.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.1.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.2.fc1.weight: 20.31%\n",
            "Sparsity in feature_extractor.features.5.block.2.fc2.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.3.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.0.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.1.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.2.fc1.weight: 20.31%\n",
            "Sparsity in feature_extractor.features.6.block.2.fc2.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.3.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.0.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.1.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.2.fc1.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.7.block.2.fc2.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.3.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.8.block.0.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.8.block.1.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.8.block.2.fc1.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.8.block.2.fc2.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.8.block.3.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.9.block.0.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.9.block.1.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.9.block.2.fc1.weight: 19.44%\n",
            "Sparsity in feature_extractor.features.9.block.2.fc2.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.9.block.3.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.10.block.0.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.10.block.1.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.10.block.2.fc1.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.10.block.2.fc2.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.10.block.3.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.11.block.0.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.11.block.1.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.11.block.2.fc1.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.11.block.2.fc2.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.11.block.3.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.12.0.weight: 19.97%\n",
            "Sparsity in class_classifier.0.weight: 0.00%\n",
            "Sparsity in class_classifier.3.weight: 0.00%\n",
            "Sparsity in domain_classifier.0.weight: 0.00%\n",
            "Sparsity in domain_classifier.2.weight: 0.00%\n",
            "Global sparsity: 8.58%\n",
            "Number of FLOPs: 0.093694 GFLOPs (93.69 MFLOPs)\n",
            "[(Inference || test loss: 6.478778839111328] [accuracy_test: 1.7500000000000002 %]\n",
            "[(Inference || test loss: 3.008251905441284] [accuracy_test: 27.32 %]\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 16, 112, 112]             432\n",
            "       BatchNorm2d-2         [-1, 16, 112, 112]              32\n",
            "         Hardswish-3         [-1, 16, 112, 112]               0\n",
            "            Conv2d-4           [-1, 16, 56, 56]             144\n",
            "       BatchNorm2d-5           [-1, 16, 56, 56]              32\n",
            "              ReLU-6           [-1, 16, 56, 56]               0\n",
            " AdaptiveAvgPool2d-7             [-1, 16, 1, 1]               0\n",
            "            Conv2d-8              [-1, 8, 1, 1]             136\n",
            "              ReLU-9              [-1, 8, 1, 1]               0\n",
            "           Conv2d-10             [-1, 16, 1, 1]             144\n",
            "      Hardsigmoid-11             [-1, 16, 1, 1]               0\n",
            "SqueezeExcitation-12           [-1, 16, 56, 56]               0\n",
            "           Conv2d-13           [-1, 16, 56, 56]             256\n",
            "      BatchNorm2d-14           [-1, 16, 56, 56]              32\n",
            " InvertedResidual-15           [-1, 16, 56, 56]               0\n",
            "           Conv2d-16           [-1, 72, 56, 56]           1,152\n",
            "      BatchNorm2d-17           [-1, 72, 56, 56]             144\n",
            "             ReLU-18           [-1, 72, 56, 56]               0\n",
            "           Conv2d-19           [-1, 72, 28, 28]             648\n",
            "      BatchNorm2d-20           [-1, 72, 28, 28]             144\n",
            "             ReLU-21           [-1, 72, 28, 28]               0\n",
            "           Conv2d-22           [-1, 24, 28, 28]           1,728\n",
            "      BatchNorm2d-23           [-1, 24, 28, 28]              48\n",
            " InvertedResidual-24           [-1, 24, 28, 28]               0\n",
            "           Conv2d-25           [-1, 88, 28, 28]           2,112\n",
            "      BatchNorm2d-26           [-1, 88, 28, 28]             176\n",
            "             ReLU-27           [-1, 88, 28, 28]               0\n",
            "           Conv2d-28           [-1, 88, 28, 28]             792\n",
            "      BatchNorm2d-29           [-1, 88, 28, 28]             176\n",
            "             ReLU-30           [-1, 88, 28, 28]               0\n",
            "           Conv2d-31           [-1, 24, 28, 28]           2,112\n",
            "      BatchNorm2d-32           [-1, 24, 28, 28]              48\n",
            " InvertedResidual-33           [-1, 24, 28, 28]               0\n",
            "           Conv2d-34           [-1, 96, 28, 28]           2,304\n",
            "      BatchNorm2d-35           [-1, 96, 28, 28]             192\n",
            "        Hardswish-36           [-1, 96, 28, 28]               0\n",
            "           Conv2d-37           [-1, 96, 14, 14]           2,400\n",
            "      BatchNorm2d-38           [-1, 96, 14, 14]             192\n",
            "        Hardswish-39           [-1, 96, 14, 14]               0\n",
            "AdaptiveAvgPool2d-40             [-1, 96, 1, 1]               0\n",
            "           Conv2d-41             [-1, 24, 1, 1]           2,328\n",
            "             ReLU-42             [-1, 24, 1, 1]               0\n",
            "           Conv2d-43             [-1, 96, 1, 1]           2,400\n",
            "      Hardsigmoid-44             [-1, 96, 1, 1]               0\n",
            "SqueezeExcitation-45           [-1, 96, 14, 14]               0\n",
            "           Conv2d-46           [-1, 40, 14, 14]           3,840\n",
            "      BatchNorm2d-47           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-48           [-1, 40, 14, 14]               0\n",
            "           Conv2d-49          [-1, 240, 14, 14]           9,600\n",
            "      BatchNorm2d-50          [-1, 240, 14, 14]             480\n",
            "        Hardswish-51          [-1, 240, 14, 14]               0\n",
            "           Conv2d-52          [-1, 240, 14, 14]           6,000\n",
            "      BatchNorm2d-53          [-1, 240, 14, 14]             480\n",
            "        Hardswish-54          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-55            [-1, 240, 1, 1]               0\n",
            "           Conv2d-56             [-1, 64, 1, 1]          15,424\n",
            "             ReLU-57             [-1, 64, 1, 1]               0\n",
            "           Conv2d-58            [-1, 240, 1, 1]          15,600\n",
            "      Hardsigmoid-59            [-1, 240, 1, 1]               0\n",
            "SqueezeExcitation-60          [-1, 240, 14, 14]               0\n",
            "           Conv2d-61           [-1, 40, 14, 14]           9,600\n",
            "      BatchNorm2d-62           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-63           [-1, 40, 14, 14]               0\n",
            "           Conv2d-64          [-1, 240, 14, 14]           9,600\n",
            "      BatchNorm2d-65          [-1, 240, 14, 14]             480\n",
            "        Hardswish-66          [-1, 240, 14, 14]               0\n",
            "           Conv2d-67          [-1, 240, 14, 14]           6,000\n",
            "      BatchNorm2d-68          [-1, 240, 14, 14]             480\n",
            "        Hardswish-69          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-70            [-1, 240, 1, 1]               0\n",
            "           Conv2d-71             [-1, 64, 1, 1]          15,424\n",
            "             ReLU-72             [-1, 64, 1, 1]               0\n",
            "           Conv2d-73            [-1, 240, 1, 1]          15,600\n",
            "      Hardsigmoid-74            [-1, 240, 1, 1]               0\n",
            "SqueezeExcitation-75          [-1, 240, 14, 14]               0\n",
            "           Conv2d-76           [-1, 40, 14, 14]           9,600\n",
            "      BatchNorm2d-77           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-78           [-1, 40, 14, 14]               0\n",
            "           Conv2d-79          [-1, 120, 14, 14]           4,800\n",
            "      BatchNorm2d-80          [-1, 120, 14, 14]             240\n",
            "        Hardswish-81          [-1, 120, 14, 14]               0\n",
            "           Conv2d-82          [-1, 120, 14, 14]           3,000\n",
            "      BatchNorm2d-83          [-1, 120, 14, 14]             240\n",
            "        Hardswish-84          [-1, 120, 14, 14]               0\n",
            "AdaptiveAvgPool2d-85            [-1, 120, 1, 1]               0\n",
            "           Conv2d-86             [-1, 32, 1, 1]           3,872\n",
            "             ReLU-87             [-1, 32, 1, 1]               0\n",
            "           Conv2d-88            [-1, 120, 1, 1]           3,960\n",
            "      Hardsigmoid-89            [-1, 120, 1, 1]               0\n",
            "SqueezeExcitation-90          [-1, 120, 14, 14]               0\n",
            "           Conv2d-91           [-1, 48, 14, 14]           5,760\n",
            "      BatchNorm2d-92           [-1, 48, 14, 14]              96\n",
            " InvertedResidual-93           [-1, 48, 14, 14]               0\n",
            "           Conv2d-94          [-1, 144, 14, 14]           6,912\n",
            "      BatchNorm2d-95          [-1, 144, 14, 14]             288\n",
            "        Hardswish-96          [-1, 144, 14, 14]               0\n",
            "           Conv2d-97          [-1, 144, 14, 14]           3,600\n",
            "      BatchNorm2d-98          [-1, 144, 14, 14]             288\n",
            "        Hardswish-99          [-1, 144, 14, 14]               0\n",
            "AdaptiveAvgPool2d-100            [-1, 144, 1, 1]               0\n",
            "          Conv2d-101             [-1, 40, 1, 1]           5,800\n",
            "            ReLU-102             [-1, 40, 1, 1]               0\n",
            "          Conv2d-103            [-1, 144, 1, 1]           5,904\n",
            "     Hardsigmoid-104            [-1, 144, 1, 1]               0\n",
            "SqueezeExcitation-105          [-1, 144, 14, 14]               0\n",
            "          Conv2d-106           [-1, 48, 14, 14]           6,912\n",
            "     BatchNorm2d-107           [-1, 48, 14, 14]              96\n",
            "InvertedResidual-108           [-1, 48, 14, 14]               0\n",
            "          Conv2d-109          [-1, 288, 14, 14]          13,824\n",
            "     BatchNorm2d-110          [-1, 288, 14, 14]             576\n",
            "       Hardswish-111          [-1, 288, 14, 14]               0\n",
            "          Conv2d-112            [-1, 288, 7, 7]           7,200\n",
            "     BatchNorm2d-113            [-1, 288, 7, 7]             576\n",
            "       Hardswish-114            [-1, 288, 7, 7]               0\n",
            "AdaptiveAvgPool2d-115            [-1, 288, 1, 1]               0\n",
            "          Conv2d-116             [-1, 72, 1, 1]          20,808\n",
            "            ReLU-117             [-1, 72, 1, 1]               0\n",
            "          Conv2d-118            [-1, 288, 1, 1]          21,024\n",
            "     Hardsigmoid-119            [-1, 288, 1, 1]               0\n",
            "SqueezeExcitation-120            [-1, 288, 7, 7]               0\n",
            "          Conv2d-121             [-1, 96, 7, 7]          27,648\n",
            "     BatchNorm2d-122             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-123             [-1, 96, 7, 7]               0\n",
            "          Conv2d-124            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-125            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-126            [-1, 576, 7, 7]               0\n",
            "          Conv2d-127            [-1, 576, 7, 7]          14,400\n",
            "     BatchNorm2d-128            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-129            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-130            [-1, 576, 1, 1]               0\n",
            "          Conv2d-131            [-1, 144, 1, 1]          83,088\n",
            "            ReLU-132            [-1, 144, 1, 1]               0\n",
            "          Conv2d-133            [-1, 576, 1, 1]          83,520\n",
            "     Hardsigmoid-134            [-1, 576, 1, 1]               0\n",
            "SqueezeExcitation-135            [-1, 576, 7, 7]               0\n",
            "          Conv2d-136             [-1, 96, 7, 7]          55,296\n",
            "     BatchNorm2d-137             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-138             [-1, 96, 7, 7]               0\n",
            "          Conv2d-139            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-140            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-141            [-1, 576, 7, 7]               0\n",
            "          Conv2d-142            [-1, 576, 7, 7]          14,400\n",
            "     BatchNorm2d-143            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-144            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-145            [-1, 576, 1, 1]               0\n",
            "          Conv2d-146            [-1, 144, 1, 1]          83,088\n",
            "            ReLU-147            [-1, 144, 1, 1]               0\n",
            "          Conv2d-148            [-1, 576, 1, 1]          83,520\n",
            "     Hardsigmoid-149            [-1, 576, 1, 1]               0\n",
            "SqueezeExcitation-150            [-1, 576, 7, 7]               0\n",
            "          Conv2d-151             [-1, 96, 7, 7]          55,296\n",
            "     BatchNorm2d-152             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-153             [-1, 96, 7, 7]               0\n",
            "          Conv2d-154            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-155            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-156            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-157            [-1, 576, 1, 1]               0\n",
            "        Identity-158                  [-1, 576]               0\n",
            "     MobileNetV3-159                  [-1, 576]               0\n",
            "          Linear-160                 [-1, 1024]         590,848\n",
            "            ReLU-161                 [-1, 1024]               0\n",
            "          Linear-162                    [-1, 2]           2,050\n",
            "      LogSoftmax-163                    [-1, 2]               0\n",
            "          Linear-164                 [-1, 1024]         590,848\n",
            "       Hardswish-165                 [-1, 1024]               0\n",
            "         Dropout-166                 [-1, 1024]               0\n",
            "          Linear-167                   [-1, 31]          31,775\n",
            "      LogSoftmax-168                   [-1, 31]               0\n",
            "================================================================\n",
            "Total params: 2,142,529\n",
            "Trainable params: 2,142,529\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 34.63\n",
            "Params size (MB): 8.17\n",
            "Estimated Total Size (MB): 43.37\n",
            "----------------------------------------------------------------\n",
            "Number of FLOPs: 0.115523 GFLOPs (115.52 MFLOPs)\n",
            "Sparsity in feature_extractor.features.0.0.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.1.block.0.0.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.1.block.1.fc1.weight: 25.00%\n",
            "Sparsity in feature_extractor.features.1.block.1.fc2.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.1.block.2.0.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.2.block.0.0.weight: 19.44%\n",
            "Sparsity in feature_extractor.features.2.block.1.0.weight: 19.44%\n",
            "Sparsity in feature_extractor.features.2.block.2.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.3.block.0.0.weight: 20.45%\n",
            "Sparsity in feature_extractor.features.3.block.1.0.weight: 20.45%\n",
            "Sparsity in feature_extractor.features.3.block.2.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.4.block.0.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.4.block.1.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.4.block.2.fc1.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.4.block.2.fc2.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.4.block.3.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.0.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.1.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.2.fc1.weight: 20.31%\n",
            "Sparsity in feature_extractor.features.5.block.2.fc2.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.3.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.0.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.1.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.2.fc1.weight: 20.31%\n",
            "Sparsity in feature_extractor.features.6.block.2.fc2.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.3.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.0.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.1.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.2.fc1.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.7.block.2.fc2.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.3.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.8.block.0.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.8.block.1.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.8.block.2.fc1.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.8.block.2.fc2.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.8.block.3.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.9.block.0.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.9.block.1.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.9.block.2.fc1.weight: 19.44%\n",
            "Sparsity in feature_extractor.features.9.block.2.fc2.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.9.block.3.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.10.block.0.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.10.block.1.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.10.block.2.fc1.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.10.block.2.fc2.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.10.block.3.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.11.block.0.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.11.block.1.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.11.block.2.fc1.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.11.block.2.fc2.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.11.block.3.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.12.0.weight: 19.97%\n",
            "Sparsity in class_classifier.0.weight: 0.00%\n",
            "Sparsity in class_classifier.3.weight: 0.00%\n",
            "Sparsity in domain_classifier.0.weight: 0.00%\n",
            "Sparsity in domain_classifier.2.weight: 0.00%\n",
            "Global sparsity: 8.58%\n",
            "Number of FLOPs: 0.093694 GFLOPs (93.69 MFLOPs)\n",
            "[(Inference || test loss: 22.85863494873047] [accuracy_test: 2.37 %]\n",
            "[(Inference || test loss: 1.0846600532531738] [accuracy_test: 81.13 %]\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 16, 112, 112]             432\n",
            "       BatchNorm2d-2         [-1, 16, 112, 112]              32\n",
            "         Hardswish-3         [-1, 16, 112, 112]               0\n",
            "            Conv2d-4           [-1, 16, 56, 56]             144\n",
            "       BatchNorm2d-5           [-1, 16, 56, 56]              32\n",
            "              ReLU-6           [-1, 16, 56, 56]               0\n",
            " AdaptiveAvgPool2d-7             [-1, 16, 1, 1]               0\n",
            "            Conv2d-8              [-1, 8, 1, 1]             136\n",
            "              ReLU-9              [-1, 8, 1, 1]               0\n",
            "           Conv2d-10             [-1, 16, 1, 1]             144\n",
            "      Hardsigmoid-11             [-1, 16, 1, 1]               0\n",
            "SqueezeExcitation-12           [-1, 16, 56, 56]               0\n",
            "           Conv2d-13           [-1, 16, 56, 56]             256\n",
            "      BatchNorm2d-14           [-1, 16, 56, 56]              32\n",
            " InvertedResidual-15           [-1, 16, 56, 56]               0\n",
            "           Conv2d-16           [-1, 72, 56, 56]           1,152\n",
            "      BatchNorm2d-17           [-1, 72, 56, 56]             144\n",
            "             ReLU-18           [-1, 72, 56, 56]               0\n",
            "           Conv2d-19           [-1, 72, 28, 28]             648\n",
            "      BatchNorm2d-20           [-1, 72, 28, 28]             144\n",
            "             ReLU-21           [-1, 72, 28, 28]               0\n",
            "           Conv2d-22           [-1, 24, 28, 28]           1,728\n",
            "      BatchNorm2d-23           [-1, 24, 28, 28]              48\n",
            " InvertedResidual-24           [-1, 24, 28, 28]               0\n",
            "           Conv2d-25           [-1, 88, 28, 28]           2,112\n",
            "      BatchNorm2d-26           [-1, 88, 28, 28]             176\n",
            "             ReLU-27           [-1, 88, 28, 28]               0\n",
            "           Conv2d-28           [-1, 88, 28, 28]             792\n",
            "      BatchNorm2d-29           [-1, 88, 28, 28]             176\n",
            "             ReLU-30           [-1, 88, 28, 28]               0\n",
            "           Conv2d-31           [-1, 24, 28, 28]           2,112\n",
            "      BatchNorm2d-32           [-1, 24, 28, 28]              48\n",
            " InvertedResidual-33           [-1, 24, 28, 28]               0\n",
            "           Conv2d-34           [-1, 96, 28, 28]           2,304\n",
            "      BatchNorm2d-35           [-1, 96, 28, 28]             192\n",
            "        Hardswish-36           [-1, 96, 28, 28]               0\n",
            "           Conv2d-37           [-1, 96, 14, 14]           2,400\n",
            "      BatchNorm2d-38           [-1, 96, 14, 14]             192\n",
            "        Hardswish-39           [-1, 96, 14, 14]               0\n",
            "AdaptiveAvgPool2d-40             [-1, 96, 1, 1]               0\n",
            "           Conv2d-41             [-1, 24, 1, 1]           2,328\n",
            "             ReLU-42             [-1, 24, 1, 1]               0\n",
            "           Conv2d-43             [-1, 96, 1, 1]           2,400\n",
            "      Hardsigmoid-44             [-1, 96, 1, 1]               0\n",
            "SqueezeExcitation-45           [-1, 96, 14, 14]               0\n",
            "           Conv2d-46           [-1, 40, 14, 14]           3,840\n",
            "      BatchNorm2d-47           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-48           [-1, 40, 14, 14]               0\n",
            "           Conv2d-49          [-1, 240, 14, 14]           9,600\n",
            "      BatchNorm2d-50          [-1, 240, 14, 14]             480\n",
            "        Hardswish-51          [-1, 240, 14, 14]               0\n",
            "           Conv2d-52          [-1, 240, 14, 14]           6,000\n",
            "      BatchNorm2d-53          [-1, 240, 14, 14]             480\n",
            "        Hardswish-54          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-55            [-1, 240, 1, 1]               0\n",
            "           Conv2d-56             [-1, 64, 1, 1]          15,424\n",
            "             ReLU-57             [-1, 64, 1, 1]               0\n",
            "           Conv2d-58            [-1, 240, 1, 1]          15,600\n",
            "      Hardsigmoid-59            [-1, 240, 1, 1]               0\n",
            "SqueezeExcitation-60          [-1, 240, 14, 14]               0\n",
            "           Conv2d-61           [-1, 40, 14, 14]           9,600\n",
            "      BatchNorm2d-62           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-63           [-1, 40, 14, 14]               0\n",
            "           Conv2d-64          [-1, 240, 14, 14]           9,600\n",
            "      BatchNorm2d-65          [-1, 240, 14, 14]             480\n",
            "        Hardswish-66          [-1, 240, 14, 14]               0\n",
            "           Conv2d-67          [-1, 240, 14, 14]           6,000\n",
            "      BatchNorm2d-68          [-1, 240, 14, 14]             480\n",
            "        Hardswish-69          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-70            [-1, 240, 1, 1]               0\n",
            "           Conv2d-71             [-1, 64, 1, 1]          15,424\n",
            "             ReLU-72             [-1, 64, 1, 1]               0\n",
            "           Conv2d-73            [-1, 240, 1, 1]          15,600\n",
            "      Hardsigmoid-74            [-1, 240, 1, 1]               0\n",
            "SqueezeExcitation-75          [-1, 240, 14, 14]               0\n",
            "           Conv2d-76           [-1, 40, 14, 14]           9,600\n",
            "      BatchNorm2d-77           [-1, 40, 14, 14]              80\n",
            " InvertedResidual-78           [-1, 40, 14, 14]               0\n",
            "           Conv2d-79          [-1, 120, 14, 14]           4,800\n",
            "      BatchNorm2d-80          [-1, 120, 14, 14]             240\n",
            "        Hardswish-81          [-1, 120, 14, 14]               0\n",
            "           Conv2d-82          [-1, 120, 14, 14]           3,000\n",
            "      BatchNorm2d-83          [-1, 120, 14, 14]             240\n",
            "        Hardswish-84          [-1, 120, 14, 14]               0\n",
            "AdaptiveAvgPool2d-85            [-1, 120, 1, 1]               0\n",
            "           Conv2d-86             [-1, 32, 1, 1]           3,872\n",
            "             ReLU-87             [-1, 32, 1, 1]               0\n",
            "           Conv2d-88            [-1, 120, 1, 1]           3,960\n",
            "      Hardsigmoid-89            [-1, 120, 1, 1]               0\n",
            "SqueezeExcitation-90          [-1, 120, 14, 14]               0\n",
            "           Conv2d-91           [-1, 48, 14, 14]           5,760\n",
            "      BatchNorm2d-92           [-1, 48, 14, 14]              96\n",
            " InvertedResidual-93           [-1, 48, 14, 14]               0\n",
            "           Conv2d-94          [-1, 144, 14, 14]           6,912\n",
            "      BatchNorm2d-95          [-1, 144, 14, 14]             288\n",
            "        Hardswish-96          [-1, 144, 14, 14]               0\n",
            "           Conv2d-97          [-1, 144, 14, 14]           3,600\n",
            "      BatchNorm2d-98          [-1, 144, 14, 14]             288\n",
            "        Hardswish-99          [-1, 144, 14, 14]               0\n",
            "AdaptiveAvgPool2d-100            [-1, 144, 1, 1]               0\n",
            "          Conv2d-101             [-1, 40, 1, 1]           5,800\n",
            "            ReLU-102             [-1, 40, 1, 1]               0\n",
            "          Conv2d-103            [-1, 144, 1, 1]           5,904\n",
            "     Hardsigmoid-104            [-1, 144, 1, 1]               0\n",
            "SqueezeExcitation-105          [-1, 144, 14, 14]               0\n",
            "          Conv2d-106           [-1, 48, 14, 14]           6,912\n",
            "     BatchNorm2d-107           [-1, 48, 14, 14]              96\n",
            "InvertedResidual-108           [-1, 48, 14, 14]               0\n",
            "          Conv2d-109          [-1, 288, 14, 14]          13,824\n",
            "     BatchNorm2d-110          [-1, 288, 14, 14]             576\n",
            "       Hardswish-111          [-1, 288, 14, 14]               0\n",
            "          Conv2d-112            [-1, 288, 7, 7]           7,200\n",
            "     BatchNorm2d-113            [-1, 288, 7, 7]             576\n",
            "       Hardswish-114            [-1, 288, 7, 7]               0\n",
            "AdaptiveAvgPool2d-115            [-1, 288, 1, 1]               0\n",
            "          Conv2d-116             [-1, 72, 1, 1]          20,808\n",
            "            ReLU-117             [-1, 72, 1, 1]               0\n",
            "          Conv2d-118            [-1, 288, 1, 1]          21,024\n",
            "     Hardsigmoid-119            [-1, 288, 1, 1]               0\n",
            "SqueezeExcitation-120            [-1, 288, 7, 7]               0\n",
            "          Conv2d-121             [-1, 96, 7, 7]          27,648\n",
            "     BatchNorm2d-122             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-123             [-1, 96, 7, 7]               0\n",
            "          Conv2d-124            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-125            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-126            [-1, 576, 7, 7]               0\n",
            "          Conv2d-127            [-1, 576, 7, 7]          14,400\n",
            "     BatchNorm2d-128            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-129            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-130            [-1, 576, 1, 1]               0\n",
            "          Conv2d-131            [-1, 144, 1, 1]          83,088\n",
            "            ReLU-132            [-1, 144, 1, 1]               0\n",
            "          Conv2d-133            [-1, 576, 1, 1]          83,520\n",
            "     Hardsigmoid-134            [-1, 576, 1, 1]               0\n",
            "SqueezeExcitation-135            [-1, 576, 7, 7]               0\n",
            "          Conv2d-136             [-1, 96, 7, 7]          55,296\n",
            "     BatchNorm2d-137             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-138             [-1, 96, 7, 7]               0\n",
            "          Conv2d-139            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-140            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-141            [-1, 576, 7, 7]               0\n",
            "          Conv2d-142            [-1, 576, 7, 7]          14,400\n",
            "     BatchNorm2d-143            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-144            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-145            [-1, 576, 1, 1]               0\n",
            "          Conv2d-146            [-1, 144, 1, 1]          83,088\n",
            "            ReLU-147            [-1, 144, 1, 1]               0\n",
            "          Conv2d-148            [-1, 576, 1, 1]          83,520\n",
            "     Hardsigmoid-149            [-1, 576, 1, 1]               0\n",
            "SqueezeExcitation-150            [-1, 576, 7, 7]               0\n",
            "          Conv2d-151             [-1, 96, 7, 7]          55,296\n",
            "     BatchNorm2d-152             [-1, 96, 7, 7]             192\n",
            "InvertedResidual-153             [-1, 96, 7, 7]               0\n",
            "          Conv2d-154            [-1, 576, 7, 7]          55,296\n",
            "     BatchNorm2d-155            [-1, 576, 7, 7]           1,152\n",
            "       Hardswish-156            [-1, 576, 7, 7]               0\n",
            "AdaptiveAvgPool2d-157            [-1, 576, 1, 1]               0\n",
            "        Identity-158                  [-1, 576]               0\n",
            "     MobileNetV3-159                  [-1, 576]               0\n",
            "          Linear-160                 [-1, 1024]         590,848\n",
            "            ReLU-161                 [-1, 1024]               0\n",
            "          Linear-162                    [-1, 2]           2,050\n",
            "      LogSoftmax-163                    [-1, 2]               0\n",
            "          Linear-164                 [-1, 1024]         590,848\n",
            "       Hardswish-165                 [-1, 1024]               0\n",
            "         Dropout-166                 [-1, 1024]               0\n",
            "          Linear-167                   [-1, 31]          31,775\n",
            "      LogSoftmax-168                   [-1, 31]               0\n",
            "================================================================\n",
            "Total params: 2,142,529\n",
            "Trainable params: 2,142,529\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 34.63\n",
            "Params size (MB): 8.17\n",
            "Estimated Total Size (MB): 43.37\n",
            "----------------------------------------------------------------\n",
            "Number of FLOPs: 0.115523 GFLOPs (115.52 MFLOPs)\n",
            "Sparsity in feature_extractor.features.0.0.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.1.block.0.0.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.1.block.1.fc1.weight: 25.00%\n",
            "Sparsity in feature_extractor.features.1.block.1.fc2.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.1.block.2.0.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.2.block.0.0.weight: 19.44%\n",
            "Sparsity in feature_extractor.features.2.block.1.0.weight: 19.44%\n",
            "Sparsity in feature_extractor.features.2.block.2.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.3.block.0.0.weight: 20.45%\n",
            "Sparsity in feature_extractor.features.3.block.1.0.weight: 20.45%\n",
            "Sparsity in feature_extractor.features.3.block.2.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.4.block.0.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.4.block.1.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.4.block.2.fc1.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.4.block.2.fc2.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.4.block.3.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.0.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.1.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.2.fc1.weight: 20.31%\n",
            "Sparsity in feature_extractor.features.5.block.2.fc2.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.5.block.3.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.0.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.1.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.2.fc1.weight: 20.31%\n",
            "Sparsity in feature_extractor.features.6.block.2.fc2.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.6.block.3.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.0.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.1.0.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.2.fc1.weight: 18.75%\n",
            "Sparsity in feature_extractor.features.7.block.2.fc2.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.7.block.3.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.8.block.0.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.8.block.1.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.8.block.2.fc1.weight: 20.00%\n",
            "Sparsity in feature_extractor.features.8.block.2.fc2.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.8.block.3.0.weight: 20.83%\n",
            "Sparsity in feature_extractor.features.9.block.0.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.9.block.1.0.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.9.block.2.fc1.weight: 19.44%\n",
            "Sparsity in feature_extractor.features.9.block.2.fc2.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.9.block.3.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.10.block.0.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.10.block.1.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.10.block.2.fc1.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.10.block.2.fc2.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.10.block.3.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.11.block.0.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.11.block.1.0.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.11.block.2.fc1.weight: 20.14%\n",
            "Sparsity in feature_extractor.features.11.block.2.fc2.weight: 19.97%\n",
            "Sparsity in feature_extractor.features.11.block.3.0.weight: 19.79%\n",
            "Sparsity in feature_extractor.features.12.0.weight: 19.97%\n",
            "Sparsity in class_classifier.0.weight: 0.00%\n",
            "Sparsity in class_classifier.3.weight: 0.00%\n",
            "Sparsity in domain_classifier.0.weight: 0.00%\n",
            "Sparsity in domain_classifier.2.weight: 0.00%\n",
            "Global sparsity: 8.58%\n",
            "Number of FLOPs: 0.093694 GFLOPs (93.69 MFLOPs)\n",
            "[(Inference || test loss: 9.594409942626953] [accuracy_test: 2.45 %]\n",
            "Results saved to pruning_result.csv\n"
          ]
        }
      ],
      "source": [
        "# Loop through your list and apply the function\n",
        "csv_output = \"pruning_result.csv\"\n",
        "PATH_CP = './cp/'\n",
        "headers = ['model_name', 'pre-pruning', 'post-pruning', 'acc']\n",
        "with open(csv_output, mode='w', newline='') as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=headers)\n",
        "    writer.writeheader()  # Write header\n",
        "    for idx, (model, model_path, train_loader, test_loader, to_save) in enumerate(lst):\n",
        "        results = {}\n",
        "        save_path = f\"pruned_model_{idx}.pth\"\n",
        "        results[\"model_name\"] = model_path.rstrip(\".pth\")\n",
        "        results[\"pre-pruning\"], results[\"post-pruning\"], results[\"acc\"] = prune_evaluate_save(model, model_path, train_loader, test_loader, save_path)\n",
        "\n",
        "\n",
        "            # Write the results to CSV\n",
        "        writer.writerow(results)\n",
        "\n",
        "    print(f\"Results saved to {csv_output}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1SxUfadQZje"
      },
      "outputs": [],
      "source": [
        "def create_balanced_dataloaders(src_dataloader, tar_dataloader, batch_size, blur_sigma=2):\n",
        "    src_size = len(src_dataloader.dataset)\n",
        "    tar_size = len(tar_dataloader.dataset)\n",
        "\n",
        "    # Define a blur transformation\n",
        "    blur_transform = transforms.GaussianBlur(kernel_size=(5, 5), sigma=blur_sigma)\n",
        "\n",
        "    def blur_images(batch):\n",
        "        images, labels = batch\n",
        "        # Apply blur to images\n",
        "        blurred_images = blur_transform(images)\n",
        "        return blurred_images, labels\n",
        "\n",
        "    # Ensure iterators are reset appropriately for smaller dataset\n",
        "    if src_size > tar_size:\n",
        "        Dl_source_iter = iter(src_dataloader)\n",
        "        Dl_target_iter = itertools.cycle(tar_dataloader)  # Cycle the smaller dataset\n",
        "        max_batches = len(src_dataloader)\n",
        "\n",
        "        # Apply blur to the cycled target images\n",
        "        Dl_target_iter = (blur_images(batch) for batch in Dl_target_iter)\n",
        "    else:\n",
        "        Dl_source_iter = itertools.cycle(src_dataloader)  # Cycle the smaller dataset\n",
        "        Dl_target_iter = iter(tar_dataloader)\n",
        "        max_batches = len(tar_dataloader)\n",
        "\n",
        "        # Apply blur to the cycled source images\n",
        "        Dl_source_iter = (blur_images(batch) for batch in Dl_source_iter)\n",
        "\n",
        "    return Dl_source_iter, Dl_target_iter, max_batches\n",
        "\n",
        "\n",
        "\n",
        "def train_model_with_balanced_batches_DANN_Pruned(model, optimizer, scheduler, loss_fn_class, loss_fn_domain,\n",
        "                                           src, tar, device,\n",
        "                                           num_epochs=30, batch_size=64, name=\"Test\"):\n",
        "    \"\"\"\n",
        "    Train the DANN model using balanced batches from both source and target domain dataloaders.\n",
        "    Args:\n",
        "        model: The DANN model (including feature extractor and domain classifier).\n",
        "        optimizer: Optimizer for the model.\n",
        "        loss_fn_class: Loss function for classification (e.g., cross-entropy).\n",
        "        loss_fn_domain: Loss function for domain classification (e.g., binary cross-entropy).\n",
        "        src_dataloader: DataLoader for the source domain.\n",
        "        tar_dataloader: DataLoader for the target domain.\n",
        "        device: Device to run the training on (e.g., 'cuda' or 'cpu').\n",
        "        num_epochs: Number of training epochs.\n",
        "        batch_size: Batch size used in training.\n",
        "\n",
        "    Returns:\n",
        "        training_logs_so: A dictionary with training loss and accuracies for source and target domains.\n",
        "    \"\"\"\n",
        "\n",
        "    # Logs to track training performance\n",
        "    training_logs_so = {\"train_loss\": [], \"train_src_acc\": [], \"train_tar_acc\": []}\n",
        "    model_save_path = name\n",
        "    best_vloss = 1_000_000.\n",
        "\n",
        "    for epoch_idx in range(num_epochs):\n",
        "        train_loss, train_src_correct, train_tar_correct = 0, 0, 0\n",
        "        actual_src_samples, actual_tar_samples = 0, 0  # Track the actual number of processed samples\n",
        "\n",
        "        print(f'Epoch {epoch_idx+1:04d} / {num_epochs:04d}', end='\\n============\\n')\n",
        "\n",
        "        # Create balanced iterators for source and target domain\n",
        "        Dl_source_iter, Dl_target_iter, max_batches = create_balanced_dataloaders(src, tar, batch_size)\n",
        "\n",
        "        for batch_idx in range(max_batches):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get next batch from source and target datasets\n",
        "            X_s, y_s = next(Dl_source_iter)\n",
        "            X_t, y_t = next(Dl_target_iter)\n",
        "\n",
        "            # Ensure the batches have the same size by taking the minimum batch size\n",
        "            if X_s.shape[0] != X_t.shape[0]:\n",
        "                min_bs = min(X_s.shape[0], X_t.shape[0])\n",
        "                X_s, y_s = X_s[:min_bs], y_s[:min_bs]\n",
        "                X_t, y_t = X_t[:min_bs], y_t[:min_bs]\n",
        "\n",
        "            # Track the number of samples processed\n",
        "            actual_src_samples += y_s.size(0)\n",
        "            actual_tar_samples += y_t.size(0)\n",
        "\n",
        "            # Send data to device (GPU or CPU)\n",
        "            X_s, y_s = X_s.to(device), y_s.to(device)\n",
        "            X_t, y_t = X_t.to(device), y_t.to(device)\n",
        "\n",
        "            # Dynamic adjustment of grl_lambda\n",
        "            p = float(batch_idx + epoch_idx * max_batches) / (num_epochs * max_batches)\n",
        "            grl_lambda = (2 / (1 + np.exp(-10 * p)) - 1) * 0.5\n",
        "\n",
        "            # Prepare labels for domain classification (source domain = 0, target domain = 1)\n",
        "            y_s_domain = torch.zeros(X_s.size(0), dtype=torch.long).to(device)\n",
        "            y_t_domain = torch.ones(X_t.size(0), dtype=torch.long).to(device)\n",
        "\n",
        "            # Forward pass through the model\n",
        "            class_prediction_s, domain_prediction_s = model(X_s, grl_lambda)\n",
        "            _, domain_prediction_t = model(X_t, grl_lambda)\n",
        "\n",
        "            # Calculate classification and domain losses\n",
        "            loss_s_label = loss_fn_class(class_prediction_s, y_s)  # Source classification loss\n",
        "            loss_s_domain = loss_fn_domain(domain_prediction_s, y_s_domain)  # Source domain loss\n",
        "            loss_t_domain = loss_fn_domain(domain_prediction_t, y_t_domain)  # Target domain loss\n",
        "\n",
        "            # Combine the losses with a weight for domain loss\n",
        "            loss = loss_s_label + loss_s_domain + loss_t_domain\n",
        "\n",
        "            # Backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Accumulate the training loss\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Compute accuracy for source domain classification\n",
        "            train_src_correct += (class_prediction_s.argmax(1) == y_s).float().sum().item()\n",
        "\n",
        "            # Compute accuracy for target domain classification (even though target labels are not used in training)\n",
        "            with torch.no_grad():\n",
        "                class_prediction_t, _ = model(X_t, grl_lambda)\n",
        "            train_tar_correct += (class_prediction_t.argmax(1) == y_t).float().sum().item()\n",
        "\n",
        "            # Print batch info\n",
        "            print(f'[{batch_idx+1}/{max_batches}] Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Calculate and store metrics for the epoch\n",
        "        training_logs_so[\"train_loss\"].append(train_loss / max_batches)\n",
        "        training_logs_so[\"train_src_acc\"].append(train_src_correct / actual_src_samples)\n",
        "        training_logs_so[\"train_tar_acc\"].append(train_tar_correct / actual_tar_samples)\n",
        "\n",
        "        print(f'Epoch: {epoch_idx+1} | '\n",
        "              f'Source Accuracy: {train_src_correct / actual_src_samples:.4f}, '\n",
        "              f'Target Accuracy: {train_tar_correct / actual_tar_samples:.4f}, '\n",
        "              f'Loss: {train_loss / max_batches:.4f}')\n",
        "        if train_loss < best_vloss:\n",
        "            best_vloss = train_loss\n",
        "            path_save_cp = './cp/'\n",
        "            if not os.path.exists(path_save_cp): os.mkdir(path_save_cp)\n",
        "            torch.save(model.state_dict(), path_save_cp+model_save_path)\n",
        "\n",
        "\n",
        "    save_training_logs(training_logs_so, f\"LOG_DANN_Pruned_{name.rstrip('.pth')}\")\n",
        "\n",
        "    return training_logs_so"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhStJQgNPXmV",
        "outputId": "c307b066-8cbb-4735-e4a5-f42b8894bcb3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-49-b57f027c97d5>:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0001 / 0010\n",
            "============\n",
            "[1/33] Loss: 5.4421\n",
            "[2/33] Loss: 5.1149\n",
            "[3/33] Loss: 4.7143\n",
            "[4/33] Loss: 4.2963\n",
            "[5/33] Loss: 4.8800\n",
            "[6/33] Loss: 3.7760\n",
            "[7/33] Loss: 4.1838\n",
            "[8/33] Loss: 4.2391\n",
            "[9/33] Loss: 3.5363\n",
            "[10/33] Loss: 3.7206\n",
            "[11/33] Loss: 3.6576\n",
            "[12/33] Loss: 3.3344\n",
            "[13/33] Loss: 2.9847\n",
            "[14/33] Loss: 3.5251\n",
            "[15/33] Loss: 3.7235\n",
            "[16/33] Loss: 2.9432\n",
            "[17/33] Loss: 3.0608\n",
            "[18/33] Loss: 2.8301\n",
            "[19/33] Loss: 2.7064\n",
            "[20/33] Loss: 3.2555\n",
            "[21/33] Loss: 2.5630\n",
            "[22/33] Loss: 2.9048\n",
            "[23/33] Loss: 2.8706\n",
            "[24/33] Loss: 2.5785\n",
            "[25/33] Loss: 2.6430\n",
            "[26/33] Loss: 2.8405\n",
            "[27/33] Loss: 3.0477\n",
            "[28/33] Loss: 2.9205\n",
            "[29/33] Loss: 2.5239\n",
            "[30/33] Loss: 2.5941\n",
            "[31/33] Loss: 2.2750\n",
            "[32/33] Loss: 2.2542\n",
            "[33/33] Loss: 2.5836\n",
            "Epoch: 1 | Source Accuracy: 0.4861, Target Accuracy: 0.2800, Loss: 3.3492\n",
            "Epoch 0002 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.9380\n",
            "[2/33] Loss: 2.4725\n",
            "[3/33] Loss: 1.9991\n",
            "[4/33] Loss: 2.0660\n",
            "[5/33] Loss: 1.6108\n",
            "[6/33] Loss: 1.6710\n",
            "[7/33] Loss: 2.1205\n",
            "[8/33] Loss: 1.8324\n",
            "[9/33] Loss: 1.9721\n",
            "[10/33] Loss: 1.9970\n",
            "[11/33] Loss: 1.9394\n",
            "[12/33] Loss: 2.3264\n",
            "[13/33] Loss: 2.1419\n",
            "[14/33] Loss: 1.9920\n",
            "[15/33] Loss: 2.1240\n",
            "[16/33] Loss: 1.8586\n",
            "[17/33] Loss: 1.8495\n",
            "[18/33] Loss: 2.2051\n",
            "[19/33] Loss: 2.6484\n",
            "[20/33] Loss: 2.4743\n",
            "[21/33] Loss: 2.4879\n",
            "[22/33] Loss: 2.1215\n",
            "[23/33] Loss: 2.2456\n",
            "[24/33] Loss: 2.1187\n",
            "[25/33] Loss: 2.6364\n",
            "[26/33] Loss: 2.2007\n",
            "[27/33] Loss: 2.6739\n",
            "[28/33] Loss: 2.4240\n",
            "[29/33] Loss: 2.3853\n",
            "[30/33] Loss: 2.2083\n",
            "[31/33] Loss: 2.6133\n",
            "[32/33] Loss: 1.5965\n",
            "[33/33] Loss: 1.5423\n",
            "Epoch: 2 | Source Accuracy: 0.7861, Target Accuracy: 0.4166, Loss: 2.1362\n",
            "Epoch 0003 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.8069\n",
            "[2/33] Loss: 1.6753\n",
            "[3/33] Loss: 1.9773\n",
            "[4/33] Loss: 1.5460\n",
            "[5/33] Loss: 1.4908\n",
            "[6/33] Loss: 2.0058\n",
            "[7/33] Loss: 2.0587\n",
            "[8/33] Loss: 1.1415\n",
            "[9/33] Loss: 2.5756\n",
            "[10/33] Loss: 3.9623\n",
            "[11/33] Loss: 3.9887\n",
            "[12/33] Loss: 4.1803\n",
            "[13/33] Loss: 2.1000\n",
            "[14/33] Loss: 1.7130\n",
            "[15/33] Loss: 3.3392\n",
            "[16/33] Loss: 2.6709\n",
            "[17/33] Loss: 2.2704\n",
            "[18/33] Loss: 2.1834\n",
            "[19/33] Loss: 1.8023\n",
            "[20/33] Loss: 1.2838\n",
            "[21/33] Loss: 1.5213\n",
            "[22/33] Loss: 3.8786\n",
            "[23/33] Loss: 4.2579\n",
            "[24/33] Loss: 5.3766\n",
            "[25/33] Loss: 4.5045\n",
            "[26/33] Loss: 3.5346\n",
            "[27/33] Loss: 4.8683\n",
            "[28/33] Loss: 3.1763\n",
            "[29/33] Loss: 2.6685\n",
            "[30/33] Loss: 3.3886\n",
            "[31/33] Loss: 3.8295\n",
            "[32/33] Loss: 3.7165\n",
            "[33/33] Loss: 4.2048\n",
            "Epoch: 3 | Source Accuracy: 0.7900, Target Accuracy: 0.3875, Loss: 2.8697\n",
            "Epoch 0004 / 0010\n",
            "============\n",
            "[1/33] Loss: 3.4909\n",
            "[2/33] Loss: 2.2990\n",
            "[3/33] Loss: 3.0707\n",
            "[4/33] Loss: 2.6386\n",
            "[5/33] Loss: 2.3078\n",
            "[6/33] Loss: 2.1197\n",
            "[7/33] Loss: 1.9440\n",
            "[8/33] Loss: 1.8494\n",
            "[9/33] Loss: 2.7466\n",
            "[10/33] Loss: 1.8142\n",
            "[11/33] Loss: 1.7058\n",
            "[12/33] Loss: 2.1158\n",
            "[13/33] Loss: 2.5661\n",
            "[14/33] Loss: 1.8841\n",
            "[15/33] Loss: 1.8663\n",
            "[16/33] Loss: 2.1618\n",
            "[17/33] Loss: 1.8050\n",
            "[18/33] Loss: 2.0361\n",
            "[19/33] Loss: 2.1647\n",
            "[20/33] Loss: 2.2381\n",
            "[21/33] Loss: 2.2714\n",
            "[22/33] Loss: 2.1045\n",
            "[23/33] Loss: 2.1619\n",
            "[24/33] Loss: 1.8518\n",
            "[25/33] Loss: 1.8944\n",
            "[26/33] Loss: 1.9778\n",
            "[27/33] Loss: 2.0375\n",
            "[28/33] Loss: 2.1202\n",
            "[29/33] Loss: 2.0931\n",
            "[30/33] Loss: 1.5907\n",
            "[31/33] Loss: 2.0544\n",
            "[32/33] Loss: 1.7591\n",
            "[33/33] Loss: 1.5168\n",
            "Epoch: 4 | Source Accuracy: 0.8109, Target Accuracy: 0.4001, Loss: 2.1290\n",
            "Epoch 0005 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.7100\n",
            "[2/33] Loss: 1.6592\n",
            "[3/33] Loss: 1.8197\n",
            "[4/33] Loss: 2.6405\n",
            "[5/33] Loss: 2.1232\n",
            "[6/33] Loss: 1.8263\n",
            "[7/33] Loss: 1.2038\n",
            "[8/33] Loss: 1.7209\n",
            "[9/33] Loss: 1.9647\n",
            "[10/33] Loss: 1.9384\n",
            "[11/33] Loss: 1.8481\n",
            "[12/33] Loss: 1.8364\n",
            "[13/33] Loss: 1.5433\n",
            "[14/33] Loss: 1.6812\n",
            "[15/33] Loss: 1.4574\n",
            "[16/33] Loss: 1.7897\n",
            "[17/33] Loss: 1.4892\n",
            "[18/33] Loss: 1.6078\n",
            "[19/33] Loss: 1.8627\n",
            "[20/33] Loss: 1.7070\n",
            "[21/33] Loss: 1.7347\n",
            "[22/33] Loss: 1.3977\n",
            "[23/33] Loss: 1.4551\n",
            "[24/33] Loss: 1.6609\n",
            "[25/33] Loss: 1.4607\n",
            "[26/33] Loss: 1.6304\n",
            "[27/33] Loss: 1.8445\n",
            "[28/33] Loss: 1.6876\n",
            "[29/33] Loss: 1.5506\n",
            "[30/33] Loss: 1.5302\n",
            "[31/33] Loss: 1.5859\n",
            "[32/33] Loss: 1.4421\n",
            "[33/33] Loss: 1.6344\n",
            "Epoch: 5 | Source Accuracy: 0.9037, Target Accuracy: 0.4633, Loss: 1.6983\n",
            "Epoch 0006 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.3685\n",
            "[2/33] Loss: 1.4576\n",
            "[3/33] Loss: 1.3455\n",
            "[4/33] Loss: 1.4607\n",
            "[5/33] Loss: 1.3227\n",
            "[6/33] Loss: 1.3314\n",
            "[7/33] Loss: 1.6198\n",
            "[8/33] Loss: 1.4007\n",
            "[9/33] Loss: 1.3415\n",
            "[10/33] Loss: 1.3744\n",
            "[11/33] Loss: 1.2278\n",
            "[12/33] Loss: 1.3096\n",
            "[13/33] Loss: 1.4227\n",
            "[14/33] Loss: 1.6714\n",
            "[15/33] Loss: 1.4064\n",
            "[16/33] Loss: 1.4749\n",
            "[17/33] Loss: 1.3946\n",
            "[18/33] Loss: 1.5381\n",
            "[19/33] Loss: 1.5042\n",
            "[20/33] Loss: 1.5604\n",
            "[21/33] Loss: 1.3973\n",
            "[22/33] Loss: 1.3293\n",
            "[23/33] Loss: 1.5109\n",
            "[24/33] Loss: 1.5146\n",
            "[25/33] Loss: 1.4707\n",
            "[26/33] Loss: 1.4920\n",
            "[27/33] Loss: 1.4268\n",
            "[28/33] Loss: 1.3420\n",
            "[29/33] Loss: 1.6149\n",
            "[30/33] Loss: 1.4405\n",
            "[31/33] Loss: 1.3480\n",
            "[32/33] Loss: 1.4244\n",
            "[33/33] Loss: 1.5342\n",
            "Epoch: 6 | Source Accuracy: 0.9665, Target Accuracy: 0.4789, Loss: 1.4357\n",
            "Epoch 0007 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.4835\n",
            "[2/33] Loss: 1.4478\n",
            "[3/33] Loss: 1.2464\n",
            "[4/33] Loss: 1.2205\n",
            "[5/33] Loss: 1.3474\n",
            "[6/33] Loss: 1.3578\n",
            "[7/33] Loss: 1.3507\n",
            "[8/33] Loss: 1.4462\n",
            "[9/33] Loss: 1.3581\n",
            "[10/33] Loss: 1.4069\n",
            "[11/33] Loss: 1.4112\n",
            "[12/33] Loss: 1.2476\n",
            "[13/33] Loss: 1.1559\n",
            "[14/33] Loss: 1.4772\n",
            "[15/33] Loss: 1.4933\n",
            "[16/33] Loss: 1.1616\n",
            "[17/33] Loss: 1.3814\n",
            "[18/33] Loss: 1.2885\n",
            "[19/33] Loss: 1.3317\n",
            "[20/33] Loss: 1.6756\n",
            "[21/33] Loss: 1.5710\n",
            "[22/33] Loss: 1.3501\n",
            "[23/33] Loss: 1.4394\n",
            "[24/33] Loss: 1.3484\n",
            "[25/33] Loss: 1.3809\n",
            "[26/33] Loss: 1.5354\n",
            "[27/33] Loss: 1.4364\n",
            "[28/33] Loss: 1.1974\n",
            "[29/33] Loss: 1.4528\n",
            "[30/33] Loss: 1.2412\n",
            "[31/33] Loss: 1.3796\n",
            "[32/33] Loss: 1.4466\n",
            "[33/33] Loss: 1.4867\n",
            "Epoch: 7 | Source Accuracy: 0.9825, Target Accuracy: 0.4541, Loss: 1.3805\n",
            "Epoch 0008 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.3611\n",
            "[2/33] Loss: 1.3795\n",
            "[3/33] Loss: 1.5384\n",
            "[4/33] Loss: 1.3967\n",
            "[5/33] Loss: 1.3153\n",
            "[6/33] Loss: 1.3678\n",
            "[7/33] Loss: 1.2871\n",
            "[8/33] Loss: 1.3247\n",
            "[9/33] Loss: 1.3747\n",
            "[10/33] Loss: 1.3160\n",
            "[11/33] Loss: 1.2539\n",
            "[12/33] Loss: 1.2610\n",
            "[13/33] Loss: 1.2138\n",
            "[14/33] Loss: 1.5402\n",
            "[15/33] Loss: 1.2138\n",
            "[16/33] Loss: 1.1528\n",
            "[17/33] Loss: 1.1240\n",
            "[18/33] Loss: 1.4245\n",
            "[19/33] Loss: 1.4243\n",
            "[20/33] Loss: 1.1886\n",
            "[21/33] Loss: 1.2127\n",
            "[22/33] Loss: 1.1201\n",
            "[23/33] Loss: 1.1930\n",
            "[24/33] Loss: 1.3167\n",
            "[25/33] Loss: 1.2708\n",
            "[26/33] Loss: 1.2335\n",
            "[27/33] Loss: 1.3247\n",
            "[28/33] Loss: 0.9851\n",
            "[29/33] Loss: 1.0846\n",
            "[30/33] Loss: 1.2914\n",
            "[31/33] Loss: 1.2816\n",
            "[32/33] Loss: 1.5989\n",
            "[33/33] Loss: 1.3437\n",
            "Epoch: 8 | Source Accuracy: 0.9898, Target Accuracy: 0.4453, Loss: 1.2944\n",
            "Epoch 0009 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.1257\n",
            "[2/33] Loss: 1.3895\n",
            "[3/33] Loss: 1.4490\n",
            "[4/33] Loss: 1.8862\n",
            "[5/33] Loss: 0.8993\n",
            "[6/33] Loss: 1.3883\n",
            "[7/33] Loss: 1.1733\n",
            "[8/33] Loss: 1.3918\n",
            "[9/33] Loss: 1.8678\n",
            "[10/33] Loss: 1.3459\n",
            "[11/33] Loss: 0.8829\n",
            "[12/33] Loss: 1.5071\n",
            "[13/33] Loss: 1.9309\n",
            "[14/33] Loss: 1.4043\n",
            "[15/33] Loss: 1.3719\n",
            "[16/33] Loss: 1.3482\n",
            "[17/33] Loss: 1.0596\n",
            "[18/33] Loss: 2.0255\n",
            "[19/33] Loss: 1.3906\n",
            "[20/33] Loss: 1.8921\n",
            "[21/33] Loss: 1.4405\n",
            "[22/33] Loss: 1.3149\n",
            "[23/33] Loss: 1.5330\n",
            "[24/33] Loss: 1.4107\n",
            "[25/33] Loss: 1.3219\n",
            "[26/33] Loss: 1.3820\n",
            "[27/33] Loss: 1.3967\n",
            "[28/33] Loss: 1.3341\n",
            "[29/33] Loss: 0.9896\n",
            "[30/33] Loss: 1.4540\n",
            "[31/33] Loss: 1.8677\n",
            "[32/33] Loss: 1.3465\n",
            "[33/33] Loss: 1.3498\n",
            "Epoch: 9 | Source Accuracy: 0.9961, Target Accuracy: 0.4453, Loss: 1.4203\n",
            "Epoch 0010 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.9356\n",
            "[2/33] Loss: 1.2757\n",
            "[3/33] Loss: 1.3040\n",
            "[4/33] Loss: 1.5746\n",
            "[5/33] Loss: 1.3796\n",
            "[6/33] Loss: 1.1036\n",
            "[7/33] Loss: 1.3731\n",
            "[8/33] Loss: 1.2548\n",
            "[9/33] Loss: 1.3823\n",
            "[10/33] Loss: 0.9797\n",
            "[11/33] Loss: 1.3363\n",
            "[12/33] Loss: 1.1054\n",
            "[13/33] Loss: 1.3806\n",
            "[14/33] Loss: 1.7558\n",
            "[15/33] Loss: 1.8683\n",
            "[16/33] Loss: 1.0654\n",
            "[17/33] Loss: 1.3550\n",
            "[18/33] Loss: 1.6762\n",
            "[19/33] Loss: 1.3477\n",
            "[20/33] Loss: 1.2276\n",
            "[21/33] Loss: 1.7996\n",
            "[22/33] Loss: 1.0043\n",
            "[23/33] Loss: 1.4698\n",
            "[24/33] Loss: 1.7089\n",
            "[25/33] Loss: 1.4619\n",
            "[26/33] Loss: 1.2607\n",
            "[27/33] Loss: 1.3513\n",
            "[28/33] Loss: 1.0151\n",
            "[29/33] Loss: 1.3942\n",
            "[30/33] Loss: 1.1702\n",
            "[31/33] Loss: 1.3724\n",
            "[32/33] Loss: 1.2488\n",
            "[33/33] Loss: 1.2973\n",
            "Epoch: 10 | Source Accuracy: 0.9947, Target Accuracy: 0.4745, Loss: 1.3708\n",
            "Training logs saved to LOG_DANN_Pruned_pruned_model_0\n",
            "[(Inference || test loss: 3.4590585231781006] [accuracy_test: 45.16 %]\n",
            "Epoch 0001 / 0010\n",
            "============\n",
            "[1/33] Loss: 5.8480\n",
            "[2/33] Loss: 5.1227\n",
            "[3/33] Loss: 5.1897\n",
            "[4/33] Loss: 4.7960\n",
            "[5/33] Loss: 4.5001\n",
            "[6/33] Loss: 4.6021\n",
            "[7/33] Loss: 3.7847\n",
            "[8/33] Loss: 3.6022\n",
            "[9/33] Loss: 3.6579\n",
            "[10/33] Loss: 3.3229\n",
            "[11/33] Loss: 3.5784\n",
            "[12/33] Loss: 3.0645\n",
            "[13/33] Loss: 3.2532\n",
            "[14/33] Loss: 3.2534\n",
            "[15/33] Loss: 3.2587\n",
            "[16/33] Loss: 2.9842\n",
            "[17/33] Loss: 3.0928\n",
            "[18/33] Loss: 2.8479\n",
            "[19/33] Loss: 2.8772\n",
            "[20/33] Loss: 3.6809\n",
            "[21/33] Loss: 2.6694\n",
            "[22/33] Loss: 2.7906\n",
            "[23/33] Loss: 3.0743\n",
            "[24/33] Loss: 3.3695\n",
            "[25/33] Loss: 2.5821\n",
            "[26/33] Loss: 2.9605\n",
            "[27/33] Loss: 2.9357\n",
            "[28/33] Loss: 2.8747\n",
            "[29/33] Loss: 2.5683\n",
            "[30/33] Loss: 2.5099\n",
            "[31/33] Loss: 2.3951\n",
            "[32/33] Loss: 2.8080\n",
            "[33/33] Loss: 2.3257\n",
            "Epoch: 1 | Source Accuracy: 0.4970, Target Accuracy: 0.2641, Loss: 3.3994\n",
            "Epoch 0002 / 0010\n",
            "============\n",
            "[1/33] Loss: 2.1653\n",
            "[2/33] Loss: 2.0317\n",
            "[3/33] Loss: 2.0887\n",
            "[4/33] Loss: 2.2768\n",
            "[5/33] Loss: 2.2677\n",
            "[6/33] Loss: 2.2239\n",
            "[7/33] Loss: 1.7890\n",
            "[8/33] Loss: 1.6217\n",
            "[9/33] Loss: 2.2460\n",
            "[10/33] Loss: 1.8397\n",
            "[11/33] Loss: 1.8230\n",
            "[12/33] Loss: 1.9331\n",
            "[13/33] Loss: 2.2647\n",
            "[14/33] Loss: 1.7067\n",
            "[15/33] Loss: 1.8397\n",
            "[16/33] Loss: 1.7408\n",
            "[17/33] Loss: 2.0501\n",
            "[18/33] Loss: 1.7645\n",
            "[19/33] Loss: 1.7400\n",
            "[20/33] Loss: 2.6993\n",
            "[21/33] Loss: 2.4738\n",
            "[22/33] Loss: 2.4573\n",
            "[23/33] Loss: 1.8606\n",
            "[24/33] Loss: 2.2578\n",
            "[25/33] Loss: 1.8842\n",
            "[26/33] Loss: 1.7124\n",
            "[27/33] Loss: 1.8865\n",
            "[28/33] Loss: 2.2104\n",
            "[29/33] Loss: 2.4134\n",
            "[30/33] Loss: 1.4452\n",
            "[31/33] Loss: 1.9095\n",
            "[32/33] Loss: 1.7227\n",
            "[33/33] Loss: 2.2727\n",
            "Epoch: 2 | Source Accuracy: 0.7813, Target Accuracy: 0.3889, Loss: 2.0188\n",
            "Epoch 0003 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.7531\n",
            "[2/33] Loss: 1.7583\n",
            "[3/33] Loss: 1.8960\n",
            "[4/33] Loss: 1.9480\n",
            "[5/33] Loss: 1.3552\n",
            "[6/33] Loss: 1.4969\n",
            "[7/33] Loss: 1.7143\n",
            "[8/33] Loss: 1.6218\n",
            "[9/33] Loss: 1.7409\n",
            "[10/33] Loss: 1.8669\n",
            "[11/33] Loss: 1.8794\n",
            "[12/33] Loss: 1.7957\n",
            "[13/33] Loss: 1.7271\n",
            "[14/33] Loss: 2.1120\n",
            "[15/33] Loss: 1.8252\n",
            "[16/33] Loss: 1.5020\n",
            "[17/33] Loss: 2.0358\n",
            "[18/33] Loss: 2.3943\n",
            "[19/33] Loss: 1.7801\n",
            "[20/33] Loss: 2.0150\n",
            "[21/33] Loss: 1.8133\n",
            "[22/33] Loss: 1.9907\n",
            "[23/33] Loss: 1.5941\n",
            "[24/33] Loss: 1.8377\n",
            "[25/33] Loss: 1.9276\n",
            "[26/33] Loss: 1.9072\n",
            "[27/33] Loss: 1.8233\n",
            "[28/33] Loss: 1.7687\n",
            "[29/33] Loss: 2.0011\n",
            "[30/33] Loss: 1.8997\n",
            "[31/33] Loss: 2.0265\n",
            "[32/33] Loss: 1.9978\n",
            "[33/33] Loss: 2.0837\n",
            "Epoch: 3 | Source Accuracy: 0.8247, Target Accuracy: 0.4843, Loss: 1.8451\n",
            "Epoch 0004 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.7964\n",
            "[2/33] Loss: 1.7928\n",
            "[3/33] Loss: 1.8386\n",
            "[4/33] Loss: 1.6718\n",
            "[5/33] Loss: 1.6188\n",
            "[6/33] Loss: 1.7352\n",
            "[7/33] Loss: 1.9099\n",
            "[8/33] Loss: 1.8528\n",
            "[9/33] Loss: 1.7143\n",
            "[10/33] Loss: 2.0929\n",
            "[11/33] Loss: 1.6725\n",
            "[12/33] Loss: 1.9070\n",
            "[13/33] Loss: 1.8192\n",
            "[14/33] Loss: 1.8360\n",
            "[15/33] Loss: 1.4345\n",
            "[16/33] Loss: 1.4257\n",
            "[17/33] Loss: 1.7300\n",
            "[18/33] Loss: 1.8713\n",
            "[19/33] Loss: 1.7847\n",
            "[20/33] Loss: 1.7461\n",
            "[21/33] Loss: 1.6830\n",
            "[22/33] Loss: 1.7691\n",
            "[23/33] Loss: 1.9012\n",
            "[24/33] Loss: 1.7386\n",
            "[25/33] Loss: 2.2087\n",
            "[26/33] Loss: 1.8520\n",
            "[27/33] Loss: 2.2800\n",
            "[28/33] Loss: 1.9544\n",
            "[29/33] Loss: 2.1300\n",
            "[30/33] Loss: 1.8573\n",
            "[31/33] Loss: 2.2049\n",
            "[32/33] Loss: 2.1411\n",
            "[33/33] Loss: 1.5401\n",
            "Epoch: 4 | Source Accuracy: 0.8460, Target Accuracy: 0.4520, Loss: 1.8337\n",
            "Epoch 0005 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.9917\n",
            "[2/33] Loss: 1.3452\n",
            "[3/33] Loss: 2.3870\n",
            "[4/33] Loss: 2.1788\n",
            "[5/33] Loss: 1.7286\n",
            "[6/33] Loss: 1.7796\n",
            "[7/33] Loss: 1.6641\n",
            "[8/33] Loss: 1.6950\n",
            "[9/33] Loss: 1.5552\n",
            "[10/33] Loss: 2.3040\n",
            "[11/33] Loss: 1.9060\n",
            "[12/33] Loss: 1.7564\n",
            "[13/33] Loss: 1.6974\n",
            "[14/33] Loss: 1.9283\n",
            "[15/33] Loss: 2.0973\n",
            "[16/33] Loss: 1.5062\n",
            "[17/33] Loss: 1.7447\n",
            "[18/33] Loss: 2.4761\n",
            "[19/33] Loss: 2.0175\n",
            "[20/33] Loss: 2.7714\n",
            "[21/33] Loss: 1.9478\n",
            "[22/33] Loss: 1.7566\n",
            "[23/33] Loss: 2.0347\n",
            "[24/33] Loss: 2.0104\n",
            "[25/33] Loss: 2.0567\n",
            "[26/33] Loss: 1.6839\n",
            "[27/33] Loss: 1.8196\n",
            "[28/33] Loss: 1.8749\n",
            "[29/33] Loss: 1.9265\n",
            "[30/33] Loss: 2.4024\n",
            "[31/33] Loss: 1.4697\n",
            "[32/33] Loss: 1.6504\n",
            "[33/33] Loss: 1.7741\n",
            "Epoch: 5 | Source Accuracy: 0.8652, Target Accuracy: 0.4712, Loss: 1.9072\n",
            "Epoch 0006 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.4015\n",
            "[2/33] Loss: 1.3939\n",
            "[3/33] Loss: 1.4613\n",
            "[4/33] Loss: 1.4457\n",
            "[5/33] Loss: 1.5391\n",
            "[6/33] Loss: 2.0881\n",
            "[7/33] Loss: 1.6016\n",
            "[8/33] Loss: 1.3681\n",
            "[9/33] Loss: 1.7776\n",
            "[10/33] Loss: 1.8445\n",
            "[11/33] Loss: 1.7207\n",
            "[12/33] Loss: 1.3988\n",
            "[13/33] Loss: 1.6685\n",
            "[14/33] Loss: 1.8406\n",
            "[15/33] Loss: 1.3772\n",
            "[16/33] Loss: 1.7682\n",
            "[17/33] Loss: 1.9170\n",
            "[18/33] Loss: 1.6610\n",
            "[19/33] Loss: 1.9296\n",
            "[20/33] Loss: 2.1549\n",
            "[21/33] Loss: 1.7917\n",
            "[22/33] Loss: 1.6398\n",
            "[23/33] Loss: 1.8458\n",
            "[24/33] Loss: 1.9547\n",
            "[25/33] Loss: 2.0672\n",
            "[26/33] Loss: 2.1168\n",
            "[27/33] Loss: 1.9140\n",
            "[28/33] Loss: 2.0108\n",
            "[29/33] Loss: 2.5662\n",
            "[30/33] Loss: 1.8654\n",
            "[31/33] Loss: 1.7554\n",
            "[32/33] Loss: 2.2377\n",
            "[33/33] Loss: 1.4193\n",
            "Epoch: 6 | Source Accuracy: 0.9247, Target Accuracy: 0.4919, Loss: 1.7740\n",
            "Epoch 0007 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.7609\n",
            "[2/33] Loss: 1.7599\n",
            "[3/33] Loss: 1.8856\n",
            "[4/33] Loss: 1.8756\n",
            "[5/33] Loss: 1.7479\n",
            "[6/33] Loss: 1.8942\n",
            "[7/33] Loss: 1.7798\n",
            "[8/33] Loss: 1.6112\n",
            "[9/33] Loss: 1.4898\n",
            "[10/33] Loss: 1.7207\n",
            "[11/33] Loss: 1.4444\n",
            "[12/33] Loss: 1.6989\n",
            "[13/33] Loss: 1.5824\n",
            "[14/33] Loss: 1.5137\n",
            "[15/33] Loss: 1.4567\n",
            "[16/33] Loss: 1.5953\n",
            "[17/33] Loss: 1.8135\n",
            "[18/33] Loss: 1.5968\n",
            "[19/33] Loss: 1.3589\n",
            "[20/33] Loss: 2.0626\n",
            "[21/33] Loss: 2.0427\n",
            "[22/33] Loss: 1.4288\n",
            "[23/33] Loss: 1.5651\n",
            "[24/33] Loss: 1.4129\n",
            "[25/33] Loss: 1.6495\n",
            "[26/33] Loss: 1.7795\n",
            "[27/33] Loss: 1.1181\n",
            "[28/33] Loss: 1.5545\n",
            "[29/33] Loss: 1.3648\n",
            "[30/33] Loss: 1.7079\n",
            "[31/33] Loss: 1.9879\n",
            "[32/33] Loss: 1.4940\n",
            "[33/33] Loss: 1.6924\n",
            "Epoch: 7 | Source Accuracy: 0.9510, Target Accuracy: 0.4823, Loss: 1.6499\n",
            "Epoch 0008 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.3473\n",
            "[2/33] Loss: 1.7763\n",
            "[3/33] Loss: 1.6180\n",
            "[4/33] Loss: 1.6773\n",
            "[5/33] Loss: 1.2244\n",
            "[6/33] Loss: 1.8405\n",
            "[7/33] Loss: 1.6736\n",
            "[8/33] Loss: 1.4937\n",
            "[9/33] Loss: 1.6701\n",
            "[10/33] Loss: 1.5770\n",
            "[11/33] Loss: 1.4999\n",
            "[12/33] Loss: 1.7417\n",
            "[13/33] Loss: 1.6912\n",
            "[14/33] Loss: 1.6051\n",
            "[15/33] Loss: 1.3031\n",
            "[16/33] Loss: 1.5832\n",
            "[17/33] Loss: 1.5582\n",
            "[18/33] Loss: 1.4610\n",
            "[19/33] Loss: 1.2745\n",
            "[20/33] Loss: 1.4994\n",
            "[21/33] Loss: 1.5927\n",
            "[22/33] Loss: 1.6140\n",
            "[23/33] Loss: 1.5909\n",
            "[24/33] Loss: 1.4659\n",
            "[25/33] Loss: 1.4108\n",
            "[26/33] Loss: 1.3685\n",
            "[27/33] Loss: 1.5170\n",
            "[28/33] Loss: 1.5327\n",
            "[29/33] Loss: 1.4108\n",
            "[30/33] Loss: 1.7362\n",
            "[31/33] Loss: 1.4967\n",
            "[32/33] Loss: 1.3874\n",
            "[33/33] Loss: 1.3545\n",
            "Epoch: 8 | Source Accuracy: 0.9652, Target Accuracy: 0.4697, Loss: 1.5331\n",
            "Epoch 0009 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.4497\n",
            "[2/33] Loss: 1.7181\n",
            "[3/33] Loss: 1.1477\n",
            "[4/33] Loss: 1.2117\n",
            "[5/33] Loss: 1.7346\n",
            "[6/33] Loss: 1.2629\n",
            "[7/33] Loss: 1.5357\n",
            "[8/33] Loss: 1.4946\n",
            "[9/33] Loss: 1.3196\n",
            "[10/33] Loss: 1.4463\n",
            "[11/33] Loss: 1.7293\n",
            "[12/33] Loss: 1.6653\n",
            "[13/33] Loss: 1.1405\n",
            "[14/33] Loss: 1.1911\n",
            "[15/33] Loss: 1.6620\n",
            "[16/33] Loss: 1.3742\n",
            "[17/33] Loss: 1.4686\n",
            "[18/33] Loss: 1.5067\n",
            "[19/33] Loss: 1.1560\n",
            "[20/33] Loss: 1.6753\n",
            "[21/33] Loss: 1.7240\n",
            "[22/33] Loss: 1.6201\n",
            "[23/33] Loss: 1.2662\n",
            "[24/33] Loss: 1.2382\n",
            "[25/33] Loss: 1.8304\n",
            "[26/33] Loss: 1.3218\n",
            "[27/33] Loss: 1.4500\n",
            "[28/33] Loss: 1.3450\n",
            "[29/33] Loss: 1.4393\n",
            "[30/33] Loss: 1.7951\n",
            "[31/33] Loss: 1.5833\n",
            "[32/33] Loss: 1.7002\n",
            "[33/33] Loss: 1.1387\n",
            "Epoch: 9 | Source Accuracy: 0.9848, Target Accuracy: 0.4581, Loss: 1.4649\n",
            "Epoch 0010 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.3731\n",
            "[2/33] Loss: 1.2930\n",
            "[3/33] Loss: 1.4231\n",
            "[4/33] Loss: 1.5913\n",
            "[5/33] Loss: 1.1021\n",
            "[6/33] Loss: 1.5270\n",
            "[7/33] Loss: 1.4604\n",
            "[8/33] Loss: 1.5366\n",
            "[9/33] Loss: 1.3671\n",
            "[10/33] Loss: 1.3858\n",
            "[11/33] Loss: 1.5046\n",
            "[12/33] Loss: 1.5966\n",
            "[13/33] Loss: 1.5887\n",
            "[14/33] Loss: 1.3743\n",
            "[15/33] Loss: 1.4472\n",
            "[16/33] Loss: 1.4247\n",
            "[17/33] Loss: 1.5045\n",
            "[18/33] Loss: 1.6106\n",
            "[19/33] Loss: 1.5131\n",
            "[20/33] Loss: 1.2790\n",
            "[21/33] Loss: 1.4581\n",
            "[22/33] Loss: 1.6046\n",
            "[23/33] Loss: 1.6748\n",
            "[24/33] Loss: 1.4466\n",
            "[25/33] Loss: 1.6298\n",
            "[26/33] Loss: 1.6768\n",
            "[27/33] Loss: 1.6425\n",
            "[28/33] Loss: 1.3229\n",
            "[29/33] Loss: 1.6362\n",
            "[30/33] Loss: 1.4889\n",
            "[31/33] Loss: 1.5034\n",
            "[32/33] Loss: 1.5051\n",
            "[33/33] Loss: 1.3384\n",
            "Epoch: 10 | Source Accuracy: 0.9874, Target Accuracy: 0.4793, Loss: 1.4797\n",
            "Training logs saved to LOG_DANN_Pruned_pruned_model_1\n",
            "[(Inference || test loss: 3.3481290340423584] [accuracy_test: 43.59 %]\n",
            "Epoch 0001 / 0010\n",
            "============\n",
            "[1/33] Loss: 4.9531\n",
            "[2/33] Loss: 4.2410\n",
            "[3/33] Loss: 3.9658\n",
            "[4/33] Loss: 3.5935\n",
            "[5/33] Loss: 3.5748\n",
            "[6/33] Loss: 3.4904\n",
            "[7/33] Loss: 3.0197\n",
            "[8/33] Loss: 2.8517\n",
            "[9/33] Loss: 2.7673\n",
            "[10/33] Loss: 2.7463\n",
            "[11/33] Loss: 2.7817\n",
            "[12/33] Loss: 2.9056\n",
            "[13/33] Loss: 2.6627\n",
            "[14/33] Loss: 2.6538\n",
            "[15/33] Loss: 2.5583\n",
            "[16/33] Loss: 2.3510\n",
            "[17/33] Loss: 2.3929\n",
            "[18/33] Loss: 2.3542\n",
            "[19/33] Loss: 2.1703\n",
            "[20/33] Loss: 1.9658\n",
            "[21/33] Loss: 1.8961\n",
            "[22/33] Loss: 1.6054\n",
            "[23/33] Loss: 1.5910\n",
            "[24/33] Loss: 1.6575\n",
            "[25/33] Loss: 1.5950\n",
            "[26/33] Loss: 1.5964\n",
            "[27/33] Loss: 1.8186\n",
            "[28/33] Loss: 1.5886\n",
            "[29/33] Loss: 1.7505\n",
            "[30/33] Loss: 1.6002\n",
            "[31/33] Loss: 1.8024\n",
            "[32/33] Loss: 1.6345\n",
            "[33/33] Loss: 1.7710\n",
            "Epoch: 1 | Source Accuracy: 0.7404, Target Accuracy: 0.1653, Loss: 2.4820\n",
            "Epoch 0002 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.6920\n",
            "[2/33] Loss: 1.7179\n",
            "[3/33] Loss: 1.5533\n",
            "[4/33] Loss: 1.8089\n",
            "[5/33] Loss: 1.9841\n",
            "[6/33] Loss: 2.1932\n",
            "[7/33] Loss: 2.4855\n",
            "[8/33] Loss: 2.5210\n",
            "[9/33] Loss: 2.3225\n",
            "[10/33] Loss: 2.4125\n",
            "[11/33] Loss: 2.3462\n",
            "[12/33] Loss: 2.3948\n",
            "[13/33] Loss: 2.5094\n",
            "[14/33] Loss: 2.4991\n",
            "[15/33] Loss: 2.4144\n",
            "[16/33] Loss: 2.4405\n",
            "[17/33] Loss: 2.2720\n",
            "[18/33] Loss: 2.4652\n",
            "[19/33] Loss: 2.6109\n",
            "[20/33] Loss: 2.6226\n",
            "[21/33] Loss: 2.7385\n",
            "[22/33] Loss: 2.9330\n",
            "[23/33] Loss: 3.1309\n",
            "[24/33] Loss: 3.3027\n",
            "[25/33] Loss: 3.4498\n",
            "[26/33] Loss: 3.3020\n",
            "[27/33] Loss: 3.0288\n",
            "[28/33] Loss: 2.5550\n",
            "[29/33] Loss: 2.2529\n",
            "[30/33] Loss: 2.3901\n",
            "[31/33] Loss: 2.5708\n",
            "[32/33] Loss: 2.5777\n",
            "[33/33] Loss: 2.5339\n",
            "Epoch: 2 | Source Accuracy: 0.9679, Target Accuracy: 0.1726, Loss: 2.4858\n",
            "Epoch 0003 / 0010\n",
            "============\n",
            "[1/33] Loss: 2.4773\n",
            "[2/33] Loss: 2.3992\n",
            "[3/33] Loss: 2.4240\n",
            "[4/33] Loss: 2.3958\n",
            "[5/33] Loss: 2.2259\n",
            "[6/33] Loss: 2.2505\n",
            "[7/33] Loss: 1.8466\n",
            "[8/33] Loss: 1.9537\n",
            "[9/33] Loss: 1.8964\n",
            "[10/33] Loss: 1.8941\n",
            "[11/33] Loss: 1.6678\n",
            "[12/33] Loss: 2.1403\n",
            "[13/33] Loss: 1.9153\n",
            "[14/33] Loss: 1.8638\n",
            "[15/33] Loss: 1.9074\n",
            "[16/33] Loss: 1.7973\n",
            "[17/33] Loss: 1.8386\n",
            "[18/33] Loss: 1.7748\n",
            "[19/33] Loss: 1.8740\n",
            "[20/33] Loss: 2.0333\n",
            "[21/33] Loss: 1.7417\n",
            "[22/33] Loss: 1.6751\n",
            "[23/33] Loss: 1.7884\n",
            "[24/33] Loss: 1.6143\n",
            "[25/33] Loss: 1.6105\n",
            "[26/33] Loss: 1.6953\n",
            "[27/33] Loss: 1.4576\n",
            "[28/33] Loss: 1.4259\n",
            "[29/33] Loss: 1.3505\n",
            "[30/33] Loss: 1.5279\n",
            "[31/33] Loss: 1.4458\n",
            "[32/33] Loss: 1.2735\n",
            "[33/33] Loss: 1.3209\n",
            "Epoch: 3 | Source Accuracy: 0.9387, Target Accuracy: 0.1988, Loss: 1.8335\n",
            "Epoch 0004 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.3626\n",
            "[2/33] Loss: 1.3026\n",
            "[3/33] Loss: 1.3916\n",
            "[4/33] Loss: 1.3180\n",
            "[5/33] Loss: 1.3835\n",
            "[6/33] Loss: 1.3855\n",
            "[7/33] Loss: 1.3118\n",
            "[8/33] Loss: 1.3668\n",
            "[9/33] Loss: 1.3265\n",
            "[10/33] Loss: 1.2990\n",
            "[11/33] Loss: 1.4697\n",
            "[12/33] Loss: 1.4273\n",
            "[13/33] Loss: 1.3369\n",
            "[14/33] Loss: 1.4183\n",
            "[15/33] Loss: 1.4066\n",
            "[16/33] Loss: 1.2729\n",
            "[17/33] Loss: 1.3705\n",
            "[18/33] Loss: 1.4197\n",
            "[19/33] Loss: 1.3849\n",
            "[20/33] Loss: 1.3262\n",
            "[21/33] Loss: 1.3822\n",
            "[22/33] Loss: 1.4083\n",
            "[23/33] Loss: 1.3728\n",
            "[24/33] Loss: 1.3296\n",
            "[25/33] Loss: 1.3176\n",
            "[26/33] Loss: 1.3750\n",
            "[27/33] Loss: 1.2560\n",
            "[28/33] Loss: 1.2846\n",
            "[29/33] Loss: 1.3002\n",
            "[30/33] Loss: 1.3343\n",
            "[31/33] Loss: 1.2945\n",
            "[32/33] Loss: 1.2475\n",
            "[33/33] Loss: 1.2340\n",
            "Epoch: 4 | Source Accuracy: 0.9878, Target Accuracy: 0.2168, Loss: 1.3460\n",
            "Epoch 0005 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.2636\n",
            "[2/33] Loss: 1.2629\n",
            "[3/33] Loss: 1.2627\n",
            "[4/33] Loss: 1.2405\n",
            "[5/33] Loss: 1.3200\n",
            "[6/33] Loss: 1.2396\n",
            "[7/33] Loss: 1.2353\n",
            "[8/33] Loss: 1.3102\n",
            "[9/33] Loss: 1.3338\n",
            "[10/33] Loss: 1.4643\n",
            "[11/33] Loss: 1.3855\n",
            "[12/33] Loss: 1.3467\n",
            "[13/33] Loss: 1.3388\n",
            "[14/33] Loss: 1.3738\n",
            "[15/33] Loss: 1.4667\n",
            "[16/33] Loss: 1.3794\n",
            "[17/33] Loss: 1.4491\n",
            "[18/33] Loss: 1.5471\n",
            "[19/33] Loss: 1.4464\n",
            "[20/33] Loss: 1.4629\n",
            "[21/33] Loss: 1.4551\n",
            "[22/33] Loss: 1.5520\n",
            "[23/33] Loss: 1.5208\n",
            "[24/33] Loss: 1.4948\n",
            "[25/33] Loss: 1.5256\n",
            "[26/33] Loss: 1.4333\n",
            "[27/33] Loss: 1.4377\n",
            "[28/33] Loss: 1.4740\n",
            "[29/33] Loss: 1.4657\n",
            "[30/33] Loss: 1.2894\n",
            "[31/33] Loss: 1.2614\n",
            "[32/33] Loss: 1.2866\n",
            "[33/33] Loss: 1.1583\n",
            "Epoch: 5 | Source Accuracy: 0.9874, Target Accuracy: 0.2086, Loss: 1.3783\n",
            "Epoch 0006 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.0897\n",
            "[2/33] Loss: 1.2315\n",
            "[3/33] Loss: 1.1886\n",
            "[4/33] Loss: 1.2387\n",
            "[5/33] Loss: 1.3195\n",
            "[6/33] Loss: 1.4231\n",
            "[7/33] Loss: 1.2359\n",
            "[8/33] Loss: 1.2682\n",
            "[9/33] Loss: 1.2631\n",
            "[10/33] Loss: 1.3254\n",
            "[11/33] Loss: 1.3535\n",
            "[12/33] Loss: 1.3748\n",
            "[13/33] Loss: 1.3390\n",
            "[14/33] Loss: 1.3230\n",
            "[15/33] Loss: 1.3192\n",
            "[16/33] Loss: 1.3697\n",
            "[17/33] Loss: 1.3796\n",
            "[18/33] Loss: 1.4580\n",
            "[19/33] Loss: 1.3219\n",
            "[20/33] Loss: 1.3912\n",
            "[21/33] Loss: 1.3582\n",
            "[22/33] Loss: 1.3548\n",
            "[23/33] Loss: 1.3969\n",
            "[24/33] Loss: 1.4041\n",
            "[25/33] Loss: 1.3570\n",
            "[26/33] Loss: 1.3982\n",
            "[27/33] Loss: 1.3923\n",
            "[28/33] Loss: 1.3736\n",
            "[29/33] Loss: 1.3271\n",
            "[30/33] Loss: 1.4403\n",
            "[31/33] Loss: 1.3773\n",
            "[32/33] Loss: 1.4367\n",
            "[33/33] Loss: 1.3821\n",
            "Epoch: 6 | Source Accuracy: 0.9893, Target Accuracy: 0.2100, Loss: 1.3398\n",
            "Epoch 0007 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.3248\n",
            "[2/33] Loss: 1.3382\n",
            "[3/33] Loss: 1.3827\n",
            "[4/33] Loss: 1.3099\n",
            "[5/33] Loss: 1.3231\n",
            "[6/33] Loss: 1.3860\n",
            "[7/33] Loss: 1.3968\n",
            "[8/33] Loss: 1.2875\n",
            "[9/33] Loss: 1.1942\n",
            "[10/33] Loss: 1.2235\n",
            "[11/33] Loss: 1.1850\n",
            "[12/33] Loss: 1.2536\n",
            "[13/33] Loss: 1.2493\n",
            "[14/33] Loss: 1.2199\n",
            "[15/33] Loss: 1.2655\n",
            "[16/33] Loss: 1.2305\n",
            "[17/33] Loss: 1.2272\n",
            "[18/33] Loss: 1.2474\n",
            "[19/33] Loss: 1.3042\n",
            "[20/33] Loss: 1.2400\n",
            "[21/33] Loss: 1.2813\n",
            "[22/33] Loss: 1.2208\n",
            "[23/33] Loss: 1.3357\n",
            "[24/33] Loss: 1.3704\n",
            "[25/33] Loss: 1.2524\n",
            "[26/33] Loss: 1.2767\n",
            "[27/33] Loss: 1.3256\n",
            "[28/33] Loss: 1.3427\n",
            "[29/33] Loss: 1.4101\n",
            "[30/33] Loss: 1.3972\n",
            "[31/33] Loss: 1.4177\n",
            "[32/33] Loss: 1.2984\n",
            "[33/33] Loss: 1.3699\n",
            "Epoch: 7 | Source Accuracy: 0.9971, Target Accuracy: 0.2110, Loss: 1.2996\n",
            "Epoch 0008 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.4418\n",
            "[2/33] Loss: 1.3645\n",
            "[3/33] Loss: 1.4948\n",
            "[4/33] Loss: 1.4094\n",
            "[5/33] Loss: 1.4742\n",
            "[6/33] Loss: 1.3821\n",
            "[7/33] Loss: 1.4235\n",
            "[8/33] Loss: 1.4194\n",
            "[9/33] Loss: 1.3531\n",
            "[10/33] Loss: 1.3322\n",
            "[11/33] Loss: 1.3648\n",
            "[12/33] Loss: 1.3250\n",
            "[13/33] Loss: 1.4582\n",
            "[14/33] Loss: 1.2989\n",
            "[15/33] Loss: 1.3330\n",
            "[16/33] Loss: 1.3333\n",
            "[17/33] Loss: 1.3347\n",
            "[18/33] Loss: 1.2663\n",
            "[19/33] Loss: 1.3237\n",
            "[20/33] Loss: 1.3635\n",
            "[21/33] Loss: 1.3705\n",
            "[22/33] Loss: 1.3457\n",
            "[23/33] Loss: 1.2906\n",
            "[24/33] Loss: 1.3111\n",
            "[25/33] Loss: 1.3360\n",
            "[26/33] Loss: 1.3497\n",
            "[27/33] Loss: 1.3015\n",
            "[28/33] Loss: 1.3098\n",
            "[29/33] Loss: 1.2783\n",
            "[30/33] Loss: 1.2582\n",
            "[31/33] Loss: 1.2924\n",
            "[32/33] Loss: 1.2895\n",
            "[33/33] Loss: 1.3177\n",
            "Epoch: 8 | Source Accuracy: 0.9956, Target Accuracy: 0.2397, Loss: 1.3499\n",
            "Epoch 0009 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.3415\n",
            "[2/33] Loss: 1.3111\n",
            "[3/33] Loss: 1.2984\n",
            "[4/33] Loss: 1.3537\n",
            "[5/33] Loss: 1.3172\n",
            "[6/33] Loss: 1.3046\n",
            "[7/33] Loss: 1.2678\n",
            "[8/33] Loss: 1.2794\n",
            "[9/33] Loss: 1.3011\n",
            "[10/33] Loss: 1.3143\n",
            "[11/33] Loss: 1.2510\n",
            "[12/33] Loss: 1.3161\n",
            "[13/33] Loss: 1.1977\n",
            "[14/33] Loss: 1.2921\n",
            "[15/33] Loss: 1.2705\n",
            "[16/33] Loss: 1.2948\n",
            "[17/33] Loss: 1.2355\n",
            "[18/33] Loss: 1.2579\n",
            "[19/33] Loss: 1.2260\n",
            "[20/33] Loss: 1.2530\n",
            "[21/33] Loss: 1.2735\n",
            "[22/33] Loss: 1.2583\n",
            "[23/33] Loss: 1.2320\n",
            "[24/33] Loss: 1.2068\n",
            "[25/33] Loss: 1.2511\n",
            "[26/33] Loss: 1.2331\n",
            "[27/33] Loss: 1.3220\n",
            "[28/33] Loss: 1.2771\n",
            "[29/33] Loss: 1.2888\n",
            "[30/33] Loss: 1.2635\n",
            "[31/33] Loss: 1.2459\n",
            "[32/33] Loss: 1.1791\n",
            "[33/33] Loss: 1.3253\n",
            "Epoch: 9 | Source Accuracy: 0.9990, Target Accuracy: 0.2523, Loss: 1.2739\n",
            "Epoch 0010 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.3130\n",
            "[2/33] Loss: 1.2656\n",
            "[3/33] Loss: 1.2508\n",
            "[4/33] Loss: 1.2317\n",
            "[5/33] Loss: 1.3795\n",
            "[6/33] Loss: 1.3297\n",
            "[7/33] Loss: 1.3055\n",
            "[8/33] Loss: 1.2160\n",
            "[9/33] Loss: 1.2952\n",
            "[10/33] Loss: 1.2720\n",
            "[11/33] Loss: 1.3192\n",
            "[12/33] Loss: 1.3128\n",
            "[13/33] Loss: 1.2831\n",
            "[14/33] Loss: 1.2966\n",
            "[15/33] Loss: 1.2595\n",
            "[16/33] Loss: 1.2855\n",
            "[17/33] Loss: 1.2877\n",
            "[18/33] Loss: 1.2957\n",
            "[19/33] Loss: 1.2377\n",
            "[20/33] Loss: 1.2250\n",
            "[21/33] Loss: 1.2304\n",
            "[22/33] Loss: 1.2590\n",
            "[23/33] Loss: 1.2489\n",
            "[24/33] Loss: 1.3530\n",
            "[25/33] Loss: 1.3484\n",
            "[26/33] Loss: 1.2924\n",
            "[27/33] Loss: 1.2477\n",
            "[28/33] Loss: 1.2469\n",
            "[29/33] Loss: 1.3006\n",
            "[30/33] Loss: 1.2933\n",
            "[31/33] Loss: 1.2861\n",
            "[32/33] Loss: 1.2793\n",
            "[33/33] Loss: 1.2312\n",
            "Epoch: 10 | Source Accuracy: 0.9976, Target Accuracy: 0.2528, Loss: 1.2812\n",
            "Training logs saved to LOG_DANN_Pruned_pruned_model_2\n",
            "[(Inference || test loss: 6.64578914642334] [accuracy_test: 26.47 %]\n",
            "Epoch 0001 / 0010\n",
            "============\n",
            "[1/10] Loss: 3.2892\n",
            "[2/10] Loss: 3.2455\n",
            "[3/10] Loss: 3.4749\n",
            "[4/10] Loss: 2.6338\n",
            "[5/10] Loss: 3.0314\n",
            "[6/10] Loss: 2.8613\n",
            "[7/10] Loss: 1.8809\n",
            "[8/10] Loss: 1.9761\n",
            "[9/10] Loss: 1.9567\n",
            "[10/10] Loss: 1.7887\n",
            "Epoch: 1 | Source Accuracy: 0.6632, Target Accuracy: 0.4838, Loss: 2.6138\n",
            "Epoch 0002 / 0010\n",
            "============\n",
            "[1/10] Loss: 1.6313\n",
            "[2/10] Loss: 1.5236\n",
            "[3/10] Loss: 1.5258\n",
            "[4/10] Loss: 1.6535\n",
            "[5/10] Loss: 1.6055\n",
            "[6/10] Loss: 1.6140\n",
            "[7/10] Loss: 1.4703\n",
            "[8/10] Loss: 1.4167\n",
            "[9/10] Loss: 1.4560\n",
            "[10/10] Loss: 1.3839\n",
            "Epoch: 2 | Source Accuracy: 0.9521, Target Accuracy: 0.7419, Loss: 1.5281\n",
            "Epoch 0003 / 0010\n",
            "============\n",
            "[1/10] Loss: 1.2923\n",
            "[2/10] Loss: 1.4934\n",
            "[3/10] Loss: 1.5070\n",
            "[4/10] Loss: 1.4407\n",
            "[5/10] Loss: 1.3945\n",
            "[6/10] Loss: 1.6174\n",
            "[7/10] Loss: 1.3086\n",
            "[8/10] Loss: 1.3649\n",
            "[9/10] Loss: 1.3550\n",
            "[10/10] Loss: 1.3978\n",
            "Epoch: 3 | Source Accuracy: 0.9709, Target Accuracy: 0.7299, Loss: 1.4172\n",
            "Epoch 0004 / 0010\n",
            "============\n",
            "[1/10] Loss: 1.4576\n",
            "[2/10] Loss: 1.2574\n",
            "[3/10] Loss: 1.5002\n",
            "[4/10] Loss: 1.3774\n",
            "[5/10] Loss: 1.4569\n",
            "[6/10] Loss: 1.5110\n",
            "[7/10] Loss: 1.2773\n",
            "[8/10] Loss: 1.1804\n",
            "[9/10] Loss: 1.4326\n",
            "[10/10] Loss: 1.4001\n",
            "Epoch: 4 | Source Accuracy: 0.9829, Target Accuracy: 0.7436, Loss: 1.3851\n",
            "Epoch 0005 / 0010\n",
            "============\n",
            "[1/10] Loss: 1.4515\n",
            "[2/10] Loss: 1.4741\n",
            "[3/10] Loss: 1.4043\n",
            "[4/10] Loss: 1.4789\n",
            "[5/10] Loss: 1.4926\n",
            "[6/10] Loss: 1.5402\n",
            "[7/10] Loss: 1.5442\n",
            "[8/10] Loss: 1.4584\n",
            "[9/10] Loss: 1.2737\n",
            "[10/10] Loss: 1.5062\n",
            "Epoch: 5 | Source Accuracy: 0.9795, Target Accuracy: 0.7761, Loss: 1.4624\n",
            "Epoch 0006 / 0010\n",
            "============\n",
            "[1/10] Loss: 1.6051\n",
            "[2/10] Loss: 1.4314\n",
            "[3/10] Loss: 1.5534\n",
            "[4/10] Loss: 1.5832\n",
            "[5/10] Loss: 1.7125\n",
            "[6/10] Loss: 1.5036\n",
            "[7/10] Loss: 1.6899\n",
            "[8/10] Loss: 1.5938\n",
            "[9/10] Loss: 1.6269\n",
            "[10/10] Loss: 1.5969\n",
            "Epoch: 6 | Source Accuracy: 0.9897, Target Accuracy: 0.6906, Loss: 1.5897\n",
            "Epoch 0007 / 0010\n",
            "============\n",
            "[1/10] Loss: 1.5545\n",
            "[2/10] Loss: 1.6581\n",
            "[3/10] Loss: 1.5738\n",
            "[4/10] Loss: 1.6182\n",
            "[5/10] Loss: 1.5565\n",
            "[6/10] Loss: 1.7326\n",
            "[7/10] Loss: 1.4608\n",
            "[8/10] Loss: 1.4606\n",
            "[9/10] Loss: 1.4299\n",
            "[10/10] Loss: 1.3591\n",
            "Epoch: 7 | Source Accuracy: 0.9795, Target Accuracy: 0.6342, Loss: 1.5404\n",
            "Epoch 0008 / 0010\n",
            "============\n",
            "[1/10] Loss: 1.4169\n",
            "[2/10] Loss: 1.3861\n",
            "[3/10] Loss: 1.4608\n",
            "[4/10] Loss: 1.3021\n",
            "[5/10] Loss: 1.3031\n",
            "[6/10] Loss: 1.3194\n",
            "[7/10] Loss: 1.3051\n",
            "[8/10] Loss: 1.2766\n",
            "[9/10] Loss: 1.2967\n",
            "[10/10] Loss: 1.2287\n",
            "Epoch: 8 | Source Accuracy: 0.9983, Target Accuracy: 0.6085, Loss: 1.3296\n",
            "Epoch 0009 / 0010\n",
            "============\n",
            "[1/10] Loss: 1.3130\n",
            "[2/10] Loss: 1.1980\n",
            "[3/10] Loss: 1.2250\n",
            "[4/10] Loss: 1.3560\n",
            "[5/10] Loss: 1.3236\n",
            "[6/10] Loss: 1.3725\n",
            "[7/10] Loss: 1.3286\n",
            "[8/10] Loss: 1.3178\n",
            "[9/10] Loss: 1.3019\n",
            "[10/10] Loss: 1.5900\n",
            "Epoch: 9 | Source Accuracy: 0.9932, Target Accuracy: 0.6291, Loss: 1.3326\n",
            "Epoch 0010 / 0010\n",
            "============\n",
            "[1/10] Loss: 1.2739\n",
            "[2/10] Loss: 1.2058\n",
            "[3/10] Loss: 1.4369\n",
            "[4/10] Loss: 1.3559\n",
            "[5/10] Loss: 1.3319\n",
            "[6/10] Loss: 1.2777\n",
            "[7/10] Loss: 1.3282\n",
            "[8/10] Loss: 1.1975\n",
            "[9/10] Loss: 1.3988\n",
            "[10/10] Loss: 1.6791\n",
            "Epoch: 10 | Source Accuracy: 0.9932, Target Accuracy: 0.6188, Loss: 1.3486\n",
            "Training logs saved to LOG_DANN_Pruned_pruned_model_3\n",
            "[(Inference || test loss: 6.318338394165039] [accuracy_test: 1.72 %]\n",
            "Epoch 0001 / 0010\n",
            "============\n",
            "[1/33] Loss: 4.2835\n",
            "[2/33] Loss: 4.0350\n",
            "[3/33] Loss: 3.5730\n",
            "[4/33] Loss: 3.3944\n",
            "[5/33] Loss: 3.4645\n",
            "[6/33] Loss: 3.2696\n",
            "[7/33] Loss: 3.2241\n",
            "[8/33] Loss: 3.2462\n",
            "[9/33] Loss: 3.0179\n",
            "[10/33] Loss: 3.0596\n",
            "[11/33] Loss: 2.6859\n",
            "[12/33] Loss: 2.7479\n",
            "[13/33] Loss: 2.6818\n",
            "[14/33] Loss: 2.3906\n",
            "[15/33] Loss: 2.3134\n",
            "[16/33] Loss: 2.5460\n",
            "[17/33] Loss: 2.4182\n",
            "[18/33] Loss: 2.2170\n",
            "[19/33] Loss: 2.1862\n",
            "[20/33] Loss: 1.8490\n",
            "[21/33] Loss: 1.8589\n",
            "[22/33] Loss: 2.1628\n",
            "[23/33] Loss: 2.0758\n",
            "[24/33] Loss: 1.9350\n",
            "[25/33] Loss: 1.8592\n",
            "[26/33] Loss: 1.8750\n",
            "[27/33] Loss: 1.8490\n",
            "[28/33] Loss: 1.8906\n",
            "[29/33] Loss: 1.7017\n",
            "[30/33] Loss: 1.3711\n",
            "[31/33] Loss: 1.5719\n",
            "[32/33] Loss: 1.7425\n",
            "[33/33] Loss: 1.6901\n",
            "Epoch: 1 | Source Accuracy: 0.7177, Target Accuracy: 0.1828, Loss: 2.4905\n",
            "Epoch 0002 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.5398\n",
            "[2/33] Loss: 1.4051\n",
            "[3/33] Loss: 1.4490\n",
            "[4/33] Loss: 1.5567\n",
            "[5/33] Loss: 1.6468\n",
            "[6/33] Loss: 1.4614\n",
            "[7/33] Loss: 1.4602\n",
            "[8/33] Loss: 1.5262\n",
            "[9/33] Loss: 1.5186\n",
            "[10/33] Loss: 1.3288\n",
            "[11/33] Loss: 1.3462\n",
            "[12/33] Loss: 1.4401\n",
            "[13/33] Loss: 1.5573\n",
            "[14/33] Loss: 1.4505\n",
            "[15/33] Loss: 1.5840\n",
            "[16/33] Loss: 1.4936\n",
            "[17/33] Loss: 1.5244\n",
            "[18/33] Loss: 1.7324\n",
            "[19/33] Loss: 1.6927\n",
            "[20/33] Loss: 1.7776\n",
            "[21/33] Loss: 1.6704\n",
            "[22/33] Loss: 1.8825\n",
            "[23/33] Loss: 1.9457\n",
            "[24/33] Loss: 2.0508\n",
            "[25/33] Loss: 2.2549\n",
            "[26/33] Loss: 2.1419\n",
            "[27/33] Loss: 2.2326\n",
            "[28/33] Loss: 2.2298\n",
            "[29/33] Loss: 2.3136\n",
            "[30/33] Loss: 2.4582\n",
            "[31/33] Loss: 2.3388\n",
            "[32/33] Loss: 2.5725\n",
            "[33/33] Loss: 2.0109\n",
            "Epoch: 2 | Source Accuracy: 0.9530, Target Accuracy: 0.2111, Loss: 1.7756\n",
            "Epoch 0003 / 0010\n",
            "============\n",
            "[1/33] Loss: 2.1266\n",
            "[2/33] Loss: 2.0468\n",
            "[3/33] Loss: 1.9812\n",
            "[4/33] Loss: 2.3367\n",
            "[5/33] Loss: 2.5486\n",
            "[6/33] Loss: 2.5528\n",
            "[7/33] Loss: 2.1610\n",
            "[8/33] Loss: 2.1658\n",
            "[9/33] Loss: 2.1607\n",
            "[10/33] Loss: 2.1126\n",
            "[11/33] Loss: 2.3644\n",
            "[12/33] Loss: 2.2673\n",
            "[13/33] Loss: 2.0974\n",
            "[14/33] Loss: 1.7664\n",
            "[15/33] Loss: 2.0110\n",
            "[16/33] Loss: 1.8924\n",
            "[17/33] Loss: 1.9460\n",
            "[18/33] Loss: 2.1049\n",
            "[19/33] Loss: 1.9966\n",
            "[20/33] Loss: 1.8294\n",
            "[21/33] Loss: 1.9296\n",
            "[22/33] Loss: 1.5772\n",
            "[23/33] Loss: 1.5336\n",
            "[24/33] Loss: 1.6393\n",
            "[25/33] Loss: 1.8645\n",
            "[26/33] Loss: 1.6422\n",
            "[27/33] Loss: 1.5271\n",
            "[28/33] Loss: 1.6682\n",
            "[29/33] Loss: 1.5744\n",
            "[30/33] Loss: 1.3686\n",
            "[31/33] Loss: 1.3483\n",
            "[32/33] Loss: 1.5161\n",
            "[33/33] Loss: 1.5449\n",
            "Epoch: 3 | Source Accuracy: 0.9268, Target Accuracy: 0.2045, Loss: 1.9152\n",
            "Epoch 0004 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.5382\n",
            "[2/33] Loss: 1.4686\n",
            "[3/33] Loss: 1.4796\n",
            "[4/33] Loss: 1.3970\n",
            "[5/33] Loss: 1.3254\n",
            "[6/33] Loss: 1.4076\n",
            "[7/33] Loss: 1.2839\n",
            "[8/33] Loss: 1.6234\n",
            "[9/33] Loss: 1.4900\n",
            "[10/33] Loss: 1.4683\n",
            "[11/33] Loss: 1.3025\n",
            "[12/33] Loss: 1.3782\n",
            "[13/33] Loss: 1.6171\n",
            "[14/33] Loss: 1.3820\n",
            "[15/33] Loss: 1.3521\n",
            "[16/33] Loss: 1.3866\n",
            "[17/33] Loss: 1.6135\n",
            "[18/33] Loss: 1.4032\n",
            "[19/33] Loss: 1.3266\n",
            "[20/33] Loss: 1.3476\n",
            "[21/33] Loss: 1.2955\n",
            "[22/33] Loss: 1.3781\n",
            "[23/33] Loss: 1.3823\n",
            "[24/33] Loss: 1.4168\n",
            "[25/33] Loss: 1.3776\n",
            "[26/33] Loss: 1.6082\n",
            "[27/33] Loss: 1.2623\n",
            "[28/33] Loss: 1.2027\n",
            "[29/33] Loss: 1.2436\n",
            "[30/33] Loss: 1.2406\n",
            "[31/33] Loss: 1.3206\n",
            "[32/33] Loss: 1.3443\n",
            "[33/33] Loss: 1.3351\n",
            "Epoch: 4 | Source Accuracy: 0.9581, Target Accuracy: 0.2384, Loss: 1.3939\n",
            "Epoch 0005 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.2649\n",
            "[2/33] Loss: 1.1676\n",
            "[3/33] Loss: 1.1909\n",
            "[4/33] Loss: 1.4362\n",
            "[5/33] Loss: 1.3429\n",
            "[6/33] Loss: 1.2406\n",
            "[7/33] Loss: 1.3027\n",
            "[8/33] Loss: 1.4962\n",
            "[9/33] Loss: 1.3678\n",
            "[10/33] Loss: 1.4550\n",
            "[11/33] Loss: 1.4623\n",
            "[12/33] Loss: 1.2591\n",
            "[13/33] Loss: 1.2912\n",
            "[14/33] Loss: 1.2763\n",
            "[15/33] Loss: 1.1842\n",
            "[16/33] Loss: 1.2223\n",
            "[17/33] Loss: 1.2579\n",
            "[18/33] Loss: 1.3411\n",
            "[19/33] Loss: 1.2707\n",
            "[20/33] Loss: 1.2979\n",
            "[21/33] Loss: 1.3755\n",
            "[22/33] Loss: 1.3579\n",
            "[23/33] Loss: 1.3268\n",
            "[24/33] Loss: 1.2958\n",
            "[25/33] Loss: 1.3137\n",
            "[26/33] Loss: 1.4571\n",
            "[27/33] Loss: 1.3049\n",
            "[28/33] Loss: 1.4244\n",
            "[29/33] Loss: 1.2574\n",
            "[30/33] Loss: 1.6989\n",
            "[31/33] Loss: 1.3453\n",
            "[32/33] Loss: 1.4739\n",
            "[33/33] Loss: 1.3799\n",
            "Epoch: 5 | Source Accuracy: 0.9758, Target Accuracy: 0.2465, Loss: 1.3376\n",
            "Epoch 0006 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.3482\n",
            "[2/33] Loss: 1.3616\n",
            "[3/33] Loss: 1.2473\n",
            "[4/33] Loss: 1.4469\n",
            "[5/33] Loss: 1.2459\n",
            "[6/33] Loss: 1.3950\n",
            "[7/33] Loss: 1.1766\n",
            "[8/33] Loss: 1.2333\n",
            "[9/33] Loss: 1.2509\n",
            "[10/33] Loss: 1.2547\n",
            "[11/33] Loss: 1.2359\n",
            "[12/33] Loss: 1.2073\n",
            "[13/33] Loss: 1.1172\n",
            "[14/33] Loss: 1.1848\n",
            "[15/33] Loss: 1.2509\n",
            "[16/33] Loss: 1.2927\n",
            "[17/33] Loss: 1.2418\n",
            "[18/33] Loss: 1.2378\n",
            "[19/33] Loss: 1.3139\n",
            "[20/33] Loss: 1.4222\n",
            "[21/33] Loss: 1.3411\n",
            "[22/33] Loss: 1.3766\n",
            "[23/33] Loss: 1.2090\n",
            "[24/33] Loss: 1.2648\n",
            "[25/33] Loss: 1.3292\n",
            "[26/33] Loss: 1.2672\n",
            "[27/33] Loss: 1.2661\n",
            "[28/33] Loss: 1.1994\n",
            "[29/33] Loss: 1.3227\n",
            "[30/33] Loss: 1.3150\n",
            "[31/33] Loss: 1.4356\n",
            "[32/33] Loss: 1.3711\n",
            "[33/33] Loss: 1.1959\n",
            "Epoch: 6 | Source Accuracy: 0.9843, Target Accuracy: 0.2222, Loss: 1.2836\n",
            "Epoch 0007 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.2664\n",
            "[2/33] Loss: 1.2832\n",
            "[3/33] Loss: 1.6031\n",
            "[4/33] Loss: 1.3127\n",
            "[5/33] Loss: 1.2337\n",
            "[6/33] Loss: 1.2396\n",
            "[7/33] Loss: 1.1885\n",
            "[8/33] Loss: 1.4244\n",
            "[9/33] Loss: 1.5028\n",
            "[10/33] Loss: 1.5710\n",
            "[11/33] Loss: 1.3987\n",
            "[12/33] Loss: 1.3050\n",
            "[13/33] Loss: 1.3151\n",
            "[14/33] Loss: 1.2810\n",
            "[15/33] Loss: 1.4538\n",
            "[16/33] Loss: 1.6322\n",
            "[17/33] Loss: 1.4233\n",
            "[18/33] Loss: 1.3360\n",
            "[19/33] Loss: 1.3368\n",
            "[20/33] Loss: 1.4862\n",
            "[21/33] Loss: 1.4854\n",
            "[22/33] Loss: 1.4814\n",
            "[23/33] Loss: 1.3782\n",
            "[24/33] Loss: 1.2902\n",
            "[25/33] Loss: 1.3302\n",
            "[26/33] Loss: 1.3177\n",
            "[27/33] Loss: 1.3263\n",
            "[28/33] Loss: 1.2817\n",
            "[29/33] Loss: 1.2601\n",
            "[30/33] Loss: 1.3564\n",
            "[31/33] Loss: 1.3902\n",
            "[32/33] Loss: 1.2856\n",
            "[33/33] Loss: 1.3272\n",
            "Epoch: 7 | Source Accuracy: 0.9818, Target Accuracy: 0.2268, Loss: 1.3668\n",
            "Epoch 0008 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.3658\n",
            "[2/33] Loss: 1.3826\n",
            "[3/33] Loss: 1.3332\n",
            "[4/33] Loss: 1.2889\n",
            "[5/33] Loss: 1.3122\n",
            "[6/33] Loss: 1.2974\n",
            "[7/33] Loss: 1.2550\n",
            "[8/33] Loss: 1.2714\n",
            "[9/33] Loss: 1.2814\n",
            "[10/33] Loss: 1.7441\n",
            "[11/33] Loss: 1.3219\n",
            "[12/33] Loss: 1.2225\n",
            "[13/33] Loss: 1.2942\n",
            "[14/33] Loss: 1.2548\n",
            "[15/33] Loss: 1.2880\n",
            "[16/33] Loss: 1.2565\n",
            "[17/33] Loss: 1.2268\n",
            "[18/33] Loss: 1.1995\n",
            "[19/33] Loss: 1.2037\n",
            "[20/33] Loss: 1.3270\n",
            "[21/33] Loss: 1.2292\n",
            "[22/33] Loss: 1.2409\n",
            "[23/33] Loss: 1.2802\n",
            "[24/33] Loss: 1.1825\n",
            "[25/33] Loss: 1.2409\n",
            "[26/33] Loss: 1.2420\n",
            "[27/33] Loss: 1.1878\n",
            "[28/33] Loss: 1.1714\n",
            "[29/33] Loss: 1.2399\n",
            "[30/33] Loss: 1.4326\n",
            "[31/33] Loss: 1.3220\n",
            "[32/33] Loss: 1.1956\n",
            "[33/33] Loss: 1.3315\n",
            "Epoch: 8 | Source Accuracy: 0.9914, Target Accuracy: 0.2379, Loss: 1.2856\n",
            "Epoch 0009 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.2758\n",
            "[2/33] Loss: 1.2477\n",
            "[3/33] Loss: 1.2296\n",
            "[4/33] Loss: 1.3658\n",
            "[5/33] Loss: 1.2719\n",
            "[6/33] Loss: 1.2160\n",
            "[7/33] Loss: 1.1858\n",
            "[8/33] Loss: 1.2764\n",
            "[9/33] Loss: 1.1369\n",
            "[10/33] Loss: 1.2142\n",
            "[11/33] Loss: 1.2595\n",
            "[12/33] Loss: 1.2604\n",
            "[13/33] Loss: 1.1974\n",
            "[14/33] Loss: 1.3180\n",
            "[15/33] Loss: 1.1474\n",
            "[16/33] Loss: 1.2134\n",
            "[17/33] Loss: 1.1336\n",
            "[18/33] Loss: 1.1710\n",
            "[19/33] Loss: 1.1509\n",
            "[20/33] Loss: 1.1544\n",
            "[21/33] Loss: 1.2733\n",
            "[22/33] Loss: 1.2428\n",
            "[23/33] Loss: 1.1587\n",
            "[24/33] Loss: 1.2830\n",
            "[25/33] Loss: 1.2057\n",
            "[26/33] Loss: 1.2294\n",
            "[27/33] Loss: 1.1449\n",
            "[28/33] Loss: 1.2417\n",
            "[29/33] Loss: 1.2062\n",
            "[30/33] Loss: 1.2744\n",
            "[31/33] Loss: 1.2558\n",
            "[32/33] Loss: 1.2433\n",
            "[33/33] Loss: 1.1299\n",
            "Epoch: 9 | Source Accuracy: 0.9980, Target Accuracy: 0.2460, Loss: 1.2217\n",
            "Epoch 0010 / 0010\n",
            "============\n",
            "[1/33] Loss: 1.3100\n",
            "[2/33] Loss: 1.1777\n",
            "[3/33] Loss: 1.3107\n",
            "[4/33] Loss: 1.2676\n",
            "[5/33] Loss: 1.1916\n",
            "[6/33] Loss: 1.2408\n",
            "[7/33] Loss: 1.1747\n",
            "[8/33] Loss: 1.1673\n",
            "[9/33] Loss: 1.2378\n",
            "[10/33] Loss: 1.3103\n",
            "[11/33] Loss: 1.3053\n",
            "[12/33] Loss: 1.1610\n",
            "[13/33] Loss: 1.2813\n",
            "[14/33] Loss: 1.2622\n",
            "[15/33] Loss: 1.2063\n",
            "[16/33] Loss: 1.2116\n",
            "[17/33] Loss: 1.1441\n",
            "[18/33] Loss: 1.1184\n",
            "[19/33] Loss: 1.2376\n",
            "[20/33] Loss: 1.4882\n",
            "[21/33] Loss: 1.2583\n",
            "[22/33] Loss: 1.2209\n",
            "[23/33] Loss: 1.2519\n",
            "[24/33] Loss: 1.2478\n",
            "[25/33] Loss: 1.2301\n",
            "[26/33] Loss: 1.2032\n",
            "[27/33] Loss: 1.2441\n",
            "[28/33] Loss: 1.2037\n",
            "[29/33] Loss: 1.2718\n",
            "[30/33] Loss: 1.3537\n",
            "[31/33] Loss: 1.2713\n",
            "[32/33] Loss: 1.2076\n",
            "[33/33] Loss: 1.2099\n",
            "Epoch: 10 | Source Accuracy: 0.9949, Target Accuracy: 0.2389, Loss: 1.2418\n",
            "Training logs saved to LOG_DANN_Pruned_pruned_model_4\n",
            "[(Inference || test loss: 6.558434009552002] [accuracy_test: 25.95 %]\n",
            "Epoch 0001 / 0010\n",
            "============\n",
            "[1/10] Loss: 3.5291\n",
            "[2/10] Loss: 3.2096\n",
            "[3/10] Loss: 2.9796\n",
            "[4/10] Loss: 2.8100\n",
            "[5/10] Loss: 2.5538\n",
            "[6/10] Loss: 2.4057\n",
            "[7/10] Loss: 2.3584\n",
            "[8/10] Loss: 2.1083\n",
            "[9/10] Loss: 2.1284\n",
            "[10/10] Loss: 1.7836\n",
            "Epoch: 1 | Source Accuracy: 0.6308, Target Accuracy: 0.5658, Loss: 2.5867\n",
            "Epoch 0002 / 0010\n",
            "============\n",
            "[1/10] Loss: 1.7559\n",
            "[2/10] Loss: 1.5584\n",
            "[3/10] Loss: 1.6784\n",
            "[4/10] Loss: 1.6886\n",
            "[5/10] Loss: 1.5671\n",
            "[6/10] Loss: 1.4656\n",
            "[7/10] Loss: 1.6067\n",
            "[8/10] Loss: 1.5184\n",
            "[9/10] Loss: 1.6289\n",
            "[10/10] Loss: 1.4657\n",
            "Epoch: 2 | Source Accuracy: 0.9248, Target Accuracy: 0.7641, Loss: 1.5933\n",
            "Epoch 0003 / 0010\n",
            "============\n",
            "[1/10] Loss: 1.4636\n",
            "[2/10] Loss: 1.3901\n",
            "[3/10] Loss: 1.3732\n",
            "[4/10] Loss: 1.5464\n",
            "[5/10] Loss: 1.3362\n",
            "[6/10] Loss: 1.4662\n",
            "[7/10] Loss: 1.5506\n",
            "[8/10] Loss: 1.4539\n",
            "[9/10] Loss: 1.7783\n",
            "[10/10] Loss: 1.3034\n",
            "Epoch: 3 | Source Accuracy: 0.9538, Target Accuracy: 0.7880, Loss: 1.4662\n",
            "Epoch 0004 / 0010\n",
            "============\n",
            "[1/10] Loss: 1.4354\n",
            "[2/10] Loss: 1.6082\n",
            "[3/10] Loss: 1.4937\n",
            "[4/10] Loss: 1.6703\n",
            "[5/10] Loss: 1.3558\n",
            "[6/10] Loss: 1.5785\n",
            "[7/10] Loss: 1.3430\n",
            "[8/10] Loss: 1.3821\n",
            "[9/10] Loss: 1.4115\n",
            "[10/10] Loss: 1.9116\n",
            "Epoch: 4 | Source Accuracy: 0.9385, Target Accuracy: 0.8103, Loss: 1.5190\n",
            "Epoch 0005 / 0010\n",
            "============\n",
            "[1/10] Loss: 1.3084\n",
            "[2/10] Loss: 1.3072\n",
            "[3/10] Loss: 1.5321\n",
            "[4/10] Loss: 1.6136\n",
            "[5/10] Loss: 1.8612\n",
            "[6/10] Loss: 1.6038\n",
            "[7/10] Loss: 1.5537\n",
            "[8/10] Loss: 1.7347\n",
            "[9/10] Loss: 1.5182\n",
            "[10/10] Loss: 1.7264\n",
            "Epoch: 5 | Source Accuracy: 0.9368, Target Accuracy: 0.8154, Loss: 1.5759\n",
            "Epoch 0006 / 0010\n",
            "============\n",
            "[1/10] Loss: 1.4078\n",
            "[2/10] Loss: 1.2586\n",
            "[3/10] Loss: 1.6199\n",
            "[4/10] Loss: 1.7356\n",
            "[5/10] Loss: 1.6452\n",
            "[6/10] Loss: 1.6051\n",
            "[7/10] Loss: 1.6751\n",
            "[8/10] Loss: 1.3285\n",
            "[9/10] Loss: 1.5426\n",
            "[10/10] Loss: 1.6490\n",
            "Epoch: 6 | Source Accuracy: 0.9744, Target Accuracy: 0.8171, Loss: 1.5467\n",
            "Epoch 0007 / 0010\n",
            "============\n",
            "[1/10] Loss: 1.4762\n",
            "[2/10] Loss: 1.3859\n",
            "[3/10] Loss: 1.4164\n",
            "[4/10] Loss: 1.4768\n",
            "[5/10] Loss: 1.5349\n",
            "[6/10] Loss: 1.5448\n",
            "[7/10] Loss: 1.4927\n",
            "[8/10] Loss: 1.4705\n",
            "[9/10] Loss: 1.4139\n",
            "[10/10] Loss: 1.4888\n",
            "Epoch: 7 | Source Accuracy: 0.9709, Target Accuracy: 0.8632, Loss: 1.4701\n",
            "Epoch 0008 / 0010\n",
            "============\n",
            "[1/10] Loss: 1.4507\n",
            "[2/10] Loss: 1.3163\n",
            "[3/10] Loss: 1.3434\n",
            "[4/10] Loss: 1.2546\n",
            "[5/10] Loss: 1.2423\n",
            "[6/10] Loss: 1.2897\n",
            "[7/10] Loss: 1.4094\n",
            "[8/10] Loss: 1.2699\n",
            "[9/10] Loss: 1.3943\n",
            "[10/10] Loss: 1.5316\n",
            "Epoch: 8 | Source Accuracy: 0.9846, Target Accuracy: 0.8496, Loss: 1.3502\n",
            "Epoch 0009 / 0010\n",
            "============\n",
            "[1/10] Loss: 1.2762\n",
            "[2/10] Loss: 1.2366\n",
            "[3/10] Loss: 1.2193\n",
            "[4/10] Loss: 1.4569\n",
            "[5/10] Loss: 1.1775\n",
            "[6/10] Loss: 1.1034\n",
            "[7/10] Loss: 1.3523\n",
            "[8/10] Loss: 1.1864\n",
            "[9/10] Loss: 1.2813\n",
            "[10/10] Loss: 1.8166\n",
            "Epoch: 9 | Source Accuracy: 0.9949, Target Accuracy: 0.8393, Loss: 1.3107\n",
            "Epoch 0010 / 0010\n",
            "============\n",
            "[1/10] Loss: 1.2204\n",
            "[2/10] Loss: 1.2540\n",
            "[3/10] Loss: 1.1106\n",
            "[4/10] Loss: 1.2520\n",
            "[5/10] Loss: 1.2456\n",
            "[6/10] Loss: 1.3301\n",
            "[7/10] Loss: 1.3347\n",
            "[8/10] Loss: 1.5310\n",
            "[9/10] Loss: 1.1659\n",
            "[10/10] Loss: 1.6708\n",
            "Epoch: 10 | Source Accuracy: 0.9897, Target Accuracy: 0.8513, Loss: 1.3115\n",
            "Training logs saved to LOG_DANN_Pruned_pruned_model_5\n",
            "[(Inference || test loss: 4.033992767333984] [accuracy_test: 3.6999999999999997 %]\n"
          ]
        }
      ],
      "source": [
        "lr = 1e-3\n",
        "num_epochs = 10\n",
        "\n",
        "for i in range(6):\n",
        "    model = load_pruned_model(DANNMBNv3s, f\"pruned_model_{i}.pth\", device)\n",
        "    src_dataloader, tar_dataloader = lst[i][2], lst[i][3]\n",
        "    optimizer = optim.Adam(model.parameters(), lr)\n",
        "    steps_per_epoch = max(len(src_dataloader), len(tar_dataloader))\n",
        "    loss_fn_class = torch.nn.NLLLoss()\n",
        "    loss_fn_domain = torch.nn.NLLLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=lr,  # Max learning rate during the cycle\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        epochs=num_epochs,\n",
        "        anneal_strategy=\"cos\"  # Can also try 'cos' for cosine annealing\n",
        "    )\n",
        "    plot_graphDA(train_model_with_balanced_batches_DANN_Pruned(model, optimizer, scheduler, loss_fn_class, loss_fn_domain, src_dataloader, tar_dataloader, device, num_epochs=num_epochs, batch_size=64, name=f\"pruned_model_{i}.pth\"), f\"DANN_Pruned {i}\")\n",
        "    inference(model, tar_dataloader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFwrbPayU3IP"
      },
      "source": [
        "# ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ4qTAE1U5HX",
        "outputId": "033e0eaf-7922-4e34-ad74-c41ce757564a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.17.0\n",
            "Collecting onnxscript\n",
            "  Downloading onnxscript-0.1.0.dev20241007-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnxscript) (1.26.4)\n",
            "Requirement already satisfied: onnx>=1.16 in /usr/local/lib/python3.10/dist-packages (from onnxscript) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from onnxscript) (4.12.2)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from onnxscript) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxscript) (24.1)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.16->onnxscript) (3.20.3)\n",
            "Downloading onnxscript-0.1.0.dev20241007-py3-none-any.whl (670 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.5/670.5 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnxscript\n",
            "Successfully installed onnxscript-0.1.0.dev20241007\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-49-b57f027c97d5>:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(path)\n",
            "<ipython-input-16-028dd41c05bf>:48: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if x.shape[1] == 1:\n",
            "<ipython-input-16-028dd41c05bf>:50: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  elif x.shape[1] != 3:\n"
          ]
        }
      ],
      "source": [
        "# Save the ONNX model in a file\n",
        "\n",
        "# Define model and export the Torch model to onnx format\n",
        "!pip install onnx\n",
        "!pip install onnxscript\n",
        "import onnx\n",
        "from torch import func\n",
        "torch_model = load_pruned_model(DANNMBNv3s, f\"pruned_model_0.pth\", device).to('cpu')\n",
        "torch_input = torch.randn(1, 3, 224, 224)\n",
        "torch.onnx.export(\n",
        "    torch_model,                       # The model to export\n",
        "    torch_input,                 # A sample input tensor for the export process\n",
        "    \"my_image_classifier.onnx\",    # The output file name\n",
        "    opset_version=18,            # Specify the opset version\n",
        "    input_names=['input'],       # Names for the input nodes\n",
        "    output_names=['output']      # Names for the output nodes\n",
        ")\n",
        "\n",
        "# Import library and load from ONNX side\n",
        "\n",
        "onnx_program = onnx.load(\"my_image_classifier.onnx\")\n",
        "onnx.checker.check_model(onnx_program)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1lbCdRzVH_R",
        "outputId": "0a34bbf5-d4c2-44b5-bf6e-dc775db28173"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnxruntime-gpu\n",
            "  Downloading onnxruntime_gpu-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting coloredlogs (from onnxruntime-gpu)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (24.3.25)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (24.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (1.13.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n",
            "Downloading onnxruntime_gpu-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (226.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.2/226.2 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime-gpu\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-gpu-1.19.2\n",
            "\n",
            "\n",
            "PyTorch and ONNX Runtime output matched!\n",
            "PyTorch Output length: 2\n",
            "PyTorch Sample Output: (tensor([[-2.9416e+01, -1.8039e+01, -1.2913e+01, -3.1296e+01, -4.2207e+01,\n",
            "         -2.7517e+01, -2.9192e+01, -4.3720e+01, -3.0120e+01, -3.8755e+01,\n",
            "         -2.6278e+01, -2.9082e+01, -3.4297e+01, -3.2459e+01, -4.0089e+01,\n",
            "         -2.2068e+01, -3.6917e+01, -2.4688e+01, -3.9485e+01, -2.6960e+01,\n",
            "         -3.9616e+01, -3.5855e+01, -2.3308e+01, -2.6226e-06, -3.0143e+01,\n",
            "         -1.6609e+01, -1.6227e+01, -2.0582e+01, -3.5823e+01, -2.8470e+01,\n",
            "         -3.5352e+01]]), tensor([[-0.7796, -0.6136]]))\n",
            "ONNXRuntime Output length: 2\n",
            "ONNXRuntime Sample output: [array([[-2.9415836e+01, -1.8039225e+01, -1.2913147e+01, -3.1295645e+01,\n",
            "        -4.2206989e+01, -2.7516792e+01, -2.9192488e+01, -4.3719524e+01,\n",
            "        -3.0119957e+01, -3.8754742e+01, -2.6277662e+01, -2.9082176e+01,\n",
            "        -3.4297104e+01, -3.2459309e+01, -4.0089367e+01, -2.2067902e+01,\n",
            "        -3.6916729e+01, -2.4687803e+01, -3.9485313e+01, -2.6959929e+01,\n",
            "        -3.9616051e+01, -3.5854702e+01, -2.3307669e+01, -2.6226010e-06,\n",
            "        -3.0142824e+01, -1.6608887e+01, -1.6226854e+01, -2.0582163e+01,\n",
            "        -3.5822708e+01, -2.8469521e+01, -3.5351902e+01]], dtype=float32), array([[-0.77955794, -0.613613  ]], dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "# Execute the ONNX model with ONNX Runtime\n",
        "# !pip install onnxruntime\n",
        "!pip install onnxruntime-gpu\n",
        "import onnxruntime\n",
        "\n",
        "# Create an ONNXRuntime session\n",
        "ort_session = onnxruntime.InferenceSession(\"./my_image_classifier.onnx\", providers=['CPUExecutionProvider'])\n",
        "\n",
        "# Function to convert PyTorch tensor to NumPy array\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "# Prepare the inputs for ONNXRuntime (directly convert PyTorch tensor to NumPy)\n",
        "onnxruntime_input = {k.name: to_numpy(torch_input) for k in ort_session.get_inputs()}\n",
        "\n",
        "# Run inference with ONNXRuntime\n",
        "onnxruntime_outputs = ort_session.run(None, onnxruntime_input)\n",
        "\n",
        "# Compare the PyTorch results with the ones from ONNXRuntime\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Set the PyTorch model to evaluation mode\n",
        "torch_model.eval()\n",
        "\n",
        "# Run inference with PyTorch and ensure no gradients are computed\n",
        "with torch.no_grad():\n",
        "    torch_outputs = torch_model(torch_input)\n",
        "\n",
        "# Squeeze the ONNXRuntime output to remove the batch dimension ([1, 1000] -> [1000])\n",
        "onnxruntime_output_squeezed = [np.squeeze(output) for output in onnxruntime_outputs]\n",
        "\n",
        "# Ensure that ONNX output has the correct shape by adding the batch dimension if necessary\n",
        "for i, onnx_output in enumerate(onnxruntime_output_squeezed):\n",
        "    if len(onnx_output.shape) == 1:  # If ONNX output is missing the batch dimension\n",
        "        onnxruntime_output_squeezed[i] = np.expand_dims(onnx_output, axis=0)  # Add batch dimension\n",
        "\n",
        "# Compare the number of outputs\n",
        "assert len(torch_outputs) == len(onnxruntime_output_squeezed), \"The number of outputs does not match.\"\n",
        "\n",
        "# Compare outputs from PyTorch and ONNXRuntime\n",
        "for torch_output, onnx_output in zip(torch_outputs, onnxruntime_output_squeezed):\n",
        "    torch.testing.assert_close(torch_output, torch.tensor(onnx_output), rtol=1e-03, atol=1e-05)\n",
        "\n",
        "print(\"\\n\\nPyTorch and ONNX Runtime output matched!\")\n",
        "\n",
        "# Display information about the outputs\n",
        "print(f\"PyTorch Output length: {len(torch_outputs)}\")\n",
        "print(f\"PyTorch Sample Output: {torch_outputs}\")\n",
        "print(f\"ONNXRuntime Output length: {len(onnxruntime_output_squeezed)}\")\n",
        "print(f\"ONNXRuntime Sample output: {onnxruntime_output_squeezed}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCid8LF8VMxL"
      },
      "outputs": [],
      "source": [
        "input_shape = (1, 3, 224, 224)\n",
        "input_data_for_inference = torch.randn(input_shape, dtype=torch.float32).cuda()\n",
        "input_data_for_inference_fp16 = input_data_for_inference.to(dtype=torch.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWXJ9cCkXKwL"
      },
      "outputs": [],
      "source": [
        "# @title Non-Optimized FP32 (Full Tensor)\n",
        "import time\n",
        "\n",
        "input_shape = (1, 3, 224, 224)\n",
        "output_shape = (1, 10)\n",
        "\n",
        "#>>>> batch runs session\n",
        "print(\"Non-optimizing FP32 bench testing...\")\n",
        "#---Non-optimized---\n",
        "nonopt_model = load_pruned_model(DANNMBNv3s, f\"pruned_model_0.pth\", device).cuda().eval()\n",
        "num_iterations = 10000\n",
        "total_time = 0.0\n",
        "with torch.no_grad():\n",
        "    for i in range(num_iterations):\n",
        "        start_time = time.time()\n",
        "        input_data = torch.randn(input_shape).cuda()\n",
        "        output_data = nonopt_model(input_data)\n",
        "        end_time = time.time()\n",
        "        total_time += end_time - start_time\n",
        "pytorch_fps = num_iterations / total_time\n",
        "print(f\"PyTorch FPS: {pytorch_fps:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "#---------Inference with FP32--------------\n",
        "import onnxruntime as ort\n",
        "# Load the ONNX model and run inference\n",
        "# session = ort.InferenceSession('my_image_classifier_fp16.onnx', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
        "# onnx_input = onnx_program.adapt_torch_inputs_to_onnx(torch_input)\n",
        "# onnxruntime_input = {k.name: to_numpy(v) for k, v in zip(ort_session.get_inputs(), onnx_input)}\n",
        "\n",
        "output_data = nonopt_model(input_data_for_inference)\n",
        "\n",
        "print(f\"\\n\\nONNXRuntime [FP32] Output length: {len(output_data)}\")\n",
        "print(f\"ONNXRuntime [FP32] Sample output: {output_data}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtnMv0v3VSrL",
        "outputId": "502af421-762a-49a4-cb40-554dbc76d485"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimized model FP16 bench testing...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-028dd41c05bf>:48: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if x.shape[1] == 1:\n",
            "<ipython-input-16-028dd41c05bf>:50: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  elif x.shape[1] != 3:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*************** EP Error ***************\n",
            "EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:490 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(PySessionOptions&, const onnxruntime::ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
            " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n",
            "****************************************\n",
            "start inferencing...FP16\n",
            "Tensor FPS [FP16]: 322.75\n",
            "Speedup: 2.39x\n",
            "\n",
            "\n",
            "ONNXRuntime [FP16] Output length: 1\n",
            "ONNXRuntime [FP16] Sample output: [[-1.9375e+01 -1.2984e+01 -1.2555e+01 -2.0406e+01 -2.9625e+01 -2.3125e+01\n",
            "  -2.0766e+01 -3.0000e+01 -1.6609e+01 -2.5797e+01 -1.9875e+01 -1.9719e+01\n",
            "  -2.2219e+01 -2.0000e+01 -2.8500e+01 -1.3625e+01 -2.3688e+01 -1.9734e+01\n",
            "  -2.7891e+01 -2.2828e+01 -2.5922e+01 -2.4047e+01 -1.0469e+01 -7.8201e-05\n",
            "  -1.8625e+01 -1.7891e+01 -1.4023e+01 -1.0086e+01 -2.4391e+01 -2.1297e+01\n",
            "  -2.1672e+01]]\n",
            "ONNXRuntime [FP16] Sample output type: float16\n"
          ]
        }
      ],
      "source": [
        "# @title Optimized FP16 (Half Tensor)\n",
        "\n",
        "import os\n",
        "os.environ[\"ALLOW_RELEASED_ONNX_OPSET_ONLY\"] = \"0\"\n",
        "import onnxruntime.backend as backend\n",
        "print(\"Optimized model FP16 bench testing...\")\n",
        "\n",
        "# Try using CUDAExecutionProvider and check if it's available\n",
        "# ort_session = onnxruntime.InferenceSession(\"./my_image_classifier.onnx\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
        "\n",
        "torch_model_fp16 = torch_model.half()\n",
        "dummy_input_fp16 = torch.randn(input_shape, dtype=torch.float16)\n",
        "input_names = ['input']\n",
        "output_names = ['output']\n",
        "torch.onnx.export(torch_model_fp16, dummy_input_fp16, './my_image_classifier_fp16.onnx', verbose=False, input_names=input_names, output_names=output_names)\n",
        "\n",
        "#onnx_model_path_fp16 = f'outputs/{opt[\"compressed_directory\"]}/compressed_student_net_fp16.onnx'\n",
        "#session_fp16 = ort.InferenceSession(onnx_model_path_fp16, providers=providers)\n",
        "\n",
        "# Create a engine from the ONNX model and measure inference speed\n",
        "model_onnx_fp16 = onnx.load('./my_image_classifier_fp16.onnx')\n",
        "onnx_engine_fp16 = backend.prepare(model_onnx_fp16, device='GPU', provider='CUDAExecutionProvider', float16=True)\n",
        "\n",
        "num_iterations = 10000\n",
        "total_time_fp16 = 0.0\n",
        "print(\"start inferencing...FP16\")\n",
        "with torch.no_grad():\n",
        "    for i in range(num_iterations):\n",
        "        input_data = torch.randn(input_shape, dtype=torch.float16).cuda()\n",
        "        start_time = time.time()\n",
        "        output_data = onnx_engine_fp16.run(input_data.cpu().numpy())[0]\n",
        "        end_time = time.time()\n",
        "        total_time_fp16 += end_time - start_time\n",
        "tensor_fps_fp16 = num_iterations /total_time_fp16\n",
        "#tensor_fps = num_iterations / total_time\n",
        "print(f\"Tensor FPS [FP16]: {tensor_fps_fp16:.2f}\")\n",
        "print(f\"Speedup: {tensor_fps_fp16/pytorch_fps:.2f}x\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#---------Inference with FP16--------------\n",
        "import onnxruntime as ort\n",
        "# Load the ONNX model and run inference\n",
        "# session = ort.InferenceSession('my_image_classifier_fp16.onnx', providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
        "# onnx_input = onnx_program.adapt_torch_inputs_to_onnx(torch_input)\n",
        "# onnxruntime_input = {k.name: to_numpy(v) for k, v in zip(ort_session.get_inputs(), onnx_input)}\n",
        "# input_data = torch.randn(input_shape, dtype=torch.float16).cuda()\n",
        "output_data = onnx_engine_fp16.run(input_data_for_inference_fp16.cpu().numpy())[0]\n",
        "\n",
        "print(f\"\\n\\nONNXRuntime [FP16] Output length: {len(output_data)}\")\n",
        "print(f\"ONNXRuntime [FP16] Sample output: {output_data}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xguzlPz3WzZr"
      },
      "outputs": [],
      "source": [
        "def measure_inference_speed(model, input_shape, dtype=torch.float16, num_iterations=10000, device='cuda'):\n",
        "    total_time = 0.0\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    input_data = torch.randn(input_shape, dtype=dtype).to(device)\n",
        "\n",
        "    print(f\"Start inferencing with {dtype} precision...\")\n",
        "\n",
        "    # Measure inference speed for the given precision\n",
        "    with torch.no_grad():\n",
        "        for i in range(num_iterations):\n",
        "            start_time = time.time()\n",
        "            _ = model(input_data)\n",
        "            end_time = time.time()\n",
        "            total_time += end_time - start_time\n",
        "\n",
        "    # Calculate frames per second (FPS)\n",
        "    tensor_fps = num_iterations / total_time\n",
        "    print(f\"Tensor FPS [{dtype}]: {tensor_fps:.2f}\")\n",
        "\n",
        "    return tensor_fps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1cU5inaVwHb",
        "outputId": "90452fae-08cc-4e92-9abc-56e8af3905ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-49-b57f027c97d5>:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchao in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "INT8 output: (tensor([[-2.7958, -2.6695, -3.2461, -3.8750, -3.8860, -3.9304, -3.7531, -5.1361,\n",
            "         -2.1449, -5.0628, -4.1636, -3.8265, -3.3535, -4.7222, -3.7332, -2.7727,\n",
            "         -4.4208, -3.9320, -3.7017, -5.6523, -2.6095, -2.8189, -2.8499, -3.6414,\n",
            "         -3.9278, -3.3038, -4.2750, -3.7516, -3.8467, -3.1405, -4.2758]]), tensor([[-0.9763, -0.4727]]))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.quantization as quant\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load your pruned model and wrap it for quantization\n",
        "torch_model = load_pruned_model(DANNMBNv3s, f\"pruned_model_0.pth\", device).cpu()\n",
        "\n",
        "# Assuming you installed the required package torchao:\n",
        "!pip install torchao\n",
        "\n",
        "from torchao.quantization.quant_api import (\n",
        "    quantize_,\n",
        "    int8_weight_only\n",
        ")\n",
        "\n",
        "# Apply INT8 weight quantization\n",
        "quantize_(torch_model, int8_weight_only())\n",
        "\n",
        "torch_model_int8 = torch_model\n",
        "\n",
        "# Create input data for inference\n",
        "input_shape = (1, 3, 224, 224)  # Example input shape\n",
        "input_data = torch.randn(input_shape)\n",
        "\n",
        "# Run inference on the quantized model\n",
        "with torch.no_grad():\n",
        "    output_data = torch_model_int8(input_data)\n",
        "\n",
        "print(f\"INT8 output: {output_data}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2E3dS9p2AMs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr45CemyfBjp",
        "outputId": "182eb73f-1041-4ea8-d565-381b31468a69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-028dd41c05bf>:48: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if x.shape[1] == 1:\n",
            "<ipython-input-16-028dd41c05bf>:50: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  elif x.shape[1] != 3:\n"
          ]
        }
      ],
      "source": [
        "import torch.onnx\n",
        "\n",
        "# Export the INT8 quantized model to ONNX\n",
        "dummy_input_int8 = torch.randn(input_shape)\n",
        "torch.onnx.export(\n",
        "    torch_model_int8,\n",
        "    dummy_input_int8,\n",
        "    './my_image_classifier_int8.onnx',\n",
        "    verbose=False,\n",
        "    input_names=['input'],\n",
        "    output_names=['output']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXC-AoxPfEmK",
        "outputId": "36aca0ab-769f-4e53-8bc8-7e64a3e4f898"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start inferencing...INT8\n",
            "Tensor FPS [INT8]: 319.07\n",
            "Speedup: 2.36x\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "from onnxruntime import backend\n",
        "\n",
        "# Load the ONNX model\n",
        "model_onnx_int8 = onnx.load('./my_image_classifier_int8.onnx')\n",
        "\n",
        "# Prepare the ONNX runtime engine for INT8 (using CPU here, as CUDA INT8 support is limited)\n",
        "onnx_engine_int8 = ort.InferenceSession('./my_image_classifier_int8.onnx', providers=['CPUExecutionProvider'])\n",
        "\n",
        "# Measure inference speed for INT8\n",
        "num_iterations = 10000\n",
        "total_time_int8 = 0.0\n",
        "print(\"start inferencing...INT8\")\n",
        "with torch.no_grad():\n",
        "    for i in range(num_iterations):\n",
        "        input_data = torch.randn(input_shape).cpu()  # Ensure input is on CPU for INT8\n",
        "        start_time = time.time()\n",
        "        output_data = onnx_engine_int8.run(None, {'input': input_data.numpy()})[0]\n",
        "        end_time = time.time()\n",
        "        total_time_int8 += end_time - start_time\n",
        "\n",
        "# Calculate frames per second for INT8\n",
        "tensor_fps_int8 = num_iterations / total_time_int8\n",
        "print(f\"Tensor FPS [INT8]: {tensor_fps_int8:.2f}\")\n",
        "\n",
        "# Assuming you have tensor_fps_fp16 from the FP16 test\n",
        "# Calculate the speedup from INT8 to FP16\n",
        "print(f\"Speedup: {tensor_fps_int8 / pytorch_fps:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aq-qC_mJMii"
      },
      "source": [
        "# Model Grading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NE0knX8RJRGF"
      },
      "outputs": [],
      "source": [
        "# Define the grade function\n",
        "def grade(model, model_name, src_val_tl, tar_val_tl, tSNE=False, adist=False, conf=True, score=True, nmi_ri=True):\n",
        "    PATH = PATH_CP + model_name\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load the model\n",
        "    model = model.to(device)\n",
        "    model.load_state_dict(torch.load(PATH))\n",
        "\n",
        "    stored_lbs, stored_preds, avg_loss, accuracy = inference(model, tar_val_tl)\n",
        "    source_feature, s_labels = collect_feature(src_val_tl, model, device)\n",
        "    target_feature, t_labels = collect_feature(tar_val_tl, model, device)\n",
        "\n",
        "    # Prepare directory to save outputs\n",
        "    output_dir = './output'\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Results dictionary to store metrics for CSV output\n",
        "    results = {\n",
        "        'model_name': model_name,\n",
        "        'accuracy': accuracy,\n",
        "        'loss': avg_loss\n",
        "    }\n",
        "\n",
        "    if tSNE:\n",
        "        model.to('cpu')\n",
        "        # plot t-SNE\n",
        "        tSNE_filename = os.path.join(output_dir, f'tSNE_{model_name.rstrip(\".pth\")}.png')\n",
        "        visualize_class_n_domain(source_feature, target_feature, s_labels, t_labels, tSNE_filename)\n",
        "        print(f\"Saving t-SNE to {tSNE_filename}\")\n",
        "\n",
        "    if adist:\n",
        "        A_distance = Adist_calculate(source_feature, target_feature, 'cpu')\n",
        "        print(f\"A-distance = {A_distance}\")\n",
        "        results['A_distance'] = A_distance\n",
        "\n",
        "    lb = stored_lbs.cpu()\n",
        "    prd = stored_preds.cpu()\n",
        "\n",
        "    if conf:\n",
        "        class_names = [f'class_{i}' for i in range(0, 31)]\n",
        "        cm_target = confusion_matrix(y_true=lb, y_pred=prd, labels=np.arange(num_classes), normalize='true')\n",
        "\n",
        "        # Save the confusion matrix\n",
        "        plt.figure()\n",
        "        plot_confusion_matrix(cm_target, classes=class_names, normalize=True,\n",
        "                              title=f'Confusion Matrix for {model_name.rstrip(\".pth\")}',\n",
        "                              cmap=plt.cm.Blues, show_labels=False, show_numbers=False)\n",
        "        cm_filename = os.path.join(output_dir, f'CM_{model_name.rstrip(\".pth\")}.png')\n",
        "        plt.savefig(cm_filename)\n",
        "        print(f\"Confusion Matrix saved to {cm_filename}\")\n",
        "\n",
        "    if score:\n",
        "        precision, recall, fscore, _ = precision_recall_fscore_support(lb, prd, average='weighted', zero_division=0)\n",
        "        print(f\"Precision: {precision}, Recall: {recall}, F-score: {fscore}\")\n",
        "        results['precision'] = precision\n",
        "        results['recall'] = recall\n",
        "        results['fscore'] = fscore\n",
        "\n",
        "    if nmi_ri:\n",
        "        nmi_score = normalized_mutual_info_score(labels_true=lb, labels_pred=prd, average_method='arithmetic')\n",
        "        ri_score = adjusted_rand_score(labels_true=lb, labels_pred=prd)\n",
        "        print(f\"NMI score: {nmi_score}, RI score: {ri_score}\")\n",
        "        results['nmi_score'] = nmi_score\n",
        "        results['ri_score'] = ri_score\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl1fxlHzrzv_",
        "outputId": "bcf67b39-2f9d-4743-8a34-bc5f44266844"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
            ")\n",
            "Mod Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=31, bias=True)\n",
            ")\n",
            "Original Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
            ")\n",
            "Mod Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=31, bias=True)\n",
            ")\n",
            "Original Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
            ")\n",
            "Mod Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=31, bias=True)\n",
            ")\n",
            "Original Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
            ")\n",
            "Mod Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=31, bias=True)\n",
            ")\n",
            "Original Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
            ")\n",
            "Mod Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=31, bias=True)\n",
            ")\n",
            "Original Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
            ")\n",
            "Mod Classifier: Sequential(\n",
            "  (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "  (1): Hardswish()\n",
            "  (2): Dropout(p=0.2, inplace=True)\n",
            "  (3): Linear(in_features=1024, out_features=31, bias=True)\n",
            ")\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-59cc2fc08358>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(PATH))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(Inference || test loss: 1.8786544799804688] [accuracy_test: 52.800000000000004 %]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  6.13it/s]\n",
            "100%|██████████| 6/6 [00:04<00:00,  1.47it/s]\n",
            "<ipython-input-3-ad3c33898564>:295: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
            "  plt.scatter(\n",
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving t-SNE to ./output/tSNE_SO_amazon_to_dslr.png\n",
            "epoch 0 accuracy: 68.98148345947266 A-dist: 0.7592592239379883\n",
            "epoch 1 accuracy: 85.64814758300781 A-dist: 1.4259259700775146\n",
            "epoch 2 accuracy: 82.87036895751953 A-dist: 1.314814805984497\n",
            "epoch 3 accuracy: 86.5740737915039 A-dist: 1.4629628658294678\n",
            "epoch 4 accuracy: 87.96295928955078 A-dist: 1.5185184478759766\n",
            "epoch 5 accuracy: 82.87036895751953 A-dist: 1.314814805984497\n",
            "epoch 6 accuracy: 82.87036895751953 A-dist: 1.314814805984497\n",
            "epoch 7 accuracy: 82.40740966796875 A-dist: 1.2962963581085205\n",
            "epoch 8 accuracy: 83.33333587646484 A-dist: 1.3333334922790527\n",
            "epoch 9 accuracy: 80.55555725097656 A-dist: 1.2222223281860352\n",
            "A-distance = 1.2222223281860352\n",
            "Confusion Matrix saved to ./output/CM_SO_amazon_to_dslr.png\n",
            "Precision: 0.6242962369376743, Recall: 0.5335120643431636, F-score: 0.5280227384225699\n",
            "NMI score: 0.6606503034165998, RI score: 0.31583403590716197\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-59cc2fc08358>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(PATH))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(Inference || test loss: 1.637669324874878] [accuracy_test: 59.209999999999994 %]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  6.59it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00,  4.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 accuracy: 80.1104965209961 A-dist: 1.2044198513031006\n",
            "epoch 1 accuracy: 81.76795959472656 A-dist: 1.2707183361053467\n",
            "epoch 2 accuracy: 83.4254150390625 A-dist: 1.3370165824890137\n",
            "epoch 3 accuracy: 82.32044219970703 A-dist: 1.2928175926208496\n",
            "epoch 4 accuracy: 79.00552368164062 A-dist: 1.1602208614349365\n",
            "epoch 5 accuracy: 81.76795959472656 A-dist: 1.2707183361053467\n",
            "epoch 6 accuracy: 80.1104965209961 A-dist: 1.2044198513031006\n",
            "epoch 7 accuracy: 78.45304107666016 A-dist: 1.1381216049194336\n",
            "epoch 8 accuracy: 81.21546936035156 A-dist: 1.2486188411712646\n",
            "epoch 9 accuracy: 81.21546936035156 A-dist: 1.2486188411712646\n",
            "A-distance = 1.2486188411712646\n",
            "Confusion Matrix saved to ./output/CM_SO_amazon_to_webcam.png\n",
            "Precision: 0.6217158748565784, Recall: 0.5979899497487438, F-score: 0.5795937494813825\n",
            "NMI score: 0.736793905837804, RI score: 0.42676765102415826\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-59cc2fc08358>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(PATH))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(Inference || test loss: 2.499004602432251] [accuracy_test: 36.46 %]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:01<00:00,  1.47it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 accuracy: 83.73493957519531 A-dist: 1.3493976593017578\n",
            "epoch 1 accuracy: 87.9518051147461 A-dist: 1.5180721282958984\n",
            "epoch 2 accuracy: 87.9518051147461 A-dist: 1.5180721282958984\n",
            "epoch 3 accuracy: 87.34939575195312 A-dist: 1.4939758777618408\n",
            "epoch 4 accuracy: 88.55421447753906 A-dist: 1.5421686172485352\n",
            "epoch 5 accuracy: 88.55421447753906 A-dist: 1.5421686172485352\n",
            "epoch 6 accuracy: 89.15662384033203 A-dist: 1.5662648677825928\n",
            "epoch 7 accuracy: 89.15662384033203 A-dist: 1.5662648677825928\n",
            "epoch 8 accuracy: 88.55421447753906 A-dist: 1.5421686172485352\n",
            "epoch 9 accuracy: 89.15662384033203 A-dist: 1.5662648677825928\n",
            "A-distance = 1.5662648677825928\n",
            "Confusion Matrix saved to ./output/CM_SO_dslr_to_amazon.png\n",
            "Precision: 0.37820791432516215, Recall: 0.3078014184397163, F-score: 0.29321965266687383\n",
            "NMI score: 0.4445297842375717, RI score: 0.14318496796601213\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-59cc2fc08358>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(PATH))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(Inference || test loss: 0.6960874199867249] [accuracy_test: 86.22 %]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:01<00:00,  1.52it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00,  4.79it/s]\n",
            "<ipython-input-3-ad3c33898564>:295: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
            "  plt.scatter(\n",
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving t-SNE to ./output/tSNE_SO_dslr_to_webcam.png\n",
            "epoch 0 accuracy: 50.769229888916016 A-dist: 0.03076910972595215\n",
            "epoch 1 accuracy: 69.23076629638672 A-dist: 0.7692306041717529\n",
            "epoch 2 accuracy: 64.61538696289062 A-dist: 0.5846154689788818\n",
            "epoch 3 accuracy: 55.38461685180664 A-dist: 0.21538472175598145\n",
            "epoch 4 accuracy: 66.15384674072266 A-dist: 0.6461539268493652\n",
            "epoch 5 accuracy: 49.230770111083984 A-dist: -0.03076934814453125\n",
            "epoch 6 accuracy: 66.15384674072266 A-dist: 0.6461539268493652\n",
            "epoch 7 accuracy: 61.53845977783203 A-dist: 0.46153831481933594\n",
            "epoch 8 accuracy: 66.15384674072266 A-dist: 0.6461539268493652\n",
            "epoch 9 accuracy: 66.15384674072266 A-dist: 0.6461539268493652\n",
            "A-distance = 0.6461539268493652\n",
            "Confusion Matrix saved to ./output/CM_SO_dslr_to_webcam.png\n",
            "Precision: 0.9196099545345777, Recall: 0.9045226130653267, F-score: 0.9046723385328174\n",
            "NMI score: 0.9231103614108782, RI score: 0.8067137666664123\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-59cc2fc08358>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(PATH))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(Inference || test loss: 2.6399755477905273] [accuracy_test: 37.5 %]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  4.85it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.54it/s]\n",
            "<ipython-input-3-ad3c33898564>:295: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
            "  plt.scatter(\n",
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving t-SNE to ./output/tSNE_SO_webcam_to_amazon.png\n",
            "epoch 0 accuracy: 82.32044219970703 A-dist: 1.2928175926208496\n",
            "epoch 1 accuracy: 72.37569427490234 A-dist: 0.8950278759002686\n",
            "epoch 2 accuracy: 83.4254150390625 A-dist: 1.3370165824890137\n",
            "epoch 3 accuracy: 80.6629867553711 A-dist: 1.2265195846557617\n",
            "epoch 4 accuracy: 74.58563232421875 A-dist: 0.9834253787994385\n",
            "epoch 5 accuracy: 82.32044219970703 A-dist: 1.2928175926208496\n",
            "epoch 6 accuracy: 83.97789764404297 A-dist: 1.3591158390045166\n",
            "epoch 7 accuracy: 86.7403335571289 A-dist: 1.4696133136749268\n",
            "epoch 8 accuracy: 85.63536071777344 A-dist: 1.4254143238067627\n",
            "epoch 9 accuracy: 87.84530639648438 A-dist: 1.5138123035430908\n",
            "A-distance = 1.5138123035430908\n",
            "Confusion Matrix saved to ./output/CM_SO_webcam_to_amazon.png\n",
            "Precision: 0.3658640040370539, Recall: 0.3191489361702128, F-score: 0.3063590732763538\n",
            "NMI score: 0.44719592066255454, RI score: 0.1485688873508052\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-59cc2fc08358>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(PATH))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(Inference || test loss: 0.40882617235183716] [accuracy_test: 92.85 %]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  4.63it/s]\n",
            "100%|██████████| 2/2 [00:01<00:00,  1.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 accuracy: 50.769229888916016 A-dist: 0.03076910972595215\n",
            "epoch 1 accuracy: 60.0 A-dist: 0.40000009536743164\n",
            "epoch 2 accuracy: 55.38461685180664 A-dist: 0.21538472175598145\n",
            "epoch 3 accuracy: 56.92307662963867 A-dist: 0.27692317962646484\n",
            "epoch 4 accuracy: 47.69230651855469 A-dist: -0.09230756759643555\n",
            "epoch 5 accuracy: 52.30769348144531 A-dist: 0.09230780601501465\n",
            "epoch 6 accuracy: 53.846153259277344 A-dist: 0.15384602546691895\n",
            "epoch 7 accuracy: 58.46154022216797 A-dist: 0.33846163749694824\n",
            "epoch 8 accuracy: 60.0 A-dist: 0.40000009536743164\n",
            "epoch 9 accuracy: 55.38461685180664 A-dist: 0.21538472175598145\n",
            "A-distance = 0.21538472175598145\n",
            "Confusion Matrix saved to ./output/CM_SO_webcam_to_dslr.png\n",
            "Precision: 0.9605999999999999, Recall: 0.928, F-score: 0.9335208791208792\n",
            "NMI score: 0.9560924231168274, RI score: 0.8603595216788089\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-59cc2fc08358>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(PATH))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(Inference || test loss: 3.7503013610839844] [accuracy_test: 56.93 %]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  6.49it/s]\n",
            "100%|██████████| 2/2 [00:01<00:00,  1.47it/s]\n",
            "<ipython-input-3-ad3c33898564>:295: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
            "  plt.scatter(\n",
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving t-SNE to ./output/tSNE_DANN(Teacher)_amazon_to_dslr.png\n",
            "epoch 0 accuracy: 81.9277114868164 A-dist: 1.2771084308624268\n",
            "epoch 1 accuracy: 81.9277114868164 A-dist: 1.2771084308624268\n",
            "epoch 2 accuracy: 81.9277114868164 A-dist: 1.2771084308624268\n",
            "epoch 3 accuracy: 81.9277114868164 A-dist: 1.2771084308624268\n",
            "epoch 4 accuracy: 81.9277114868164 A-dist: 1.2771084308624268\n",
            "epoch 5 accuracy: 81.9277114868164 A-dist: 1.2771084308624268\n",
            "epoch 6 accuracy: 81.9277114868164 A-dist: 1.2771084308624268\n",
            "epoch 7 accuracy: 81.9277114868164 A-dist: 1.2771084308624268\n",
            "epoch 8 accuracy: 81.9277114868164 A-dist: 1.2771084308624268\n",
            "epoch 9 accuracy: 81.9277114868164 A-dist: 1.2771084308624268\n",
            "A-distance = 1.2771084308624268\n",
            "Confusion Matrix saved to ./output/CM_DANN(Teacher)_amazon_to_dslr.png\n",
            "Precision: 0.6546666666666666, Recall: 0.568, F-score: 0.5795242535242535\n",
            "NMI score: 0.7687066377880778, RI score: 0.37299977710327825\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-59cc2fc08358>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(PATH))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(Inference || test loss: 4.371996879577637] [accuracy_test: 49.72 %]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  6.47it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00,  4.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 accuracy: 75.13812255859375 A-dist: 1.0055248737335205\n",
            "epoch 1 accuracy: 75.13812255859375 A-dist: 1.0055248737335205\n",
            "epoch 2 accuracy: 75.13812255859375 A-dist: 1.0055248737335205\n",
            "epoch 3 accuracy: 75.13812255859375 A-dist: 1.0055248737335205\n",
            "epoch 4 accuracy: 75.13812255859375 A-dist: 1.0055248737335205\n",
            "epoch 5 accuracy: 75.13812255859375 A-dist: 1.0055248737335205\n",
            "epoch 6 accuracy: 75.13812255859375 A-dist: 1.0055248737335205\n",
            "epoch 7 accuracy: 75.13812255859375 A-dist: 1.0055248737335205\n",
            "epoch 8 accuracy: 75.13812255859375 A-dist: 1.0055248737335205\n",
            "epoch 9 accuracy: 75.13812255859375 A-dist: 1.0055248737335205\n",
            "A-distance = 1.0055248737335205\n",
            "Confusion Matrix saved to ./output/CM_DANN(Teacher)_amazon_to_webcam.png\n",
            "Precision: 0.6422219740058936, Recall: 0.5577889447236181, F-score: 0.564886969720435\n",
            "NMI score: 0.7320467626996665, RI score: 0.362236466887248\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-59cc2fc08358>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(PATH))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(Inference || test loss: 2.640855550765991] [accuracy_test: 33.46 %]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:01<00:00,  1.46it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 accuracy: 84.33734893798828 A-dist: 1.3734939098358154\n",
            "epoch 1 accuracy: 57.83132553100586 A-dist: 0.31325292587280273\n",
            "epoch 2 accuracy: 66.86746978759766 A-dist: 0.6746988296508789\n",
            "epoch 3 accuracy: 81.9277114868164 A-dist: 1.2771084308624268\n",
            "epoch 4 accuracy: 83.73493957519531 A-dist: 1.3493976593017578\n",
            "epoch 5 accuracy: 82.53012084960938 A-dist: 1.3012049198150635\n",
            "epoch 6 accuracy: 51.20481872558594 A-dist: 0.048192739486694336\n",
            "epoch 7 accuracy: 84.93975830078125 A-dist: 1.3975903987884521\n",
            "epoch 8 accuracy: 85.54216766357422 A-dist: 1.4216866493225098\n",
            "epoch 9 accuracy: 66.26506042480469 A-dist: 0.6506023406982422\n",
            "A-distance = 0.6506023406982422\n",
            "Confusion Matrix saved to ./output/CM_DANN(Teacher)_dslr_to_amazon.png\n",
            "Precision: 0.3109157527753172, Recall: 0.275177304964539, F-score: 0.26243081062934676\n",
            "NMI score: 0.4318763682118539, RI score: 0.1249206461159188\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-59cc2fc08358>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(PATH))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(Inference || test loss: 0.5129060745239258] [accuracy_test: 86.66 %]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:01<00:00,  1.48it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00,  4.79it/s]\n",
            "<ipython-input-3-ad3c33898564>:295: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
            "  plt.scatter(\n",
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving t-SNE to ./output/tSNE_DANN(Teacher)_dslr_to_webcam.png\n",
            "epoch 0 accuracy: 32.30769348144531 A-dist: -0.7076921463012695\n",
            "epoch 1 accuracy: 32.30769348144531 A-dist: -0.7076921463012695\n",
            "epoch 2 accuracy: 32.30769348144531 A-dist: -0.7076921463012695\n",
            "epoch 3 accuracy: 32.30769348144531 A-dist: -0.7076921463012695\n",
            "epoch 4 accuracy: 32.30769348144531 A-dist: -0.7076921463012695\n",
            "epoch 5 accuracy: 32.30769348144531 A-dist: -0.7076921463012695\n",
            "epoch 6 accuracy: 32.30769348144531 A-dist: -0.7076921463012695\n",
            "epoch 7 accuracy: 32.30769348144531 A-dist: -0.7076921463012695\n",
            "epoch 8 accuracy: 32.30769348144531 A-dist: -0.7076921463012695\n",
            "epoch 9 accuracy: 32.30769348144531 A-dist: -0.7076921463012695\n",
            "A-distance = -0.7076921463012695\n",
            "Confusion Matrix saved to ./output/CM_DANN(Teacher)_dslr_to_webcam.png\n",
            "Precision: 0.8954893515195024, Recall: 0.8693467336683417, F-score: 0.8633007403111794\n",
            "NMI score: 0.8995185552659003, RI score: 0.741596305417136\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-59cc2fc08358>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(PATH))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(Inference || test loss: 2.888970375061035] [accuracy_test: 31.900000000000002 %]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  4.72it/s]\n",
            "100%|██████████| 12/12 [00:01<00:00,  6.55it/s]\n",
            "<ipython-input-3-ad3c33898564>:295: UserWarning: *c* argument looks like a single numeric RGB or RGBA sequence, which should be avoided as value-mapping will have precedence in case its length matches with *x* & *y*.  Please use the *color* keyword-argument or provide a 2D array with a single row if you intend to specify the same RGB or RGBA value for all points.\n",
            "  plt.scatter(\n",
            "WARNING:matplotlib.legend:No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving t-SNE to ./output/tSNE_DANN(Teacher)_webcam_to_amazon.png\n",
            "epoch 0 accuracy: 76.24309539794922 A-dist: 1.0497238636016846\n",
            "epoch 1 accuracy: 19.889503479003906 A-dist: -1.2044198513031006\n",
            "epoch 2 accuracy: 79.55801391601562 A-dist: 1.1823205947875977\n",
            "epoch 3 accuracy: 22.651933670043945 A-dist: -1.0939226150512695\n",
            "epoch 4 accuracy: 22.651933670043945 A-dist: -1.0939226150512695\n",
            "epoch 5 accuracy: 79.55801391601562 A-dist: 1.1823205947875977\n",
            "epoch 6 accuracy: 59.66850662231445 A-dist: 0.38674020767211914\n",
            "epoch 7 accuracy: 79.55801391601562 A-dist: 1.1823205947875977\n",
            "epoch 8 accuracy: 85.08287048339844 A-dist: 1.4033148288726807\n",
            "epoch 9 accuracy: 82.8729248046875 A-dist: 1.3149170875549316\n",
            "A-distance = 1.3149170875549316\n",
            "Confusion Matrix saved to ./output/CM_DANN(Teacher)_webcam_to_amazon.png\n",
            "Precision: 0.29167377219059387, Recall: 0.2581560283687943, F-score: 0.24234775579760567\n",
            "NMI score: 0.424297878360507, RI score: 0.13915187421872927\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-59cc2fc08358>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(PATH))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(Inference || test loss: 0.8799011707305908] [accuracy_test: 87.22999999999999 %]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  4.79it/s]\n",
            "100%|██████████| 2/2 [00:01<00:00,  1.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 accuracy: 53.846153259277344 A-dist: 0.15384602546691895\n",
            "epoch 1 accuracy: 53.846153259277344 A-dist: 0.15384602546691895\n",
            "epoch 2 accuracy: 53.846153259277344 A-dist: 0.15384602546691895\n",
            "epoch 3 accuracy: 53.846153259277344 A-dist: 0.15384602546691895\n",
            "epoch 4 accuracy: 53.846153259277344 A-dist: 0.15384602546691895\n",
            "epoch 5 accuracy: 53.846153259277344 A-dist: 0.15384602546691895\n",
            "epoch 6 accuracy: 53.846153259277344 A-dist: 0.15384602546691895\n",
            "epoch 7 accuracy: 53.846153259277344 A-dist: 0.15384602546691895\n",
            "epoch 8 accuracy: 53.846153259277344 A-dist: 0.15384602546691895\n",
            "epoch 9 accuracy: 53.846153259277344 A-dist: 0.15384602546691895\n",
            "A-distance = 0.15384602546691895\n",
            "Confusion Matrix saved to ./output/CM_DANN(Teacher)_webcam_to_dslr.png\n",
            "Precision: 0.8639333333333333, Recall: 0.872, F-score: 0.855912087912088\n",
            "NMI score: 0.9371508395606333, RI score: 0.838049973879028\n",
            "Results saved to evaluation_results.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Function to evaluate all models in the loop\n",
        "def evaluate_models(lst, csv_output='evaluation_results.csv'):\n",
        "    # List of headers for the CSV file\n",
        "    headers = ['model_name', 'accuracy', 'loss', 'A_distance', 'precision', 'recall', 'fscore', 'nmi_score', 'ri_score']\n",
        "\n",
        "    # Open the CSV file for writing results\n",
        "    with open(csv_output, mode='w', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=headers)\n",
        "        writer.writeheader()  # Write header\n",
        "\n",
        "        for model, model_name, src_val_tl, tar_val_tl, tSNE, adist in lst:\n",
        "            # Grade the model and get the results\n",
        "            results = grade(model, model_name, src_val_tl, tar_val_tl, tSNE=tSNE, adist=adist)\n",
        "\n",
        "            # Write the results to CSV\n",
        "            writer.writerow(results)\n",
        "\n",
        "    print(f\"Results saved to {csv_output}\")\n",
        "\n",
        "# Example list of models and settings (as per your code)\n",
        "PATH_CP = './cp/'\n",
        "lst = [\n",
        "    [get_model_SO(), \"SO_amazon_to_dslr.pth\", amazon_vl, dslr_tl, True, True],\n",
        "    [get_model_SO(), \"SO_amazon_to_webcam.pth\", amazon_vl, webcam_vl, False, True],\n",
        "    [get_model_SO(), \"SO_dslr_to_amazon.pth\", dslr_vl, amazon_vl, False, True],\n",
        "    [get_model_SO(), \"SO_dslr_to_webcam.pth\", dslr_vl, webcam_vl, True, True],\n",
        "    [get_model_SO(), \"SO_webcam_to_amazon.pth\", webcam_vl, amazon_vl, True, True],\n",
        "    [get_model_SO(), \"SO_webcam_to_dslr.pth\", webcam_vl, dslr_vl, False, True],\n",
        "\n",
        "    [DANNMBNv3s(), \"DANN(Teacher)_amazon_to_dslr.pth\", amazon_vl, dslr_vl, True, True],\n",
        "    [DANNMBNv3s(), \"DANN(Teacher)_amazon_to_webcam.pth\", amazon_vl, webcam_vl, False, True],\n",
        "    [DANNMBNv3s(), \"DANN(Teacher)_dslr_to_amazon.pth\", dslr_vl, amazon_vl, False, True],\n",
        "    [DANNMBNv3s(), \"DANN(Teacher)_dslr_to_webcam.pth\", dslr_vl, webcam_vl, True, True],\n",
        "    [DANNMBNv3s(), \"DANN(Teacher)_webcam_to_amazon.pth\", webcam_vl, amazon_vl, True, True],\n",
        "    [DANNMBNv3s(), \"DANN(Teacher)_webcam_to_dslr.pth\", webcam_vl, dslr_vl, False, True],\n",
        "\n",
        "    # [models.mobilenet_v3_small(pretrained=True), \"Baseline_amazon.pth\", amazon_vl, amazon_vl, False, False],\n",
        "    # [models.mobilenet_v3_small(pretrained=True), \"Baseline_dslr.pth\", dslr_vl, dslr_vl, False, False],\n",
        "    # [models.mobilenet_v3_small(pretrained=True), \"Baseline_webcam.pth\", webcam_vl, webcam_vl, False, False],\n",
        "]\n",
        "\n",
        "# Run the evaluation and save results to CSV\n",
        "evaluate_models(lst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMELD3J2gYTR"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVhGQNvfgaW1"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meKhYV_Y4wZw",
        "outputId": "79140d80-dde0-4636-a862-4caa42397807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder '/content/output' deleted successfully.\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "# Delete the \"/content/cp\" folder and its contents\n",
        "shutil.rmtree('/content/output')\n",
        "\n",
        "print(\"Folder '/content/output' deleted successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPJDzHPn-5h3"
      },
      "outputs": [],
      "source": [
        "zip_directory(\"/content/output\", \"All_Graph.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9UX3eYsgIwx",
        "outputId": "8c91ce75-9d3e-4c8f-91ee-68f8e90ed1ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training logs loaded from LOG_DANN_amazon_to_dslr\n",
            "Training logs loaded from LOG_SO_amazon_to_dslr\n",
            "Training logs loaded from LOG_DANN_amazon_to_webcam\n",
            "Training logs loaded from LOG_SO_amazon_to_webcam\n",
            "Training logs loaded from LOG_DANN_dslr_to_amazon\n",
            "Training logs loaded from LOG_SO_dslr_to_amazon\n",
            "Training logs loaded from LOG_DANN_dslr_to_webcam\n",
            "Training logs loaded from LOG_SO_dslr_to_webcam\n",
            "Training logs loaded from LOG_DANN_webcam_to_amazon\n",
            "Training logs loaded from LOG_SO_webcam_to_amazon\n",
            "Training logs loaded from LOG_DANN_webcam_to_dslr\n",
            "Training logs loaded from LOG_SO_webcam_to_dslr\n"
          ]
        }
      ],
      "source": [
        "def plot_comparison(file1_data, file2_data, file1_name=\"File 1\", file2_name=\"File 2\", save_name=\"file1_vs_file2.png\"):\n",
        "    \"\"\"\n",
        "    This function plots three graphs (Train Loss, Source Accuracy, and Target Accuracy) comparing two datasets.\n",
        "    It then saves the figure with the provided filename.\n",
        "\n",
        "    Args:\n",
        "    - file1_data (dict): Dictionary with keys 'train_loss', 'train_src_acc', 'train_tar_acc' for the first dataset.\n",
        "    - file2_data (dict): Dictionary with keys 'train_loss', 'train_src_acc', 'train_tar_acc' for the second dataset.\n",
        "    - file1_name (str): Name to label the first dataset on the plots.\n",
        "    - file2_name (str): Name to label the second dataset on the plots.\n",
        "    - save_name (str): Filename to save the generated plot image.\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(3, 1, figsize=(10, 15))\n",
        "\n",
        "    # Plot 1: Train Loss\n",
        "    axs[0].plot(file1_data[\"train_loss\"], label=f'{file1_name} Loss')\n",
        "    axs[0].plot(file2_data[\"train_loss\"], label=f'{file2_name} Loss')\n",
        "    axs[0].set_title('Train Loss')\n",
        "    axs[0].set_xlabel('Epochs')\n",
        "    axs[0].set_ylabel('Loss')\n",
        "    axs[0].legend()\n",
        "\n",
        "    # Plot 2: Source Accuracy\n",
        "    axs[1].plot(file1_data[\"train_src_acc\"], label=f'{file1_name} Source Accuracy')\n",
        "    axs[1].plot(file2_data[\"train_src_acc\"], label=f'{file2_name} Source Accuracy')\n",
        "    axs[1].set_title('Source Domain Accuracy')\n",
        "    axs[1].set_xlabel('Epochs')\n",
        "    axs[1].set_ylabel('Accuracy')\n",
        "    axs[1].legend()\n",
        "\n",
        "    # Plot 3: Target Accuracy\n",
        "    axs[2].plot(file1_data[\"train_tar_acc\"], label=f'{file1_name} Target Accuracy')\n",
        "    axs[2].plot(file2_data[\"train_tar_acc\"], label=f'{file2_name} Target Accuracy')\n",
        "    axs[2].set_title('Target Domain Accuracy')\n",
        "    axs[2].set_xlabel('Epochs')\n",
        "    axs[2].set_ylabel('Accuracy')\n",
        "    axs[2].legend()\n",
        "\n",
        "    # Save the plots to a file\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_name)\n",
        "\n",
        "    # Show the plots\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# load_training_logs(filename)\n",
        "log_pairs = [\n",
        "    [\"LOG_DANN_amazon_to_dslr\", \"LOG_SO_amazon_to_dslr\"],\n",
        "    [\"LOG_DANN_amazon_to_webcam\", \"LOG_SO_amazon_to_webcam\"],\n",
        "    [\"LOG_DANN_dslr_to_amazon\", \"LOG_SO_dslr_to_amazon\"],\n",
        "    [\"LOG_DANN_dslr_to_webcam\", \"LOG_SO_dslr_to_webcam\"],\n",
        "    [\"LOG_DANN_webcam_to_amazon\", \"LOG_SO_webcam_to_amazon\"],\n",
        "    [\"LOG_DANN_webcam_to_dslr\", \"LOG_SO_webcam_to_dslr\"]\n",
        "]\n",
        "for so, dann in log_pairs:\n",
        "    file1_data = load_training_logs(so)\n",
        "    file2_data = load_training_logs(dann)\n",
        "    plot_comparison(file1_data, file2_data, file1_name=f\"{so}\", file2_name=f\"{dann}\", save_name=f\"{so}_vs_{dann}.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa8Czopag3d1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6uXLV78uv4Ig",
        "aLoBd1xCgo6S",
        "HyDpxFuDKV43",
        "Bf7Kxfh8pGfy",
        "TME271QwpVSn",
        "xsc6skLU277K"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}