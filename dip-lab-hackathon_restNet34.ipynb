{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":84870,"databundleVersionId":9546329,"sourceType":"competition"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2024-09-16T10:19:35.083897Z","iopub.execute_input":"2024-09-16T10:19:35.084194Z","iopub.status.idle":"2024-09-16T10:19:49.257932Z","shell.execute_reply.started":"2024-09-16T10:19:35.084161Z","shell.execute_reply":"2024-09-16T10:19:49.256880Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\nInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\nimport time\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom datetime import datetime","metadata":{"execution":{"iopub.status.busy":"2024-09-16T10:19:49.260293Z","iopub.execute_input":"2024-09-16T10:19:49.261040Z","iopub.status.idle":"2024-09-16T10:19:53.572263Z","shell.execute_reply.started":"2024-09-16T10:19:49.260992Z","shell.execute_reply":"2024-09-16T10:19:53.571479Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport os\nfrom sklearn.model_selection import train_test_split\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader\nimport matplotlib as plt\nimport numpy as np\nimport pandas as pd\n\nimage_paths = ['/kaggle/input/dip-lab-hackathon-2024-image-classification/data/train', '/kaggle/input/dip-lab-hackathon-2024-image-classification/data/test']\n\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),    \n    transforms.RandomHorizontalFlip(),    \n    transforms.RandomRotation(15),        \n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), \n    transforms.ToTensor(),                \n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),   \n])\n\nval_transform = transforms.Compose([\n    transforms.Resize(256),              \n    transforms.CenterCrop(224),           \n    transforms.ToTensor(),               \n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  \n])\n\nDs_target = datasets.ImageFolder(root='/kaggle/input/dip-lab-hackathon-2024-image-classification/data/train', transform=train_transform)\n\ntrain_size = int(0.9 * len(Ds_target))\nval_size = len(Ds_target) - train_size\ntraining_set, validation_set = torch.utils.data.random_split(Ds_target, [train_size, val_size])\n\nvalidation_set.dataset.transform = val_transform\n\ntraining_loader = DataLoader(training_set, batch_size=64, shuffle=True)\nvalidation_loader = DataLoader(validation_set, batch_size=64, shuffle=False)\n\nprint(f\"Training set size: {len(training_set)}\")\nprint(f\"Validation set size: {len(validation_set)}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T10:19:53.573252Z","iopub.execute_input":"2024-09-16T10:19:53.573641Z","iopub.status.idle":"2024-09-16T10:19:55.693915Z","shell.execute_reply.started":"2024-09-16T10:19:53.573605Z","shell.execute_reply":"2024-09-16T10:19:55.692961Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Training set size: 4502\nValidation set size: 501\n","output_type":"stream"}]},{"cell_type":"code","source":"model_urls = {\n     'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n     'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n     'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n     'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n     'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n }","metadata":{"execution":{"iopub.status.busy":"2024-09-16T10:19:55.696087Z","iopub.execute_input":"2024-09-16T10:19:55.696520Z","iopub.status.idle":"2024-09-16T10:19:55.701093Z","shell.execute_reply.started":"2024-09-16T10:19:55.696486Z","shell.execute_reply":"2024-09-16T10:19:55.700255Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion *\n                               planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n        # self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.in_planes)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.maxpool(F.relu(self.bn1(self.conv1(x))))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n# Model Selection\ndef ResNet18(n_C=10):\n    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=n_C)\n\ndef ResNet34(n_C=10):\n    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes=n_C)\n\ndef ResNet50(n_C=10):\n    return ResNet(Bottleneck, [3, 4, 6, 3], num_classes=n_C)\n\ndef ResNet101(n_C=10):\n    return ResNet(Bottleneck, [3, 4, 23, 3], num_classes=n_C)\n\ndef ResNet152(n_C=10):\n    return ResNet(Bottleneck, [3, 8, 36, 3], num_classes=n_C)\n\n\n!pip install wget\nimport wget\ndef download_n_load_model(model, model_name):\n    if model_name in model_urls:\n      if not os.path.exists(f\"{model_name}.pth\"):\n        url = model_urls[model_name]\n        wget.download(url, out=f\"{model_name}.pth\")\n      rn_model_dict=model.state_dict()\n      url = model_urls[model_name]\n      pretrained_dict = torch.load(f'./{model_name}.pth')\n      pretrained_dict = {k: v for k, v in pretrained_dict.items() if (k in rn_model_dict) and (rn_model_dict[k].shape == pretrained_dict[k].shape)}\n      rn_model_dict.update(pretrained_dict)\n      model.load_state_dict(rn_model_dict)\n      print(f\"load checkpoint...{model_name}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T10:19:55.702704Z","iopub.execute_input":"2024-09-16T10:19:55.703063Z","iopub.status.idle":"2024-09-16T10:20:11.126637Z","shell.execute_reply.started":"2024-09-16T10:19:55.703016Z","shell.execute_reply":"2024-09-16T10:20:11.125541Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting wget\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: wget\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=28a33b1e0ff57e5f0ffe882ab055daa6c9a03782afefa511cb157b2ba57edb85\n  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\nSuccessfully built wget\nInstalling collected packages: wget\nSuccessfully installed wget-3.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.optim as optim\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = ResNet34(n_C=10)\nmodel.to(device)\nsummary(model, (3, 224, 224))\n\ndownload_n_load_model(model, 'resnet34')\n\n# Loss function\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(),\n    lr=0.01,\n    momentum=0.9,\n    weight_decay=5e-4\n)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\nepoch_number = 0\n\nEPOCHS = 19\npath_save_cp = './cp/'\nbest_vloss = 1_000_000.\ntraining_logs = {\"train_loss\": [],  \"train_acc\": [], \"validate_loss\": [], \"validate_acc\": []}\n\nt_0_accelerated = time.time()\nfor epoch in range(EPOCHS):\n    train_loss, train_correct = 0, 0\n    # Make sure gradient tracking is on, and do a pass over the data\n    model.train(True)\n    # Here, we use enumerate(training_loader) instead of\n    # iter(training_loader) so that we can track the batch\n    # index and do some intra-epoch reporting\n    for i, data in enumerate(training_loader):\n        # Every data instance is an input + label pair\n        inputs, labels = data[0].to(device), data[1].to(device)\n\n        # Zero your gradients for every batch!\n        optimizer.zero_grad()\n\n        # Make predictions for this batch\n        outputs = model(inputs)\n\n        # Compute the loss and its gradients\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n\n        # Adjust learning weights\n        optimizer.step()\n\n        train_loss += loss.item()\n        train_correct += (outputs.argmax(1) == labels).float().sum().item()\n    scheduler.step()\n\n    training_logs[\"train_loss\"].append(train_loss / len(training_loader))\n    training_logs[\"train_acc\"].append(train_correct / len(training_loader.dataset))\n\n    running_vloss = 0.0\n    # Set the model to evaluation mode, disabling dropout and using population\n    # statistics for batch normalization.\n    model.eval()\n    # Disable gradient computation and reduce memory consumption.\n    valid_loss, valid_correct = 0, 0\n    with torch.no_grad():\n        for i, vdata in enumerate(validation_loader):\n            vinputs, vlabels = vdata[0].to(device), vdata[1].to(device)\n            voutputs = model(vinputs)\n            vloss = loss_fn(voutputs, vlabels)\n            valid_loss += loss_fn(voutputs, vlabels).item()\n            valid_correct += (voutputs.argmax(1) == vlabels).float().sum().item()\n        # save validation logs\n        training_logs[\"validate_loss\"].append(valid_loss / len(validation_loader))\n        training_logs[\"validate_acc\"].append(valid_correct / len(validation_loader.dataset))\n\n    if epoch % 1 == 0:\n        print(f\"Epochs {epoch+1}\".ljust(10),\n            f\"train loss {training_logs['train_loss'][-1]:.5f}\",\n            f\"train acc {training_logs['train_acc'][-1]:.5f}\",\n\n            f\"validate loss {training_logs['validate_loss'][-1]:.5f}\",\n            f\"validate acc {training_logs['validate_acc'][-1]:.5f}\",\n            )\n        print(\"-\"*80)\n\n    # Track best performance, and save the model's state\n    if valid_loss < best_vloss:\n        best_vloss = valid_loss\n        # model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n        if not os.path.exists(path_save_cp): os.mkdir(path_save_cp)\n        torch.save(model.state_dict(), path_save_cp+'best_pretrainedmodel.pth')\n\n    epoch_number += 1\n\nt_end_accelerated = time.time()-t_0_accelerated\nprint(f\"Time consumption for accelerated CUDA training (device:{device}): {t_end_accelerated} sec\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T10:29:02.705083Z","iopub.execute_input":"2024-09-16T10:29:02.706134Z","iopub.status.idle":"2024-09-16T10:35:03.253425Z","shell.execute_reply.started":"2024-09-16T10:29:02.706064Z","shell.execute_reply":"2024-09-16T10:35:03.251866Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 112, 112]           9,408\n       BatchNorm2d-2         [-1, 64, 112, 112]             128\n         MaxPool2d-3           [-1, 64, 56, 56]               0\n            Conv2d-4           [-1, 64, 56, 56]          36,864\n       BatchNorm2d-5           [-1, 64, 56, 56]             128\n            Conv2d-6           [-1, 64, 56, 56]          36,864\n       BatchNorm2d-7           [-1, 64, 56, 56]             128\n        BasicBlock-8           [-1, 64, 56, 56]               0\n            Conv2d-9           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-10           [-1, 64, 56, 56]             128\n           Conv2d-11           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-12           [-1, 64, 56, 56]             128\n       BasicBlock-13           [-1, 64, 56, 56]               0\n           Conv2d-14           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-15           [-1, 64, 56, 56]             128\n           Conv2d-16           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-17           [-1, 64, 56, 56]             128\n       BasicBlock-18           [-1, 64, 56, 56]               0\n           Conv2d-19          [-1, 128, 28, 28]          73,728\n      BatchNorm2d-20          [-1, 128, 28, 28]             256\n           Conv2d-21          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-22          [-1, 128, 28, 28]             256\n           Conv2d-23          [-1, 128, 28, 28]           8,192\n      BatchNorm2d-24          [-1, 128, 28, 28]             256\n       BasicBlock-25          [-1, 128, 28, 28]               0\n           Conv2d-26          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-27          [-1, 128, 28, 28]             256\n           Conv2d-28          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-29          [-1, 128, 28, 28]             256\n       BasicBlock-30          [-1, 128, 28, 28]               0\n           Conv2d-31          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-32          [-1, 128, 28, 28]             256\n           Conv2d-33          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-34          [-1, 128, 28, 28]             256\n       BasicBlock-35          [-1, 128, 28, 28]               0\n           Conv2d-36          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-37          [-1, 128, 28, 28]             256\n           Conv2d-38          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-39          [-1, 128, 28, 28]             256\n       BasicBlock-40          [-1, 128, 28, 28]               0\n           Conv2d-41          [-1, 256, 14, 14]         294,912\n      BatchNorm2d-42          [-1, 256, 14, 14]             512\n           Conv2d-43          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-44          [-1, 256, 14, 14]             512\n           Conv2d-45          [-1, 256, 14, 14]          32,768\n      BatchNorm2d-46          [-1, 256, 14, 14]             512\n       BasicBlock-47          [-1, 256, 14, 14]               0\n           Conv2d-48          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-49          [-1, 256, 14, 14]             512\n           Conv2d-50          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-51          [-1, 256, 14, 14]             512\n       BasicBlock-52          [-1, 256, 14, 14]               0\n           Conv2d-53          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-54          [-1, 256, 14, 14]             512\n           Conv2d-55          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-56          [-1, 256, 14, 14]             512\n       BasicBlock-57          [-1, 256, 14, 14]               0\n           Conv2d-58          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-59          [-1, 256, 14, 14]             512\n           Conv2d-60          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-61          [-1, 256, 14, 14]             512\n       BasicBlock-62          [-1, 256, 14, 14]               0\n           Conv2d-63          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-64          [-1, 256, 14, 14]             512\n           Conv2d-65          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-66          [-1, 256, 14, 14]             512\n       BasicBlock-67          [-1, 256, 14, 14]               0\n           Conv2d-68          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-69          [-1, 256, 14, 14]             512\n           Conv2d-70          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-71          [-1, 256, 14, 14]             512\n       BasicBlock-72          [-1, 256, 14, 14]               0\n           Conv2d-73            [-1, 512, 7, 7]       1,179,648\n      BatchNorm2d-74            [-1, 512, 7, 7]           1,024\n           Conv2d-75            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-76            [-1, 512, 7, 7]           1,024\n           Conv2d-77            [-1, 512, 7, 7]         131,072\n      BatchNorm2d-78            [-1, 512, 7, 7]           1,024\n       BasicBlock-79            [-1, 512, 7, 7]               0\n           Conv2d-80            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-81            [-1, 512, 7, 7]           1,024\n           Conv2d-82            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-83            [-1, 512, 7, 7]           1,024\n       BasicBlock-84            [-1, 512, 7, 7]               0\n           Conv2d-85            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-86            [-1, 512, 7, 7]           1,024\n           Conv2d-87            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-88            [-1, 512, 7, 7]           1,024\n       BasicBlock-89            [-1, 512, 7, 7]               0\n           Linear-90                   [-1, 10]           5,130\n================================================================\nTotal params: 21,289,802\nTrainable params: 21,289,802\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 69.10\nParams size (MB): 81.21\nEstimated Total Size (MB): 150.89\n----------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1274581106.py:119: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  pretrained_dict = torch.load(f'./{model_name}.pth')\n","output_type":"stream"},{"name":"stdout","text":"load checkpoint...resnet34\nEpochs 1   train loss 1.88309 train acc 0.32563 validate loss 1.16427 validate acc 0.61876\n--------------------------------------------------------------------------------\nEpochs 2   train loss 0.69431 train acc 0.76721 validate loss 0.88033 validate acc 0.72056\n--------------------------------------------------------------------------------\nEpochs 3   train loss 0.38212 train acc 0.87161 validate loss 0.70578 validate acc 0.78842\n--------------------------------------------------------------------------------\nEpochs 4   train loss 0.22253 train acc 0.92914 validate loss 0.72517 validate acc 0.78244\n--------------------------------------------------------------------------------\nEpochs 5   train loss 0.13274 train acc 0.95313 validate loss 0.67083 validate acc 0.83234\n--------------------------------------------------------------------------------\nEpochs 6   train loss 0.09891 train acc 0.97046 validate loss 0.59092 validate acc 0.84232\n--------------------------------------------------------------------------------\nEpochs 7   train loss 0.06256 train acc 0.97801 validate loss 0.60522 validate acc 0.83633\n--------------------------------------------------------------------------------\nEpochs 8   train loss 0.01953 train acc 0.99556 validate loss 0.49911 validate acc 0.86627\n--------------------------------------------------------------------------------\nEpochs 9   train loss 0.00760 train acc 0.99911 validate loss 0.50084 validate acc 0.87824\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[9], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# Adjust learning weights\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 50\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     train_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (outputs\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     52\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"import torch\n\nPATH = '/kaggle/working/cp/best_pretrainedmodel.pth'\nmodel = ResNet34(n_C=10)\nmodel.load_state_dict(torch.load(PATH), strict=False)\nmodel.to(device).eval()\n\nacc_test = 0\ntest_loss = 0\n\n# Ensure no gradient computation is performed\nwith torch.no_grad():\n    for tinputs, tlabels in validation_loader:\n        tinputs, tlabels = tinputs.to(device), tlabels.to(device)\n        toutputs = model(tinputs)\n        \n        # Compute loss\n        loss = loss_fn(toutputs, tlabels)\n        test_loss += loss.item()\n        \n        # Compute accuracy\n        _, preds_t = torch.max(toutputs, 1)\n        acc_test += (preds_t == tlabels).float().sum().item()\n\n# Calculate average metrics\naccuracy_t = round(acc_test / len(validation_loader.dataset) * 100, 2)\navg_tloss = test_loss / len(validation_loader)\n\nprint(f'[Test loss: {avg_tloss}] [Accuracy test: {accuracy_t}%]')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-16T10:35:03.254494Z","iopub.status.idle":"2024-09-16T10:35:03.254874Z","shell.execute_reply.started":"2024-09-16T10:35:03.254683Z","shell.execute_reply":"2024-09-16T10:35:03.254708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\nmodel.eval()\ndf = pd.read_csv(\"/kaggle/input/dip-lab-hackathon-2024-image-classification/sample-submission.csv\")\nimage_dir = '/kaggle/input/dip-lab-hackathon-2024-image-classification/data/test/data'\npredictions = []\n\nfor idx, row in df.iterrows():\n    image_path = os.path.join(image_dir, row['ID'])\n    image = Image.open(image_path)\n    \n    image = val_transform(image).unsqueeze(0)\n    \n    if torch.cuda.is_available():\n        image = image.cuda()\n        loaded_model = model.cuda()\n    \n    with torch.no_grad():\n        output = model(image)\n        _, predicted = torch.max(output, 1)\n    \n    predictions.append(predicted.item())\n\ndf['predicted_class'] = predictions\n\ndf.to_csv('/kaggle/working/submission.csv', index=False)\n\nprint(\"CSV file updated with predictions.\")","metadata":{"execution":{"iopub.status.busy":"2024-09-16T10:35:03.255776Z","iopub.status.idle":"2024-09-16T10:35:03.256165Z","shell.execute_reply.started":"2024-09-16T10:35:03.255936Z","shell.execute_reply":"2024-09-16T10:35:03.255952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['NCCL_DEBUG'] = 'INFO'","metadata":{"execution":{"iopub.status.busy":"2024-09-16T10:25:30.374746Z","iopub.status.idle":"2024-09-16T10:25:30.375129Z","shell.execute_reply.started":"2024-09-16T10:25:30.374920Z","shell.execute_reply":"2024-09-16T10:25:30.374938Z"},"trusted":true},"execution_count":null,"outputs":[]}]}